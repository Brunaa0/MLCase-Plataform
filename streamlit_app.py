########### Bibliotecas Necess√°rias ###########

# -------------------------------------
# üìå Bibliotecas para Interface com Utilizador (Streamlit)
# -------------------------------------
import streamlit as st  # Framework para cria√ß√£o de interfaces web interativas
import streamlit.components.v1 as components  # Permite adicionar componentes HTML/CSS personalizados

# -------------------------------------
# üìå Manipula√ß√£o e An√°lise de Dados
# -------------------------------------
import pandas as pd  # Manipula√ß√£o de DataFrames e s√©ries temporais
import numpy as np  # Opera√ß√µes num√©ricas e matrizes eficientes

# -------------------------------------
# üìå Visualiza√ß√£o de Dados
# -------------------------------------
import matplotlib.pyplot as plt  # Cria√ß√£o de gr√°ficos est√°ticos
import seaborn as sns  # Gr√°ficos estat√≠sticos avan√ßados baseados no Matplotlib
import plotly.express as px  # Gr√°ficos interativos e visualiza√ß√µes din√¢micas

# -------------------------------------
# üìå Modelos de Machine Learning
# -------------------------------------
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor  # Modelos baseados em √°rvores de decis√£o
from sklearn.linear_model import LogisticRegression, LinearRegression  # Modelos lineares para classifica√ß√£o e regress√£o
from sklearn.svm import SVC, SVR  # Modelos de Support Vector Machine (SVM) para classifica√ß√£o e regress√£o
from sklearn.cluster import KMeans, AgglomerativeClustering  # Algoritmos de clustering
from sklearn.neighbors import KNeighborsClassifier  # Modelo de vizinhos mais pr√≥ximos (KNN)
from sklearn import svm, tree, neighbors  # Modelos adicionais do sklearn

# -------------------------------------
# üìå Sele√ß√£o de Features (Atributos)
# -------------------------------------
from mlxtend.feature_selection import SequentialFeatureSelector  # Sele√ß√£o sequencial de vari√°veis para otimizar modelos

# -------------------------------------
# üìå M√©tricas de Avalia√ß√£o
# -------------------------------------
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,  # M√©tricas para classifica√ß√£o
    confusion_matrix, classification_report, roc_auc_score,  # Matriz de confus√£o e an√°lise ROC
    mean_squared_error, mean_absolute_error, r2_score,  # M√©tricas para regress√£o
    silhouette_score, davies_bouldin_score, calinski_harabasz_score  # M√©tricas para clustering
)

# -------------------------------------
# üìå Pr√©-Processamento e Pipeline
# -------------------------------------
from sklearn.model_selection import (
    train_test_split,  # Separa√ß√£o entre dados de treino e teste
    KFold, LeaveOneOut, cross_val_score,  # Valida√ß√£o cruzada para avaliar modelos
    GridSearchCV  # Procura de melhores hiperpar√¢metros usando Grid Search
)
from sklearn.preprocessing import StandardScaler, LabelEncoder  # Normaliza√ß√£o e codifica√ß√£o de vari√°veis categ√≥ricas
from sklearn.impute import SimpleImputer  # Tratamento de valores ausentes

# -------------------------------------
# üìå Utilit√°rios Diversos
# -------------------------------------
import os  # Opera√ß√µes no sistema de arquivos (cria√ß√£o de pastas, leitura de arquivos)
import joblib  # Guardar e carregar  modelos treinados
import pickle  # Serializa√ß√£o e desserializa√ß√£o de objetos Python
import json  # Manipula√ß√£o de arquivos JSON
import requests  # Requisi√ß√µes HTTP para acesso a APIs externas
import unidecode  # Remo√ß√£o de acentos e normaliza√ß√£o de caracteres especiais

# -------------------------------------
# üìå Manipula√ß√£o de Arquivos e Dados Bin√°rios
# -------------------------------------
from io import BytesIO  # Manipula√ß√£o de streams bin√°rios para arquivos em mem√≥ria
import tempfile  # Cria√ß√£o de arquivos e diret√≥rios tempor√°rios

# -------------------------------------
# üìå Manipula√ß√£o de Datas e C√°lculos Matem√°ticos
# -------------------------------------
from datetime import datetime  # Manipula√ß√£o de datas e horas
from decimal import Decimal  # Precis√£o extra em c√°lculos decimais
from fractions import Fraction  # Trabalha com fra√ß√µes matem√°ticas exatas
from scipy.sparse import csr_matrix  # Representa√ß√£o eficiente de matrizes esparsas
import scipy  # Biblioteca cient√≠fica para estat√≠sticas, √°lgebra linear e otimiza√ß√£o
import time  # Medi√ß√£o do tempo de execu√ß√£o de processos

# -------------------------------------
# üìå Bibliotecas para Gera√ß√£o de Relat√≥rios
# -------------------------------------
from fpdf import FPDF  # Cria√ß√£o de documentos PDF programaticamente
from reportlab.lib.pagesizes import letter  # Defini√ß√£o do tamanho da p√°gina nos relat√≥rios
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle  # Estilos para formata√ß√£o de texto
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, Image  # Estrutura√ß√£o de documentos PDF
from reportlab.lib import colors  # Defini√ß√£o de cores em relat√≥rios
from reportlab.lib.units import inch  # Unidades de medida para layout de documentos



##############################################
# -------------------------------------
# üìå Fun√ß√£o JavaScript para voltar ao topo da p√°gina
# -------------------------------------

# Script JavaScript que permite rolar automaticamente para o topo da p√°gina
scroll_to_top_js = """
<script>
    function scrollToTop() {
        window.scrollTo(0, 0);  // Move a p√°gina para o topo (coordenadas 0,0)
    }
</script>
"""

# Insere o JavaScript na p√°gina com Streamlit
# Defini√ß√£o de height=0 e width=0 para evitar que o c√≥digo ocupe espa√ßo vis√≠vel na interface
components.html(scroll_to_top_js, height=0, width=0)  

# -------------------------------------
# üìå Ajustes de Exibi√ß√£o do Pandas Styler
# -------------------------------------

# Define o n√∫mero m√°ximo de elementos a serem renderizados no Styler do Pandas
pd.set_option("styler.render.max_elements", 2000000)  # Ajustar se necess√°rio para grandes DataFrames

# Configura a exibi√ß√£o de todas as linhas e colunas de um DataFrame
pd.set_option("display.max_rows", None)  # Permite visualizar todas as linhas sem truncamento
pd.set_option("display.max_columns", None)  # Permite visualizar todas as colunas sem truncamento


##############################################
def fix_dataframe_types(df):
    """Corrigir tipos de dados num DataFrame para compatibilidade com PyArrow"""

    # Verificar se o objeto √© um Styler e extrair o DataFrame
    if hasattr(df, 'data'):  # Objetos Styler possuem um atributo .data
        df = df.data
    elif hasattr(df, 'render') and not hasattr(df, 'copy'):  # Outra forma de identificar um Styler
        # Para vers√µes mais recentes do pandas
        if hasattr(df, '_data'):
            df = df._data
        # Para vers√µes ainda mais recentes do pandas, onde a estrutura pode ser diferente
        elif hasattr(df, 'data'):
            df = df.data
        # Se ainda n√£o for poss√≠vel extrair o DataFrame
        else:
            # Tentar converter primeiro para dicion√°rio e depois para DataFrame
            try:
                df = pd.DataFrame(df.to_dict())
            except:
                # Se falhar, retornar um DataFrame vazio
                return pd.DataFrame()
    
    # Se o objeto final n√£o for um DataFrame, retornar um DataFrame vazio
    if not isinstance(df, pd.DataFrame):
        return pd.DataFrame()
        
    # Criar uma c√≥pia do DataFrame para evitar modificar o original
    df_fixed = df.copy()
    
    # Percorrer todas as colunas para corrigir tipos de dados problem√°ticos
    for col in df_fixed.columns:
        # Converter colunas do tipo Int64 para int64 padr√£o (evita problemas de compatibilidade)
        if hasattr(df_fixed[col], 'dtype') and str(df_fixed[col].dtype) == 'Int64':
            df_fixed[col] = df_fixed[col].fillna(-1).astype('int64')  # Substituir valores nulos por -1 antes da convers√£o
        
        # Converter colunas do tipo objeto (strings e dados complexos) para string
        elif df_fixed[col].dtype == 'object':
            try:
                # Tentar converter diretamente para string
                df_fixed[col] = df_fixed[col].astype(str)
            except:
                # Se falhar, aplicar uma convers√£o manual, garantindo que valores None sejam tratados
                df_fixed[col] = df_fixed[col].apply(lambda x: str(x) if x is not None else "")
    
    # Retornar o DataFrame corrigido
    return df_fixed


##############################################
# -------------------------------------
# üìå Fun√ß√£o para Configurar a Barra Lateral
# -------------------------------------

def configure_sidebar():
    """Configura a barra lateral com o log√≥tipo da institui√ß√£o e informa√ß√µes sobre a plataforma."""
    
    with st.sidebar:  # Define que os elementos ser√£o adicionados na barra lateral
        st.image(
            "https://www.ipleiria.pt/normasgraficas/wp-content/uploads/sites/80/2017/09/estg_v-01.jpg",  # URL da imagem
            width=80,  # Define o tamanho da imagem (largura em pixels)
            caption="Log√≥tipo da Escola"  # Texto exibido abaixo da imagem
        )
        
        # Exibe o nome da plataforma em formato HTML para maior personaliza√ß√£o
        st.markdown("<p>MLCase - Plataforma de Machine Learning</p>", unsafe_allow_html=True)
        
        # Exibe o nome da autora com destaque em negrito usando HTML
        st.markdown("<p><b>Autora:</b> Bruna Sousa</p>", unsafe_allow_html=True)

# Chamada da fun√ß√£o para configurar a barra lateral
configure_sidebar()


##############################################
import matplotlib
matplotlib.use('Agg')  # Usar backend n√£o interativo
import matplotlib.pyplot as plt
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
##############################################

# -------------------------------------
# üìå FUN√á√ÉO DE UPLOAD DE FICHEIROS
# -------------------------------------

# Fun√ß√£o para inicializar vari√°veis de estado na aplica√ß√£o
def initialize_state():
    """Inicializa vari√°veis de estado utilizadas na aplica√ß√£o para gerir diferentes etapas do processo."""
    st.session_state.step = 'data_preview'  # Define o estado inicial como pr√©-visualiza√ß√£o dos dados
    st.session_state.selected_columns = []  # Lista para armazenar colunas selecionadas pelo utilizador
    st.session_state.numeric_types = {}  # Dicion√°rio para armazenar tipos num√©ricos das vari√°veis
    st.session_state.variable_types = {}  # Dicion√°rio para armazenar os tipos das vari√°veis
    st.session_state.treatment_state = {}  # Dicion√°rio para armazenar o estado do tratamento dos dados
    st.session_state.all_treated = False  # Flag para indicar se todos os dados foram tratados

# -------------------------------------
# üìå Fun√ß√£o auxiliar para escolher o delimitador de ficheiros CSV
# -------------------------------------

def choose_delimiter():
    """Permite ao utilizador escolher um delimitador para ficheiros CSV carregados."""
    
    # Lista de delimitadores comuns, incluindo a op√ß√£o personalizada
    delimiters = [",", ";", "\t", "|", "Outro"]
    
    # Cria um seletor na barra lateral para escolha do delimitador
    delimiter = st.sidebar.selectbox("Escolha o delimitador para CSV", delimiters, index=0)
    
    # Se o utilizador escolher a op√ß√£o "Outro", permite inserir um delimitador personalizado
    if delimiter == "Outro":
        delimiter = st.sidebar.text_input("Digite o delimitador personalizado:")
    
    return delimiter

# -------------------------------------
# üìå Fun√ß√£o para a etapa de upload do ficheiro
# -------------------------------------

def upload_file():
    """Permite ao utilizador carregar um ficheiro de dados para a plataforma."""
    
    st.title("MLCase - Plataforma de Machine Learning")  # T√≠tulo principal da aplica√ß√£o

    # Sele√ß√£o do tipo de ficheiro a ser carregado
    file_type = st.sidebar.selectbox("Selecione o tipo de arquivo", ["CSV", "Excel", "JSON"])
    delimiter = ","  # Define o delimitador padr√£o para CSV

    # Processo de upload conforme o tipo de ficheiro selecionado
    if file_type == "CSV":
        delimiter = choose_delimiter()  # Permite selecionar um delimitador para o CSV
        file = st.sidebar.file_uploader("Carregar arquivo", type=["csv"])  # Bot√£o de upload
    elif file_type == "Excel":
        file = st.sidebar.file_uploader("Carregar arquivo", type=["xlsx", "xls"])  # Upload de ficheiro Excel
    elif file_type == "JSON":
        file = st.sidebar.file_uploader("Carregar arquivo", type=["json"])  # Upload de ficheiro JSON

    # Se um ficheiro for carregado, tenta process√°-lo
    if file is not None:
        try:
            # Chama a fun√ß√£o de carregamento de dados e inicializa as vari√°veis de estado
            st.session_state.data = load_data(file_type, file, delimiter)
            initialize_state()
            st.sidebar.success(f"Conjunto de dados {file_type} carregado com sucesso!")  # Mensagem de sucesso

            # Bot√£o para avan√ßar para a pr√≥xima etapa (pr√©-visualiza√ß√£o dos dados)
            if st.sidebar.button("Dados Carregados"):
                st.session_state.step = 'data_preview'  # Atualiza o estado para a pr√©-visualiza√ß√£o
                st.stop()  # Para a execu√ß√£o para refletir as mudan√ßas

        except Exception as e:
            st.sidebar.error(f"Erro ao carregar o arquivo: {e}")  # Exibe mensagem de erro caso algo corra mal

# -------------------------------------
# üìå Fun√ß√£o para carregar dados com cache (evita recarregamento desnecess√°rio)
# -------------------------------------

@st.cache_data  # Usa cache para evitar recarregar os dados v√°rias vezes
def load_data(file_type, file, delimiter):
    """Carrega um ficheiro de dados conforme o tipo selecionado pelo utilizador."""
    
    if file_type == "CSV":
        return pd.read_csv(file, delimiter=delimiter)  # Carrega dados CSV com o delimitador escolhido
    elif file_type == "Excel":
        return pd.read_excel(file)  # Carrega ficheiro Excel
    elif file_type == "JSON":
        return pd.read_json(file)  # Carrega ficheiro JSON

##############################################
# -------------------------------------
# üìå FUN√á√ÉO DE SELE√á√ÉO DE COLUNAS
# -------------------------------------

# Fun√ß√£o para pr√©-visualizar os dados e permitir a sele√ß√£o de colunas e tipos de vari√°veis
def data_preview():
    """Permite visualizar os dados carregados, selecionar colunas e definir os seus tipos."""

    # Exibir uma pr√©-visualiza√ß√£o dos primeiros registos do dataset (com corre√ß√£o de tipos)
    st.subheader("Pr√©-visualiza√ß√£o dos dados")
    st.dataframe(fix_dataframe_types(st.session_state.data.head()))  # Corrige os tipos antes da exibi√ß√£o

    # Obter a lista de colunas do dataset
    columns = st.session_state.data.columns.tolist()

    # Criar uma caixa de sele√ß√£o m√∫ltipla para escolher quais colunas utilizar
    selected_columns = st.multiselect("Colunas", columns, columns)  # Por defeito, todas as colunas s√£o selecionadas
    st.session_state.selected_columns = selected_columns  # Guardar as colunas selecionadas no estado global

    # Preservar transforma√ß√µes no estado global
    if 'filtered_data' not in st.session_state:
        st.session_state.filtered_data = st.session_state.data.copy()  # Criar uma c√≥pia inicial dos dados
    else:
        # Atualizar os dados filtrados apenas com as colunas selecionadas, mantendo transforma√ß√µes j√° aplicadas
        st.session_state.filtered_data = st.session_state.data[selected_columns]

    # Se houver colunas selecionadas, permitir a identifica√ß√£o dos tipos de vari√°veis
    if selected_columns:
        st.subheader("Identificar tipos de vari√°veis")

        # Inicializar dicion√°rio para armazenar os tipos de vari√°veis, caso ainda n√£o exista
        if 'variable_types' not in st.session_state:
            st.session_state.variable_types = {}

        variable_types = st.session_state.variable_types
        st.session_state.numeric_types = {}  # Dicion√°rio para armazenar os tipos num√©ricos

        # Percorrer cada coluna selecionada para definir os tipos de vari√°veis
        for col in selected_columns:
            # Criar um seletor para definir se a vari√°vel √© Num√©rica, Categ√≥rica ou Data
            var_type = st.selectbox(
                f"Tipo de vari√°vel para {col}",
                ["Num√©rica", "Categ√≥rica", "Data"],
                index=0 if pd.api.types.is_numeric_dtype(st.session_state.filtered_data[col]) else 1,
                key=f"var_{col}"  # Cada seletor tem uma chave √∫nica para evitar conflitos
            )
            variable_types[col] = var_type  # Guardar o tipo selecionado

            # Se a vari√°vel for num√©rica, permitir configurar o tipo espec√≠fico
            if var_type == "Num√©rica":
                num_type = st.selectbox(
                    f"Tipo num√©rico para {col}",
                    ["Int", "Float", "Complex", "Dec", "Frac", "Bool"],
                    index=0 if pd.api.types.is_integer_dtype(st.session_state.filtered_data[col]) else 1,
                    key=f"num_{col}"  # Chave √∫nica para o seletor de tipo num√©rico
                )
                st.session_state.numeric_types[col] = num_type  # Guardar o tipo num√©rico no estado global

                # Discretiza√ß√£o da vari√°vel (convers√£o para categorias)
                # Verifica primeiro se a coluna j√° foi discretizada
                if col not in st.session_state.filtered_data.columns or pd.api.types.is_numeric_dtype(st.session_state.filtered_data[col]):
                    if st.checkbox(f"Discretizar {col}?", key=f"discretize_{col}"):
                        discretize_column(col)  # Aplica a fun√ß√£o de discretiza√ß√£o
                else:
                    st.write(f"Coluna {col} j√° foi discretizada.")  # Informa√ß√£o para o utilizador

        # Atualizar o estado global com os tipos de vari√°veis definidos
        st.session_state.variable_types = variable_types

    # Criar uma c√≥pia dos dados filtrados para manter altera√ß√µes recentes
    st.session_state.filtered_data = st.session_state.filtered_data.copy()

    # -------------------------------------
    # üìå Navega√ß√£o entre etapas
    # -------------------------------------

    col1, col2 = st.columns(2)  # Criar duas colunas para os bot√µes "Voltar" e "Pr√≥xima etapa"

    # Bot√£o para voltar √† etapa anterior
    with col1:
        if st.button("Voltar"):
            # Apagar estados salvos explicitamente para evitar conflitos
            keys_to_reset = [
                'filtered_data', 'selected_columns', 'variable_types',
                'numeric_types', 'treatment_state'
            ]
            for key in keys_to_reset:
                st.session_state.pop(key, None)  # Remove do estado se existir

            # Restaurar os dados originais
            st.session_state.data = st.session_state.data.copy()

            # Voltar para a etapa de upload do ficheiro
            st.session_state.step = 'file_upload'
            st.rerun()  # Recarregar a aplica√ß√£o para refletir as mudan√ßas

    # Bot√£o para avan√ßar para a pr√≥xima etapa
    with col2:
        if st.button("Pr√≥xima etapa"):
            apply_numeric_types()  # Aplicar os tipos num√©ricos definidos pelo utilizador
            st.session_state.step = 'missing_values'  # Atualizar o estado para a etapa seguinte
            st.rerun()  # Recarregar a aplica√ß√£o para refletir as altera√ß√µes


# -------------------------------------
# üìå Fun√ß√£o para Aplicar Tipos Num√©ricos √†s Colunas Filtradas
# -------------------------------------

def apply_numeric_types():
    """Aplica os tipos num√©ricos definidos pelo utilizador √†s colunas filtradas no dataset."""
    
    # Percorre todas as colunas que t√™m tipos num√©ricos definidos pelo utilizador
    for col, num_type in st.session_state.numeric_types.items():
        # Verifica se a coluna ainda existe no conjunto de dados filtrado
        if col in st.session_state.filtered_data.columns:
            # Converte a coluna para o tipo num√©rico selecionado
            st.session_state.filtered_data[col] = convert_numeric_type(st.session_state.filtered_data[col], num_type)

# -------------------------------------
# üìå Fun√ß√£o para Convers√£o de Tipos de Dados Num√©ricos
# -------------------------------------

def convert_numeric_type(series, num_type):
    """
    Converte uma s√©rie de dados para o tipo num√©rico especificado.
    
    Par√¢metros:
    - series: pd.Series -> Coluna do DataFrame a ser convertida.
    - num_type: str -> Tipo num√©rico desejado ("Int", "Float", "Complex", "Dec", "Frac", "Bool", "Date", "Duration").

    Retorna:
    - pd.Series convertida para o tipo especificado ou a mesma s√©rie original caso ocorra um erro.
    """
    
    try:
        # Convers√£o para n√∫mero inteiro (Int64)
        if num_type == "Int":
            return pd.to_numeric(series, errors='coerce').astype('Int64')  # Mant√©m valores nulos compat√≠veis com Pandas

        # Convers√£o para n√∫mero decimal (Float)
        elif num_type == "Float":
            return pd.to_numeric(series, errors='coerce').astype(float)

        # Convers√£o para n√∫mero complexo
        elif num_type == "Complex":
            return pd.to_numeric(series, errors='coerce').apply(lambda x: complex(x) if pd.notnull(x) else np.nan)

        # Convers√£o para Decimal (melhor precis√£o para c√°lculos financeiros)
        elif num_type == "Dec":
            return series.apply(lambda x: Decimal(x) if pd.notnull(x) else np.nan)

        # Convers√£o para Fra√ß√£o (representa√ß√£o matem√°tica exata)
        elif num_type == "Frac":
            return series.apply(lambda x: Fraction(x) if pd.notnull(x) else np.nan)

        # Convers√£o para Booleano (True/False)
        elif num_type == "Bool":
            return series.apply(lambda x: str(x).strip().lower() in ['true', '1'])

        # Convers√£o para Data/Hora
        elif num_type == "Date":
            return pd.to_datetime(series, errors='coerce')

        # Convers√£o para Dura√ß√£o/Intervalo de Tempo
        elif num_type == "Duration":
            return pd.to_timedelta(series, errors='coerce')

        # Se o tipo especificado n√£o estiver listado, retorna a s√©rie original sem altera√ß√µes
        else:
            return series

    except Exception as e:
        # Exibe um erro no Streamlit caso ocorra um problema na convers√£o
        st.error(f"Erro ao converter coluna {series.name} para tipo {num_type}: {e}")


# -------------------------------------
# üìå Fun√ß√£o para Discretizar uma Coluna Num√©rica
# -------------------------------------

def discretize_column(col):
    """Permite ao utilizador discretizar uma coluna num√©rica, transformando-a em categorias definidas manualmente."""

    # -------------------------------------
    # üìå Se√ß√£o de Ajuda - Explica√ß√£o sobre Discretiza√ß√£o
    # -------------------------------------
    
    # Explica√ß√£o interativa sobre como definir bins (intervalos) e labels (categorias)
    with st.expander("Como preencher os bins e labels?"):
        st.write("**Bins:** Intervalos num√©ricos para discretiza√ß√£o.")
        st.write("**Labels:** Nomeiam os intervalos.")
        st.write("**Exemplo:**")
        st.write("- **Bins:** -2,1,2,6,inf")
        st.write("- **Labels:** Baixo, M√©dio, Alto, Muito Alto")

    # -------------------------------------
    # üìå Diagn√≥stico Inicial Antes da Discretiza√ß√£o
    # -------------------------------------

    st.write("### Diagn√≥stico antes da discretiza√ß√£o:")
    st.write(f"- **M√≠nimo:** {st.session_state.filtered_data[col].min()}")  # Valor m√≠nimo da coluna
    st.write(f"- **M√°ximo:** {st.session_state.filtered_data[col].max()}")  # Valor m√°ximo da coluna
    st.write(f"- **M√©dia:** {st.session_state.filtered_data[col].mean():.2f}")  # M√©dia da coluna
    st.write(f"- **Mediana:** {st.session_state.filtered_data[col].median():.2f}")  # Mediana da coluna
    st.write(f"- **Valores ausentes antes:** {st.session_state.filtered_data[col].isna().sum()}")  # Contagem de valores nulos

    # -------------------------------------
    # üìå Entrada de Dados do Utilizador (Bins e Labels)
    # -------------------------------------

    # Caixa de texto para o utilizador inserir os bins (intervalos num√©ricos)
    bins_input = st.text_input(
        f"Digite os bins para {col} (separados por v√≠rgulas)",
        value="-2,1,2,6,inf", key=f"bins_{col}"
    )

    # Caixa de texto para o utilizador inserir os labels (nomes das categorias correspondentes aos bins)
    labels_input = st.text_input(
        f"Digite os labels para {col} (separados por v√≠rgulas)",
        value="Baixo,M√©dio,Alto,Muito Alto", key=f"labels_{col}"
    )

    # -------------------------------------
    # üìå Aplica√ß√£o da Discretiza√ß√£o Ap√≥s Confirma√ß√£o
    # -------------------------------------

    # Se o utilizador clicar no bot√£o, iniciar a convers√£o
    if st.button(f"Confirmar Discretiza√ß√£o para {col}", key=f"confirm_{col}"):

        # Verificar se o utilizador preencheu os bins e labels corretamente
        if bins_input and labels_input:
            try:
                # Converter a string de bins para uma lista de valores num√©ricos (float)
                bins = list(map(float, bins_input.split(',')))

                # Converter a string de labels para uma lista de nomes de categorias
                labels = labels_input.split(',')

                # -------------------------------------
                # üìå Valida√ß√£o de Dados Antes da Convers√£o
                # -------------------------------------

                # O n√∫mero de labels deve ser igual ao n√∫mero de bins menos um
                if len(labels) != len(bins) - 1:
                    st.error(f"O n√∫mero de labels deve ser igual ao n√∫mero de bins menos um para a coluna {col}.")

                else:
                    # Converter a coluna para tipo num√©rico para evitar erros
                    st.session_state.filtered_data[col] = pd.to_numeric(
                        st.session_state.filtered_data[col], errors='coerce'
                    )

                    # Preencher valores ausentes com a mediana da coluna
                    median_value = st.session_state.filtered_data[col].median()
                    st.session_state.filtered_data[col].fillna(median_value, inplace=True)

                    # Diagn√≥stico ap√≥s preenchimento de valores ausentes
                    st.write(f"Valores ausentes ap√≥s preenchimento: {st.session_state.filtered_data[col].isna().sum()}")

                    # -------------------------------------
                    # üìå Aplica√ß√£o da Discretiza√ß√£o
                    # -------------------------------------

                    # Criar categorias com base nos bins e labels definidos pelo utilizador
                    categorized = pd.cut(
                        st.session_state.filtered_data[col],  # Coluna de dados a ser discretizada
                        bins=bins,  # Intervalos definidos
                        labels=labels,  # Nomes das categorias correspondentes
                        include_lowest=True  # Inclui o menor valor nos intervalos
                    )

                    # Converter para tipo categ√≥rico
                    categorized = categorized.astype('category')

                    # Adicionar uma categoria extra para valores fora do intervalo definido
                    categorized = categorized.cat.add_categories(["Fora do Intervalo"])
                    categorized = categorized.fillna("Fora do Intervalo")  # Substituir valores n√£o categorizados

                    # -------------------------------------
                    # üìå Atualiza√ß√£o do Estado Global e Diagn√≥stico Final
                    # -------------------------------------

                    # Salvar a coluna discretizada no dataset filtrado
                    st.session_state.filtered_data[col] = categorized

                    # Criar uma nova c√≥pia do dataset para garantir a consist√™ncia dos dados
                    st.session_state.filtered_data = st.session_state.filtered_data.copy()

                    # Mensagem de sucesso
                    st.success(f"Coluna {col} discretizada com sucesso!")

                    # Exibir o tipo de dados final da coluna
                    st.write(st.session_state.filtered_data[col].dtype)

                    # Exibir as categorias √∫nicas geradas
                    st.write(st.session_state.filtered_data[col].unique())

                    # Exibir uma pr√©-visualiza√ß√£o dos dados ap√≥s a discretiza√ß√£o
                    st.write("Pr√©-visualiza√ß√£o dos dados ap√≥s discretiza√ß√£o:")
                    st.dataframe(fix_dataframe_types(st.session_state.filtered_data.head()))

            except ValueError as e:
                # Mensagem de erro caso a convers√£o falhe
                st.error(f"Erro ao discretizar {col}: {e}")



##############################################
# -------------------------------------
# üìå FUN√á√ÉO DE TRATAMENTO DE VALORES OMISSOS (MISSING VALUES)
# -------------------------------------

# -------------------------------------
# üìå Fun√ß√£o para destacar valores ausentes no DataFrame
# -------------------------------------

def highlight_missing():
    """Aplica um estilo ao DataFrame, destacando c√©lulas com valores ausentes em amarelo."""

    # Fun√ß√£o interna que aplica a cor amarela √†s c√©lulas com valores nulos (NaN)
    def highlight_na(s):
        return ['background-color: yellow' if pd.isnull(v) else '' for v in s]

    # Aplica o estilo ao DataFrame filtrado e retorna o objeto Styler
    return st.session_state.filtered_data.style.apply(highlight_na, subset=st.session_state.filtered_data.columns)

# -------------------------------------
# üìå Fun√ß√£o para formatar valores na tabela
# -------------------------------------

def format_table():
    """Formata os valores do DataFrame para exibi√ß√£o, ajustando casas decimais e representa√ß√µes de NaN."""
    
    # Criar uma c√≥pia do DataFrame para evitar modificar os dados originais
    formatted_df = st.session_state.filtered_data.copy()

    # Iterar sobre todas as colunas do DataFrame
    for col in formatted_df.columns:
        # Verificar se a coluna cont√©m valores num√©ricos
        if pd.api.types.is_numeric_dtype(formatted_df[col]):
            # Formatar os valores num√©ricos para exibi√ß√£o com 2 casas decimais
            formatted_df[col] = formatted_df[col].map(lambda x: f"{x:.2f}" if pd.notnull(x) else 'NaN')

    return formatted_df  # Retorna o DataFrame formatado

# -------------------------------------
# üìå Fun√ß√£o para exibir a pr√©-visualiza√ß√£o dos dados com tipos de vari√°veis
# -------------------------------------

def show_preview_with_types(variable_types):
    """Exibe os dados com uma pr√©-visualiza√ß√£o dos tipos de vari√°veis identificados."""

    # T√≠tulo da se√ß√£o
    st.subheader("Pr√©-visualiza√ß√£o dos dados com tipos de vari√°veis")

    # Exibir os tipos de vari√°veis definidos pelo utilizador
    st.write("Tipos de vari√°veis:")
    st.write(variable_types)

    # Formatar os dados antes da exibi√ß√£o
    formatted_df = format_table()

    # Aplicar destaque para valores ausentes e corrigir tipos de dados antes de exibir
    st.dataframe(fix_dataframe_types(highlight_missing(formatted_df)))


# -------------------------------------
# üìå Fun√ß√£o para Aplicar Tratamento de Valores Ausentes
# -------------------------------------

def apply_missing_value_treatment(column, method, constant_value=None):
    """Aplica um tratamento espec√≠fico para valores ausentes numa coluna selecionada do dataset."""

    # Usa diretamente os dados filtrados armazenados no estado global
    data = st.session_state.filtered_data

    # Verifica se a coluna √© num√©rica
    if pd.api.types.is_numeric_dtype(data[column]):
        # Substituir valores ausentes pela m√©dia da coluna
        if method == "M√©dia":
            data[column].fillna(data[column].mean(), inplace=True)

        # Substituir valores ausentes pela mediana da coluna
        elif method == "Mediana":
            data[column].fillna(data[column].median(), inplace=True)

        # Substituir valores ausentes pela moda (valor mais frequente) da coluna
        elif method == "Moda":
            data[column].fillna(data[column].mode().iloc[0], inplace=True)

        # Excluir linhas onde h√° valores ausentes nesta coluna
        elif method == "Excluir":
            data.dropna(subset=[column], inplace=True)

        # Substituir por um valor constante definido pelo utilizador
        elif method == "Valor constante" and constant_value is not None:
            data[column].fillna(constant_value, inplace=True)

    # Se a coluna for categ√≥rica (texto, categorias, etc.)
    else:
        # Substituir valores ausentes pela moda (valor mais frequente)
        if method == "Substituir por moda":
            data[column].fillna(data[column].mode().iloc[0], inplace=True)

        # Substituir valores ausentes por um valor fixo definido pelo utilizador
        elif method == "Substituir por valor constante" and constant_value is not None:
            data[column].fillna(constant_value, inplace=True)

        # N√£o faz nada (mant√©m os valores ausentes)
        elif method == "Manter valores ausentes":
            pass  

        # Excluir linhas com valores ausentes nesta coluna
        elif method == "Excluir":
            data.dropna(subset=[column], inplace=True)

    # Atualiza os dados processados no estado global
    st.session_state.filtered_data = data

# -------------------------------------
# üìå Fun√ß√£o para Selecionar Automaticamente o M√©todo de Tratamento de Valores Ausentes
# -------------------------------------

def auto_select_method(column_name):
    """Seleciona automaticamente o melhor m√©todo para tratar valores ausentes numa coluna."""

    # Obt√©m a coluna a partir dos dados filtrados
    column = st.session_state.filtered_data[column_name]

    # Calcula a percentagem de valores ausentes na coluna
    missing_percentage = column.isnull().sum() / len(column)

    # Para colunas num√©ricas
    if pd.api.types.is_numeric_dtype(column):
        if missing_percentage > 0.5:
            return "Excluir"  # Se mais de 50% dos valores est√£o ausentes, sugere excluir a coluna
        else:
            return "Substituir por Mediana"  # Caso contr√°rio, sugere substituir pela mediana

    # Para colunas categ√≥ricas (texto, categorias)
    else:
        if missing_percentage > 0.5:
            return "Excluir"  # Se mais de 50% dos valores est√£o ausentes, sugere excluir a coluna
        else:
            return "Substituir por Moda"  # Caso contr√°rio, sugere substituir pela moda (valor mais frequente)

# -------------------------------------
# üìå Fun√ß√£o para Exibir Tabela com Valores Ausentes
# -------------------------------------

def display_missing_values(dataframe):
    """Exibe uma tabela com a contagem de valores ausentes em cada coluna do dataset."""

    # Conta o n√∫mero de valores ausentes por coluna
    missing_data = dataframe.isnull().sum()

    # Mant√©m apenas as colunas que possuem valores ausentes
    missing_data = missing_data[missing_data > 0]
    
    # Converte para DataFrame para melhor visualiza√ß√£o
    missing_data = missing_data.reset_index()
    missing_data.columns = ['Coluna', 'Valores Ausentes']

    # Se houver valores ausentes, exibir a tabela
    if not missing_data.empty:
        st.write("Tabela de valores ausentes:")
        st.dataframe(fix_dataframe_types(missing_data))  # Aplica corre√ß√µes de tipo antes de exibir
    else:
        st.write("N√£o h√° valores ausentes.")  # Mensagem caso n√£o existam valores em falta

# -------------------------------------
# üìå FUN√á√ÉO PARA MOSTRAR E TRATAR VALORES AUSENTES
# -------------------------------------

def handle_missing_values():
    """Gerencia o tratamento de valores ausentes no dataset carregado."""

    # Exibe o t√≠tulo da se√ß√£o no Streamlit
    st.subheader("Tratamento de Valores Ausentes")

    # Obt√©m os dados filtrados armazenados no estado da sess√£o
    filtered_data = st.session_state.get('filtered_data', None)

    # -------------------------------------
    # üìå Verifica√ß√£o Inicial dos Dados
    # -------------------------------------

    # Verifica se h√° dados carregados e n√£o est√£o vazios
    if filtered_data is not None and not filtered_data.empty:

        # -------------------------------------
        # üìå Fun√ß√£o Interna para Exibir Valores Ausentes
        # -------------------------------------

        def display_missing_values(df):
            """Gera uma tabela resumida com a contagem de valores ausentes por coluna."""

            # Conta a quantidade de valores ausentes em cada coluna
            missing_data = df.isnull().sum()

            # Mant√©m apenas as colunas que possuem valores ausentes
            missing_data = missing_data[missing_data > 0]

            # Exibe os valores ausentes caso existam
            if not missing_data.empty:
                st.write("Resumo dos Valores Ausentes:")
                st.dataframe(fix_dataframe_types(missing_data.rename("Total de Valores Ausentes")))
            else:
                st.success("N√£o h√° valores ausentes nos dados.")  # Exibe uma mensagem caso n√£o haja valores ausentes

        # Exibir o resumo dos valores ausentes no dataset
        display_missing_values(filtered_data)

        # -------------------------------------
        # üìå Configura√ß√£o das Op√ß√µes de Tratamento de Valores Ausentes
        # -------------------------------------

        # Verifica se existem valores ausentes em qualquer coluna
        has_missing_values = filtered_data.isnull().any().any()

        if has_missing_values:
            # Inicializar dicion√°rio de tratamento no estado global, caso ainda n√£o exista
            if 'treatment_state' not in st.session_state:
                st.session_state.treatment_state = {
                    col: {"method": None, "constant": None}
                    for col in filtered_data.columns
                }

            # Percorre cada coluna que possui valores ausentes para exibir op√ß√µes de tratamento
            for col in filtered_data.columns:
                if filtered_data[col].isnull().sum() > 0:
                    col_state = st.session_state.treatment_state.get(col, {"method": None, "constant": None})
                    is_numeric = pd.api.types.is_numeric_dtype(filtered_data[col])

                    # -------------------------------------
                    # üìå Tratamento de Valores Ausentes em Colunas Num√©ricas
                    # -------------------------------------

                    if is_numeric:
                        # Op√ß√µes dispon√≠veis para tratamento de valores ausentes em vari√°veis num√©ricas
                        options = ["Substituir por M√©dia", "Substituir por Mediana", "Substituir por Moda", 
                                   "Substituir por Valor Constante", "Excluir", "Manter Valores Ausentes"]
                        
                        # Seletor para escolher o m√©todo de tratamento
                        missing_value_method = st.selectbox(
                            f"M√©todo para tratar valores ausentes em {col}",
                            options,
                            index=options.index(col_state["method"]) if col_state["method"] in options else 0,
                            key=f"missing_value_{col}"
                        )

                        # Definir valor constante caso o utilizador escolha essa op√ß√£o
                        constant_value = None
                        if missing_value_method == "Substituir por Valor Constante":
                            constant_value = st.text_input(
                                f"Digite o valor constante para {col}:",
                                value=col_state["constant"] if col_state["constant"] else '',
                                key=f"constant_{col}"
                            )

                    # -------------------------------------
                    # üìå Tratamento de Valores Ausentes em Colunas Categ√≥ricas
                    # -------------------------------------

                    else:
                        # Op√ß√µes dispon√≠veis para colunas categ√≥ricas
                        options = ["Substituir por Moda", "Substituir por Valor Constante", "Manter Valores Ausentes", "Excluir"]
                        
                        # Seletor para escolher o m√©todo de tratamento
                        missing_value_method = st.selectbox(
                            f"M√©todo para tratar valores ausentes em {col}",
                            options,
                            index=options.index(col_state["method"]) if col_state["method"] in options else 0,
                            key=f"cat_missing_value_{col}"
                        )

                        # Definir valor constante caso o utilizador escolha essa op√ß√£o
                        constant_value = None
                        if missing_value_method == "Substituir por Valor Constante":
                            constant_value = st.text_input(
                                f"Digite o valor constante para {col}:",
                                value=col_state["constant"] if col_state["constant"] else '',
                                key=f"cat_constant_{col}"
                            )

                    # Atualizar o estado global com as escolhas do utilizador para essa coluna
                    st.session_state.treatment_state[col] = {"method": missing_value_method, "constant": constant_value}

            # -------------------------------------
            # üìå Aplica√ß√£o dos Tratamentos Escolhidos
            # -------------------------------------

            if st.button("Aplicar tratamentos"):
                for col, treatment in st.session_state.treatment_state.items():
                    method = treatment["method"]
                    constant_value = treatment["constant"]

                    # Aplicar o m√©todo selecionado para tratamento dos valores ausentes
                    if method == "Substituir por M√©dia":
                        filtered_data[col].fillna(filtered_data[col].mean(), inplace=True)
                    elif method == "Substituir por Mediana":
                        filtered_data[col].fillna(filtered_data[col].median(), inplace=True)
                    elif method == "Substituir por Moda":
                        filtered_data[col].fillna(filtered_data[col].mode().iloc[0], inplace=True)
                    elif method == "Substituir por Valor Constante" and constant_value is not None:
                        filtered_data[col].fillna(constant_value, inplace=True)
                    elif method == "Excluir":
                        filtered_data.dropna(subset=[col], inplace=True)

                # Atualizar os dados processados no estado global
                st.session_state.data = filtered_data.copy()

                # Mensagem de sucesso
                st.success("Tratamentos aplicados com sucesso!")

        # -------------------------------------
        # üìå Navega√ß√£o entre Etapas
        # -------------------------------------

        col1, col2 = st.columns(2)

        # Bot√£o para voltar √† etapa anterior
        with col1:
            if st.button("Voltar"):
                st.session_state.step = 'data_preview'
                st.rerun()

        # Bot√£o para avan√ßar para a pr√≥xima etapa
        with col2:
            if st.button("Pr√≥xima etapa"):
                st.session_state.step = 'outlier_detection'
                st.rerun()

    else:
        # Caso n√£o haja dados dispon√≠veis, exibir uma mensagem de erro
        st.error("Nenhum dado dispon√≠vel para tratamento de valores ausentes.")


##############################################
# -------------------------------------
# üìå FUN√á√ÉO DE TRATAMENTO DE OUTLIERS (VALORES EXTREMOS)
# -------------------------------------

# -------------------------------------
# üìå Fun√ß√£o para Detetar e Calcular Informa√ß√µes sobre Outliers
# -------------------------------------

@st.cache_data  # Usa cache para evitar rec√°lculo desnecess√°rio ao interagir com a aplica√ß√£o
def calculate_outliers(columns, data):
    """
    Identifica e calcula estat√≠sticas sobre outliers em vari√°veis num√©ricas.

    Par√¢metros:
    - columns: lista com os nomes das colunas a serem analisadas.
    - data: DataFrame contendo os dados.

    Retorna:
    - variables_with_outliers: Lista com as vari√°veis que possuem outliers.
    - outlier_summary: Lista de dicion√°rios com informa√ß√µes detalhadas sobre os outliers identificados.
    """

    # Lista para armazenar os nomes das vari√°veis que cont√™m outliers
    variables_with_outliers = []

    # Lista para armazenar o resumo estat√≠stico dos outliers encontrados
    outlier_summary = []

    # Percorre todas as colunas selecionadas para an√°lise de outliers
    for col in columns:
        # Verifica se a coluna cont√©m dados num√©ricos antes de continuar a an√°lise
        if pd.api.types.is_numeric_dtype(data[col]):

            # -------------------------------------
            # üìå C√°lculo do Intervalo Interquartil (IQR)
            # -------------------------------------

            # Primeiro quartil (Q1) - 25% dos dados est√£o abaixo deste valor
            Q1 = data[col].quantile(0.25)

            # Terceiro quartil (Q3) - 75% dos dados est√£o abaixo deste valor
            Q3 = data[col].quantile(0.75)

            # Intervalo Interquartil (IQR) - Diferen√ßa entre Q3 e Q1
            IQR = Q3 - Q1

            # Defini√ß√£o dos limites para dete√ß√£o de outliers
            lower_bound = Q1 - 1.5 * IQR  # Limite inferior
            upper_bound = Q3 + 1.5 * IQR  # Limite superior

            # -------------------------------------
            # üìå Identifica√ß√£o de Outliers
            # -------------------------------------

            # Contagem de outliers, ou seja, valores que est√£o abaixo do limite inferior ou acima do superior
            num_outliers = len(data[(data[col] < lower_bound) | (data[col] > upper_bound)])

            # Se forem encontrados outliers na coluna, armazenar os resultados
            if num_outliers > 0:
                # Calcular a percentagem de outliers em rela√ß√£o ao total de dados na vari√°vel
                percentage_outliers = (num_outliers / len(data[col])) * 100

                # Adicionar o nome da vari√°vel √† lista de vari√°veis com outliers
                variables_with_outliers.append(col)

                # Criar um dicion√°rio com o resumo estat√≠stico dos outliers na vari√°vel analisada
                outlier_summary.append({
                    "Vari√°vel": col,
                    "Total de Outliers": num_outliers,
                    "Percentagem de Outliers (%)": round(percentage_outliers, 2)
                })

    # Retorna a lista de vari√°veis que possuem outliers e o resumo estat√≠stico
    return variables_with_outliers, outlier_summary


# Interface de detec√ß√£o e tratamento de outliers
# -------------------------------------
# üìå FUN√á√ÉO DE DETE√á√ÉO E TRATAMENTO DE OUTLIERS
# -------------------------------------

def outlier_detection():
    """Realiza a dete√ß√£o e o tratamento de outliers (valores extremos) em vari√°veis num√©ricas do dataset."""

    # Exibir o t√≠tulo da se√ß√£o no Streamlit
    st.subheader("Dete√ß√£o de Outliers")

    # -------------------------------------
    # üìå Armazenamento dos Dados Originais
    # -------------------------------------

    # Se for a primeira execu√ß√£o, armazenar uma c√≥pia dos dados originais
    if 'original_data' not in st.session_state:
        st.session_state.original_data = st.session_state.data.copy()

    # -------------------------------------
    # üìå Boxplot Inicial (Visualiza√ß√£o dos Dados Antes do Tratamento)
    # -------------------------------------

    st.write("### Boxplot Inicial (Dados Originais)")
    fig, ax = plt.subplots(figsize=(12, 6))
    st.session_state.original_data.boxplot(ax=ax)  # Criar boxplot para visualizar outliers
    plt.xticks(rotation=45)  # Ajustar rota√ß√£o dos r√≥tulos do eixo X
    st.pyplot(fig)  # Exibir gr√°fico no Streamlit

    # -------------------------------------
    # üìå Inicializar Estados Globais Necess√°rios
    # -------------------------------------

    # Armazena colunas que j√° passaram por tratamento
    if 'treated_columns' not in st.session_state:
        st.session_state.treated_columns = []

    # Armazena detalhes sobre os outliers identificados
    if 'outlier_details' not in st.session_state:
        st.session_state.outlier_details = {}

    # Armazena os limites iniciais dos outliers (antes do tratamento)
    if 'initial_limits' not in st.session_state:
        st.session_state.initial_limits = {}

    # Lista de colunas que possuem outliers
    if 'columns_with_outliers' not in st.session_state:
        st.session_state.columns_with_outliers = []

    # Estado global para armazenar as decis√µes do utilizador sobre tratamento de outliers
    if 'outlier_treatment_state' not in st.session_state:
        st.session_state.outlier_treatment_state = {}

    # Flag para indicar se todos os outliers foram tratados
    if 'all_outliers_treated' not in st.session_state:
        st.session_state.all_outliers_treated = False

    # -------------------------------------
    # üìå Verifica√ß√£o da Disponibilidade dos Dados
    # -------------------------------------

    if 'data' not in st.session_state or st.session_state.data is None:
        st.error("Os dados n√£o est√£o carregados! Volte para a etapa anterior.")
        return

    # -------------------------------------
    # üìå Identifica√ß√£o de Outliers
    # -------------------------------------

    # Selecionar apenas as colunas num√©ricas do dataset
    numeric_columns = list(st.session_state.data.select_dtypes(include=[np.number]).columns)

    # Lista para armazenar resumo dos outliers
    outlier_summary = []

    # Percorrer todas as colunas num√©ricas para calcular limites e identificar outliers
    for col in numeric_columns:

        # Ignorar colunas que j√° foram tratadas
        if col in st.session_state.treated_columns:
            continue

        # Calcular o primeiro quartil (Q1) e o terceiro quartil (Q3)
        Q1 = st.session_state.data[col].quantile(0.25)
        Q3 = st.session_state.data[col].quantile(0.75)

        # Calcular o intervalo interquartil (IQR)
        IQR = Q3 - Q1

        # Definir limites inferior e superior para identifica√ß√£o de outliers
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Contar outliers normais (fora do intervalo IQR)
        total_outliers = len(st.session_state.data[(st.session_state.data[col] < lower_bound) | 
                                                   (st.session_state.data[col] > upper_bound)])

        # Contar outliers severos (fora do intervalo 3*IQR)
        total_severe_outliers = len(st.session_state.data[(st.session_state.data[col] < (Q1 - 3.0 * IQR)) | 
                                                           (st.session_state.data[col] > (Q3 + 3.0 * IQR))])

        # Se a vari√°vel contiver outliers, armazenar detalhes
        if total_outliers > 0:
            st.session_state.initial_limits[col] = {
                "lower_bound": lower_bound,
                "upper_bound": upper_bound,
            }

            st.session_state.outlier_details[col] = {
                "total_outliers": total_outliers,
                "total_severe_outliers": total_severe_outliers,
                "skewness": st.session_state.data[col].skew()  # Assimetria da distribui√ß√£o
            }

            # Adicionar ao resumo estat√≠stico
            outlier_summary.append({
                "Nome vari√°vel": col,
                "Total de outliers": total_outliers,
                "Total de outliers severos": total_severe_outliers
            })

            # Adicionar √† lista de colunas com outliers
            if col not in st.session_state.columns_with_outliers:
                st.session_state.columns_with_outliers.append(col)

    # Salvar o resumo inicial no estado global
    st.session_state.initial_outlier_summary = outlier_summary

    # -------------------------------------
    # üìå Verificar se Restam Outliers para Tratar
    # -------------------------------------

    remaining_outliers = [col for col in st.session_state.columns_with_outliers 
                          if col not in st.session_state.treated_columns]

    if not remaining_outliers:
        if not outlier_summary and not st.session_state.columns_with_outliers:
            st.success("Nenhum outlier detetado nas vari√°veis num√©ricas!")
        else:
            st.success("Todos os outliers detetados foram tratados!")
    else:
        st.write("Resumo dos Outliers:")
        st.dataframe(fix_dataframe_types(pd.DataFrame(outlier_summary)))

    # -------------------------------------
    # üìå Exibi√ß√£o e Tratamento de Outliers Restantes
    # -------------------------------------

    for col in remaining_outliers:
        st.write(f"**Diagn√≥stico para {col}:**")
        details = st.session_state.outlier_details[col]
        st.write(f"- Total de Registos: {len(st.session_state.data)}")
        st.write(f"- Outliers: {details['total_outliers']} ({(details['total_outliers'] / len(st.session_state.data)):.2%})")
        st.write(f"- Outliers Severos: {details['total_severe_outliers']} ({(details['total_severe_outliers'] / len(st.session_state.data)):.2%})")
        st.write(f"- Assimetria (Skewness): {details['skewness']:.2f}")

        # Sugest√£o autom√°tica de m√©todo de tratamento
        if col not in st.session_state.outlier_treatment_state:
            suggested_method = auto_select_outlier_treatment(
                col, st.session_state.data, st.session_state.initial_limits[col]["lower_bound"], st.session_state.initial_limits[col]["upper_bound"]
            )
            st.session_state.outlier_treatment_state[col] = suggested_method

        # Seletor de m√©todo de tratamento
        method = st.selectbox(
            f"Selecione o m√©todo para tratar outliers em {col}",
            ["Sem A√ß√£o", "Remover Outliers", "Remover Outliers Severos", "Substituir por Limites", "Substituir por M√©dia", "Substituir por Mediana"],
            index=["Sem A√ß√£o", "Remover Outliers", "Remover Outliers Severos", "Substituir por Limites", "Substituir por M√©dia", "Substituir por Mediana"].index(
                st.session_state.outlier_treatment_state[col]
            ),
            key=f"outlier_method_{col}_{len(st.session_state.treated_columns)}"
        )

        # Bot√£o para aplicar o tratamento selecionado
        if st.button(f"Aplicar tratamento em {col}"):
            apply_outlier_treatment(col, method, st.session_state.initial_limits[col]["lower_bound"], st.session_state.initial_limits[col]["upper_bound"])
            if col not in st.session_state.treated_columns:
                st.session_state.treated_columns.append(col)
            st.rerun()

    # -------------------------------------
    # üìå Boxplot Final Ap√≥s Tratamento
    # -------------------------------------

    st.write("### Boxplot Ap√≥s Tratamento")
    fig, ax = plt.subplots(figsize=(12, 6))
    st.session_state.data.boxplot(ax=ax)
    plt.xticks(rotation=45)
    st.pyplot(fig)

    # -------------------------------------
    # üìå Bot√£o para Avan√ßar para a Pr√≥xima Etapa
    # -------------------------------------

    if st.button("Pr√≥xima etapa"):
        st.session_state.step = 'data_summary'
        st.rerun()

# -------------------------------------
# üìå FUN√á√ÉO DE SUGEST√ÉO AUTOM√ÅTICA PARA TRATAMENTO DE OUTLIERS
# -------------------------------------

def auto_select_outlier_treatment(col, data, lower_bound, upper_bound):
    """
    Sugere automaticamente o melhor m√©todo de tratamento de outliers com base na distribui√ß√£o dos dados.

    Par√¢metros:
    - col: Nome da coluna a ser analisada.
    - data: DataFrame contendo os dados.
    - lower_bound: Limite inferior dos valores considerados normais (IQR 1.5x abaixo do Q1).
    - upper_bound: Limite superior dos valores considerados normais (IQR 1.5x acima do Q3).

    Retorna:
    - M√©todo sugerido para tratamento dos outliers.
    """

    # -------------------------------------
    # üìå C√°lculo da Propor√ß√£o de Outliers
    # -------------------------------------

    total = len(data)  # N√∫mero total de registos

    # Contar outliers normais (fora do intervalo de 1.5 * IQR)
    total_outliers = len(data[(data[col] < lower_bound) | (data[col] > upper_bound)])

    # Contar outliers severos (fora do intervalo de 3 * IQR)
    total_severe_outliers = len(data[(data[col] < (lower_bound - 1.5 * (upper_bound - lower_bound))) |
                                     (data[col] > (upper_bound + 1.5 * (upper_bound - lower_bound)))])

    # Calcular percentagens
    percentage = total_outliers / total  # Percentagem de outliers normais
    severe_percentage = total_severe_outliers / total  # Percentagem de outliers severos

    # -------------------------------------
    # üìå Verifica√ß√£o da Assimetria dos Dados (Skewness)
    # -------------------------------------

    skewness = data[col].skew()  # Medida de assimetria da distribui√ß√£o dos dados

    # -------------------------------------
    # üìå Defini√ß√£o das Regras para Sugerir o Melhor M√©todo
    # -------------------------------------

    if severe_percentage > 0.10:
        # Se mais de 10% dos valores forem outliers severos, recomenda-se remover apenas os extremos
        return "Remover Outliers Severos"
    elif percentage > 0.20:
        # Se mais de 20% dos valores forem outliers, recomenda-se remover todos os outliers
        return "Remover Outliers"
    elif percentage > 0.05:
        # Se entre 5% e 20% forem outliers, recomenda-se substitu√≠-los pelos limites aceit√°veis
        return "Substituir por Limites"
    else:
        # Se houver menos de 5% de outliers, a escolha entre m√©dia e mediana √© baseada na simetria
        if abs(skewness) > 1:
            return "Substituir por Mediana"  # Se houver alta assimetria, usa-se a mediana
        else:
            return "Substituir por M√©dia"  # Caso contr√°rio, a m√©dia √© uma escolha razo√°vel

# -------------------------------------
# üìå FUN√á√ÉO PARA APLICAR TRATAMENTO DE OUTLIERS
# -------------------------------------

def apply_outlier_treatment(col, method, lower_bound, upper_bound):
    """
    Aplica o tratamento de outliers na coluna especificada, conforme o m√©todo escolhido.

    Par√¢metros:
    - col: Nome da coluna a ser tratada.
    - method: M√©todo de tratamento selecionado.
    - lower_bound: Limite inferior considerado aceit√°vel.
    - upper_bound: Limite superior considerado aceit√°vel.
    """

    # Obter os dados do estado global
    data = st.session_state.data

    # -------------------------------------
    # üìå Remover Todos os Outliers (Fora do Intervalo 1.5 * IQR)
    # -------------------------------------
    
    if method == "Remover Outliers":
        st.session_state.data = data[
            (data[col] >= lower_bound) & (data[col] <= upper_bound)
        ]
        st.success(f"Todos os outliers removidos na coluna '{col}'.")

    # -------------------------------------
    # üìå Remover Apenas Outliers Severos (Fora do Intervalo 3 * IQR)
    # -------------------------------------

    elif method == "Remover Outliers Severos":
        Q1 = data[col].quantile(0.25)
        Q3 = data[col].quantile(0.75)
        IQR = Q3 - Q1

        # Definir limites mais rigorosos para outliers severos (3 * IQR)
        severe_lower = Q1 - 3.0 * IQR
        severe_upper = Q3 + 3.0 * IQR

        st.session_state.data = data[
            (data[col] >= severe_lower) & (data[col] <= severe_upper)
        ]
        st.success(f"Outliers severos removidos na coluna '{col}'.")

    # -------------------------------------
    # üìå Substituir Outliers pelos Limites Aceit√°veis
    # -------------------------------------

    elif method == "Substituir por Limites":
        st.session_state.data[col] = data[col].clip(lower_bound, upper_bound)
        st.success(f"Valores substitu√≠dos pelos limites na coluna '{col}'.")

    # -------------------------------------
    # üìå Substituir Outliers pela M√©dia da Coluna
    # -------------------------------------

    elif method == "Substituir por M√©dia":
        mean_value = data[col].mean()
        mask = (data[col] < lower_bound) | (data[col] > upper_bound)
        st.session_state.data.loc[mask, col] = mean_value
        st.success(f"Valores substitu√≠dos pela m√©dia ({mean_value:.2f}) na coluna '{col}'.")

    # -------------------------------------
    # üìå Substituir Outliers pela Mediana da Coluna
    # -------------------------------------

    elif method == "Substituir por Mediana":
        median_value = data[col].median()
        mask = (data[col] < lower_bound) | (data[col] > upper_bound)
        st.session_state.data.loc[mask, col] = median_value
        st.success(f"Valores substitu√≠dos pela mediana ({median_value:.2f}) na coluna '{col}'.")


##########################################################
# -------------------------------------
# üìå FUN√á√ÉO PARA GUARDAR O DATASET AP√ìS O PR√â-PROCESSAMENTO
# -------------------------------------

def save_modified_dataset_in_memory():
    """
    Salva o dataset tratado na mem√≥ria (session_state) para uso posterior.
    """

    # Criar uma c√≥pia do dataset tratado e armazen√°-lo no estado da sess√£o
    st.session_state.data_tratada = st.session_state.data.copy()

    # Exibir uma mensagem de sucesso
    st.success("O dataset tratado foi salvo na mem√≥ria para uso posterior.")

# -------------------------------------
# üìå FUN√á√ÉO PARA PERMITIR O DOWNLOAD DO DATASET TRATADO
# -------------------------------------

def download_button(df, filename="dataset_tratado.csv"):
    """
    Permite ao utilizador descarregar o dataset tratado em formato CSV.

    Par√¢metros:
    - df: DataFrame tratado a ser disponibilizado para download.
    - filename: Nome do ficheiro CSV a ser descarregado (padr√£o: "dataset_tratado.csv").
    """

    # Converter o DataFrame para formato CSV (sem √≠ndice)
    csv = df.to_csv(index=False)

    # Criar um buffer de mem√≥ria para armazenar o conte√∫do do ficheiro
    buf = io.BytesIO()

    # Escrever o conte√∫do do CSV no buffer e posicionar o cursor no in√≠cio
    buf.write(csv.encode())  # Converter para bytes e armazenar no buffer
    buf.seek(0)  # Definir a posi√ß√£o do cursor para o in√≠cio do ficheiro

    # Criar um bot√£o de download no Streamlit
    st.download_button(
        label="Baixar Dataset Tratado",  # Texto do bot√£o
        data=buf,  # Ficheiro a ser descarregado
        file_name=filename,  # Nome do ficheiro ao fazer o download
        mime="text/csv"  # Tipo MIME do ficheiro
    )


##########################################################
# -------------------------------------
# üìå CLASSE PARA CRIAR O PDF COM O RESUMO AP√ìS O PR√â-PROCESSAMENTO
# -------------------------------------

from fpdf import FPDF
import requests
import tempfile
from datetime import datetime

class CustomPDF(FPDF):
    """
    Classe personalizada para gerar um relat√≥rio em PDF com cabe√ßalho e rodap√© customizados.
    """

    def header(self):
        """
        M√©todo para gerar o cabe√ßalho do PDF, incluindo o log√≥tipo da institui√ß√£o.
        """

        # URL do log√≥tipo da institui√ß√£o
        logo_url = 'https://www.ipleiria.pt/normasgraficas/wp-content/uploads/sites/80/2017/09/estg_v-01.jpg'

        # Fazer o download da imagem
        response = requests.get(logo_url)

        if response.status_code == 200:
            # Criar um ficheiro tempor√°rio para armazenar a imagem baixada
            with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as tmpfile:
                tmpfile.write(response.content)  # Escrever o conte√∫do da imagem no ficheiro tempor√°rio
                tmpfile_path = tmpfile.name  # Obter o caminho do ficheiro

                # Adicionar a imagem no cabe√ßalho do PDF
                self.image(tmpfile_path, x=10, y=8, w=20)  # Definir posi√ß√£o e tamanho da imagem
        else:
            # Se a imagem n√£o for baixada corretamente, exibir mensagem no PDF
            self.set_font('Arial', 'B', 12)
            self.cell(0, 10, "Logo n√£o dispon√≠vel", align='C')

        # Definir a fonte do cabe√ßalho
        self.set_font('Arial', 'B', 12)

        # Adicionar o t√≠tulo da plataforma no cabe√ßalho
        self.cell(0, 10, 'MLCase - Plataforma de Machine Learning', align='C', ln=True)

        # Criar um espa√ßo entre o cabe√ßalho e o conte√∫do
        self.ln(15)

    def footer(self):
        """
        M√©todo para gerar o rodap√© do PDF, incluindo a data e n√∫mero da p√°gina.
        """

        # Definir a posi√ß√£o do rodap√© a 1.5 cm do final da p√°gina
        self.set_y(-15)

        # Definir a fonte do rodap√©
        self.set_font('Arial', 'I', 10)

        # Obter a data atual no formato dia/m√™s/ano
        current_date = datetime.now().strftime('%d/%m/%Y')

        # Adicionar rodap√© com a data e o n√∫mero da p√°gina
        self.cell(0, 10, f'{current_date} - P√°gina {self.page_no()}  |  Autora da Plataforma: Bruna Sousa', align='C')

# -------------------------------------
# üìå FUN√á√ÉO PARA GERAR O PDF COM O RESUMO DO PR√â-PROCESSAMENTO
# -------------------------------------

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from io import BytesIO

def generate_pdf_resumo(dataset, summary_df, missing_data, outlier_summary):
    """
    Gera um relat√≥rio em PDF com informa√ß√µes estat√≠sticas do dataset, valores ausentes, outliers,
    matriz de correla√ß√£o e boxplot.

    Par√¢metros:
    - dataset: DataFrame original ap√≥s pr√©-processamento.
    - summary_df: DataFrame com estat√≠sticas descritivas do dataset.
    - missing_data: S√©rie contendo a contagem de valores ausentes por coluna.
    - outlier_summary: Lista contendo o resumo dos outliers identificados.

    Retorna:
    - Um buffer de mem√≥ria contendo o PDF gerado.
    """

    # -------------------------------------
    # üìå Fun√ß√£o Auxiliar para Limpar Texto
    # -------------------------------------

    def clean_text(text):
        """Remove caracteres incompat√≠veis com a codifica√ß√£o do PDF."""
        if not isinstance(text, str):
            return text
        return text.encode('latin-1', errors='ignore').decode('latin-1')

    # -------------------------------------
    # üìå Inicializa√ß√£o do PDF
    # -------------------------------------

    pdf = CustomPDF(format='A4')
    pdf.set_auto_page_break(auto=True, margin=15)
    pdf.add_page()
    pdf.set_font("Arial", size=8)

    # -------------------------------------
    # üìå T√≠tulo do Relat√≥rio
    # -------------------------------------

    pdf.set_font("Arial", style="B", size=12)
    pdf.cell(0, 10, txt=clean_text("Relat√≥rio Resumo dos Dados"), ln=True, align="C")
    pdf.ln(5)

    # -------------------------------------
    # üìå Estat√≠sticas Descritivas Simplificadas
    # -------------------------------------

    pdf.set_font("Arial", style="B", size=12)
    pdf.cell(0, 10, txt=clean_text("Estat√≠sticas Descritivas"), ln=True)
    pdf.set_font("Arial", size=8)

    # Criar DataFrame simplificado com estat√≠sticas principais
    summary_simplified = pd.DataFrame({
        'Coluna': dataset.columns,
        'Tipo de Dados': dataset.dtypes,
        'Count': dataset.count(),
        'Top': dataset.mode().iloc[0],  # Valor mais frequente (moda)
    })

    # Inicializar colunas estat√≠sticas apenas para colunas num√©ricas
    summary_simplified['std'] = None
    summary_simplified['min'] = None
    summary_simplified['max'] = None
    summary_simplified['M√©dia'] = None

    numeric_columns = dataset.select_dtypes(include=['float64', 'int64']).columns
    summary_simplified.loc[summary_simplified['Coluna'].isin(numeric_columns), 'M√©dia'] = dataset[numeric_columns].mean()
    summary_simplified.loc[summary_simplified['Coluna'].isin(numeric_columns), 'std'] = dataset[numeric_columns].std()
    summary_simplified.loc[summary_simplified['Coluna'].isin(numeric_columns), 'min'] = dataset[numeric_columns].min()
    summary_simplified.loc[summary_simplified['Coluna'].isin(numeric_columns), 'max'] = dataset[numeric_columns].max()

    # Formatar valores num√©ricos para 4 casas decimais
    for col in ['M√©dia', 'std', 'min', 'max']:
        summary_simplified[col] = summary_simplified[col].apply(lambda x: f"{x:.4f}" if isinstance(x, (int, float)) else x)

    # Substituir 'nan' por vazio
    summary_simplified = summary_simplified.fillna('')

    # -------------------------------------
    # üìå Adicionar Tabela das Estat√≠sticas ao PDF
    # -------------------------------------

    pdf.set_fill_color(144, 238, 144)  # Cor de fundo do cabe√ßalho
    col_widths = [pdf.get_string_width(col) for col in summary_simplified.columns]
    max_width = 180

    total_width = sum(col_widths)
    scale_factor = max_width / total_width
    col_widths = [width * scale_factor for width in col_widths]

    for i, col in enumerate(summary_simplified.columns):
        pdf.cell(col_widths[i], 10, clean_text(col), 1, 0, 'C', True)
    pdf.ln()

    for i, row in summary_simplified.iterrows():
        for j, cell in enumerate(row):
            pdf.cell(col_widths[j], 8, clean_text(str(cell)), 1, 0, 'C')
        pdf.ln()

    pdf.ln(10)

    # -------------------------------------
    # üìå Resumo de Valores Ausentes
    # -------------------------------------

    pdf.set_font("Arial", style="B", size=12)
    pdf.cell(0, 10, txt=clean_text("Resumo de Valores Ausentes"), ln=True)
    pdf.set_font("Arial", size=8)

    if not missing_data.empty:
        pdf.set_fill_color(144, 238, 144)
        pdf.cell(50, 10, clean_text("Vari√°vel"), 1, 0, 'C', True)
        pdf.cell(50, 10, clean_text("Total de Ausentes"), 1, 1, 'C', True)
        for col, count in missing_data.items():
            pdf.cell(50, 10, clean_text(col), 1)
            pdf.cell(50, 10, clean_text(str(count)), 1, 1)
        pdf.ln(10)
    else:
        pdf.cell(0, 10, txt=clean_text("N√£o h√° valores ausentes."), ln=True)

    # -------------------------------------
    # üìå Resumo de Outliers
    # -------------------------------------

    pdf.set_font("Arial", style="B", size=12)
    pdf.cell(0, 10, txt=clean_text("Resumo de Outliers"), ln=True)
    pdf.set_font("Arial", size=8)

    if outlier_summary:
        pdf.set_fill_color(144, 238, 144)
        pdf.cell(50, 10, clean_text("Vari√°vel"), 1, 0, 'C', True)
        pdf.cell(50, 10, clean_text("Total de Outliers"), 1, 1, 'C', True)
        for entry in outlier_summary:
            pdf.cell(50, 10, clean_text(entry["Vari√°vel"]), 1)
            pdf.cell(50, 10, clean_text(str(entry["Total de Outliers"])), 1, 1)
        pdf.ln(10)
    else:
        pdf.cell(0, 10, txt=clean_text("N√£o h√° outliers."), ln=True)

    # -------------------------------------
    # üìå Matriz de Correla√ß√£o (Heatmap)
    # -------------------------------------

    pdf.cell(0, 10, txt=clean_text("Matriz de Correla√ß√£o das Vari√°veis"), ln=True)
    numeric_data = dataset.select_dtypes(include=['float64', 'int64'])
    correlation_matrix = numeric_data.corr()

    plt.figure(figsize=(8, 6))
    sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".4f", cbar=True, square=True)
    plt.title('Matriz de Correla√ß√£o das Vari√°veis', fontsize=14, fontweight='bold')

    temp_filename = "correlation_heatmap.png"
    plt.savefig(temp_filename)
    plt.close()
    pdf.image(temp_filename, x=10, w=180)
    pdf.ln(95)

    # -------------------------------------
    # üìå Boxplot das Vari√°veis Num√©ricas
    # -------------------------------------

    pdf.cell(0, 10, txt=clean_text("Boxplot das Vari√°veis Num√©ricas"), ln=True)
    plt.figure(figsize=(10, 6))
    sns.boxplot(data=numeric_data)
    plt.title('Boxplot das Vari√°veis Num√©ricas')

    temp_filename_boxplot = "boxplot_combined.png"
    plt.savefig(temp_filename_boxplot)
    plt.close()
    pdf.image(temp_filename_boxplot, x=10, w=180)
    pdf.ln(75)

    # -------------------------------------
    # üìå Gerar o PDF no Buffer de Mem√≥ria
    # -------------------------------------

    pdf_buffer = BytesIO()
    pdf_output = pdf.output(dest='S').encode('latin-1', errors='ignore')
    pdf_buffer.write(pdf_output)
    pdf_buffer.seek(0)

    return pdf_buffer

# -------------------------------------
# üìå FUN√á√ÉO PARA SALVAR UMA TABELA COMO IMAGEM (PNG)
# -------------------------------------

import matplotlib.pyplot as plt

def save_table_as_image(df, filename="table_image.png"):
    """
    Converte um DataFrame Pandas numa imagem (PNG), formatando os valores para melhor visualiza√ß√£o.

    Par√¢metros:
    - df: DataFrame contendo a tabela a ser convertida em imagem.
    - filename: Nome do ficheiro da imagem a ser salva (padr√£o: "table_image.png").
    """

    # -------------------------------------
    # üìå Tratamento de Valores no DataFrame Antes da Gera√ß√£o da Imagem
    # -------------------------------------

    # Substituir valores `NaN` por valores vazios para evitar exibi√ß√µes incorretas
    df = df.fillna('')

    # Formatar valores num√©ricos para 4 casas decimais
    for col in df.select_dtypes(include=['float64', 'int64']).columns:
        df[col] = df[col].apply(lambda x: f"{x:.4f}" if isinstance(x, (int, float)) else x)

    # -------------------------------------
    # üìå Configura√ß√£o da Figura para Gera√ß√£o da Tabela
    # -------------------------------------

    fig, ax = plt.subplots(figsize=(8, 4))  # Define o tamanho da imagem gerada
    ax.axis('tight')  # Ajusta os limites para caber na figura
    ax.axis('off')  # Remove os eixos para melhor visualiza√ß√£o

    # Criar a tabela no gr√°fico
    table = ax.table(
        cellText=df.values,  # Conte√∫do da tabela
        colLabels=df.columns,  # Cabe√ßalhos das colunas
        loc='center',  # Centralizar a tabela na imagem
        cellLoc='center',  # Centralizar o texto nas c√©lulas
        colColours=['#D9EAF7'] * len(df.columns)  # Definir cor do cabe√ßalho da tabela
    )

    # -------------------------------------
    # üìå Ajustes de Formata√ß√£o da Tabela
    # -------------------------------------

    table.auto_set_font_size(False)  # Desativar ajuste autom√°tico do tamanho da fonte
    table.set_fontsize(10)  # Definir tamanho da fonte manualmente
    table.auto_set_column_width(col=list(range(len(df.columns))))  # Ajustar automaticamente a largura das colunas

    # -------------------------------------
    # üìå Salvamento da Tabela Como Imagem (PNG)
    # -------------------------------------

    plt.savefig(filename, format='png', bbox_inches='tight')  # Salvar imagem no formato PNG
    plt.close()  # Fechar a figura para evitar sobrecarga de mem√≥ria

# Resumo do Pr√©-processamento de dados:
# -------------------------------------
# üìå FUN√á√ÉO PARA GERAR O RESUMO DOS DADOS
# -------------------------------------

def data_summary():
    """
    Apresenta um resumo dos dados tratados, incluindo estat√≠sticas descritivas, valores ausentes,
    detec√ß√£o de outliers, boxplots e matriz de correla√ß√£o. Al√©m disso, permite o download do resumo
    em PDF e do dataset tratado.
    """

    st.subheader("Resumo dos Dados")

    # -------------------------------------
    # üìå Verificar Disponibilidade do Dataset
    # -------------------------------------

    if 'data' in st.session_state and st.session_state.data is not None:
        dataset = st.session_state.data
        st.success("Usando o dataset tratado!")
    else:
        st.error("Nenhum dataset est√° dispon√≠vel. Por favor, execute o tratamento de dados antes.")
        return  # Encerra a fun√ß√£o caso n√£o haja dados dispon√≠veis

    # -------------------------------------
    # üìå Sele√ß√£o de Colunas para Exibi√ß√£o
    # -------------------------------------

    # Obter colunas selecionadas ou usar todas as colunas do dataset
    selected_columns = st.session_state.get('selected_columns', [])
    if not selected_columns:
        selected_columns = dataset.columns.tolist()

    # Permitir que o utilizador selecione as colunas para visualiza√ß√£o
    selected_columns_to_display = st.multiselect(
        "Selecione as vari√°veis para visualizar as estat√≠sticas",
        options=selected_columns,
        default=selected_columns
    )

    # Exibir o n√∫mero de linhas e colunas do dataset filtrado
    st.write("N√∫mero de linhas e colunas:", dataset[selected_columns_to_display].shape)

    # -------------------------------------
    # üìå Estat√≠sticas Descritivas
    # -------------------------------------

    # Identificar colunas num√©ricas
    numeric_columns = dataset[selected_columns_to_display].select_dtypes(include=['number']).columns

    # Criar um dicion√°rio para armazenar estat√≠sticas
    summary_data = {
        'Count': dataset[selected_columns_to_display].count(),
        'Mean': dataset[numeric_columns].mean(),
        'Std': dataset[numeric_columns].std(),
        'Min': dataset[numeric_columns].min(),
        '25%': dataset[numeric_columns].quantile(0.25),
        '50%': dataset[numeric_columns].median(),
        '75%': dataset[numeric_columns].quantile(0.75),
        'Max': dataset[numeric_columns].max(),
    }

    # Converter para DataFrame e adicionar os tipos de dados
    summary_df = pd.DataFrame(summary_data)
    summary_df['Tipo de Dados'] = dataset[selected_columns_to_display].dtypes

    # Arredondar valores num√©ricos para 4 casas decimais e preencher valores ausentes com 0
    summary_df = summary_df.round(4).fillna(0)

    # Exibir a tabela de estat√≠sticas descritivas
    st.write("Estat√≠sticas Descritivas e Tipos de Dados")
    st.dataframe(fix_dataframe_types(summary_df))

    # -------------------------------------
    # üìå An√°lise de Valores Ausentes
    # -------------------------------------

    st.subheader("Resumo de Valores Ausentes")

    # Identificar colunas com valores ausentes
    missing_data = dataset[selected_columns_to_display].isnull().sum()
    missing_data = missing_data[missing_data > 0]

    if not missing_data.empty:
        st.write("Valores ausentes encontrados:")
        st.dataframe(fix_dataframe_types(missing_data.rename("Total de Valores Ausentes")))
    else:
        st.write("N√£o h√° valores ausentes nas vari√°veis selecionadas.")

    # -------------------------------------
    # üìå An√°lise de Outliers
    # -------------------------------------

    st.subheader("Resumo de Outliers")

    # Selecionar apenas colunas num√©ricas
    numeric_data = dataset[selected_columns_to_display].select_dtypes(include=['number'])

    # Obter colunas j√° tratadas
    treated_columns = st.session_state.get('treated_columns', [])

    # Criar lista para armazenar o resumo dos outliers
    outlier_summary = []

    if not numeric_data.empty:
        for column in numeric_data.columns:
            if column in treated_columns:  # Ignorar colunas j√° tratadas
                continue

            # C√°lculo dos quartis e do intervalo interquartil (IQR)
            Q1 = numeric_data[column].quantile(0.25)
            Q3 = numeric_data[column].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR

            # Identificar outliers
            outliers = numeric_data[(numeric_data[column] < lower_bound) | (numeric_data[column] > upper_bound)]
            if len(outliers) > 0:
                outlier_summary.append({
                    "Vari√°vel": column,
                    "Total de Outliers": len(outliers)
                })

        # Exibir o resumo dos outliers encontrados
        if outlier_summary:
            st.dataframe(fix_dataframe_types(pd.DataFrame(outlier_summary)))
        else:
            st.write("N√£o h√° outliers nas vari√°veis selecionadas.")
    else:
        st.write("Nenhuma vari√°vel num√©rica para an√°lise de outliers.")

    # -------------------------------------
    # üìå Gr√°fico Boxplot das Vari√°veis Num√©ricas
    # -------------------------------------

    st.subheader("Boxplot das Vari√°veis Num√©ricas")

    plt.figure(figsize=(10, 6))
    sns.boxplot(data=numeric_data)
    plt.title('Boxplot das Vari√°veis Num√©ricas')
    st.pyplot(plt)

    # -------------------------------------
    # üìå Matriz de Correla√ß√£o (Heatmap)
    # -------------------------------------

    st.subheader("Matriz de Correla√ß√£o das Vari√°veis")

    # Calcular a correla√ß√£o entre vari√°veis num√©ricas
    correlation_matrix = numeric_data.corr()

    # Gerar e exibir o heatmap da correla√ß√£o
    plt.figure(figsize=(8, 6))
    sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".4f", cbar=True, square=True)
    plt.title('Matriz de Correla√ß√£o das Vari√°veis', fontsize=14, fontweight='bold', fontname='Arial')
    st.pyplot(plt)

    # -------------------------------------
    # üìå Download do Resumo em PDF
    # -------------------------------------

    pdf_buffer = generate_pdf_resumo(dataset, summary_df, missing_data, outlier_summary)
    st.download_button(
        label="Baixar PDF com o Resumo",
        data=pdf_buffer,
        file_name="resumo_dos_dados.pdf",
        mime="application/pdf"
    )

    # -------------------------------------
    # üìå Download do Dataset Tratado
    # -------------------------------------

    dataset_to_download = dataset[selected_columns_to_display]
    download_button(dataset_to_download)

    # -------------------------------------
    # üìå Navega√ß√£o Entre Etapas
    # -------------------------------------

    col1, col2 = st.columns([1, 1])

    with col1:
        if st.button("Voltar"):
            st.session_state.step = 'outlier_detection'
            st.rerun()

    with col2:
        if st.button("Pr√≥xima etapa"):
            st.session_state.step = 'model_selection'
            st.rerun()


##########################################################
# -------------------------------------
# üìå FUN√á√ÉO PARA PLOTAR M√âTRICAS DE DESEMPENHO DOS MODELOS
# -------------------------------------

import streamlit as st
import matplotlib.pyplot as plt

def plot_metrics(metrics_df):
    """
    Gera gr√°ficos para visualizar as m√©tricas de desempenho dos modelos, diferenciando entre
    tarefas de classifica√ß√£o e regress√£o.

    Par√¢metros:
    - metrics_df: DataFrame contendo as m√©tricas de desempenho dos modelos.

    Retorno:
    - Exibe os gr√°ficos no Streamlit.
    """

    try:
        # -------------------------------------
        # üìå Inicializar Armazenamento de M√©tricas no Estado da Sess√£o
        # -------------------------------------

        # Se a chave 'metrics' ainda n√£o estiver no session_state, inicializ√°-la
        if 'metrics' not in st.session_state:
            st.session_state['metrics'] = {}

        # Verificar se o DataFrame est√° vazio
        if metrics_df.empty:
            st.warning("Nenhum dado para exibir no gr√°fico.")
            return

        # Armazenar as m√©tricas no estado da sess√£o para refer√™ncia posterior
        for _, row in metrics_df.iterrows():
            model_name = row.name  # Assumindo que o √≠ndice cont√©m o nome do modelo
            st.session_state['metrics'][model_name] = row.to_dict()

        # -------------------------------------
        # üìå Configura√ß√£o do √çndice e Identifica√ß√£o do Tipo de Modelo
        # -------------------------------------

        # Definir a coluna 'Modelo' como √≠ndice, se ainda n√£o estiver
        metrics_df.set_index('Modelo', inplace=True)

        # Listas de m√©tricas t√≠picas para classifica√ß√£o e regress√£o
        classification_columns = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
        regression_columns = ['MSE', 'MAE', 'R¬≤']

        # -------------------------------------
        # üìå Plotagem de Gr√°ficos de Classifica√ß√£o
        # -------------------------------------

        if all(col in metrics_df.columns for col in classification_columns):
            # Criar a figura do gr√°fico de barras
            fig, ax = plt.subplots(figsize=(10, 6))
            
            # Plotar as m√©tricas de classifica√ß√£o
            metrics_df[classification_columns].plot(kind='bar', ax=ax)

            # Configura√ß√£o do gr√°fico
            plt.title('M√©tricas de Desempenho dos Modelos (Classifica√ß√£o)', fontsize=16)
            plt.ylabel('Valor', fontsize=14)
            plt.xlabel('Modelos', fontsize=14)
            plt.xticks(rotation=45, ha='right', fontsize=12)
            plt.ylim(0, 1)  # As m√©tricas de classifica√ß√£o geralmente variam entre 0 e 1
            plt.legend(loc='lower right', fontsize=12)
            plt.grid(axis='y', linestyle='--', alpha=0.7)

        # -------------------------------------
        # üìå Plotagem de Gr√°ficos de Regress√£o
        # -------------------------------------

        elif all(col in metrics_df.columns for col in regression_columns):
            # Criar a figura do gr√°fico de barras
            fig, ax = plt.subplots(figsize=(10, 6))

            # Plotar as m√©tricas de regress√£o
            metrics_df[regression_columns].plot(kind='bar', ax=ax)

            # Configura√ß√£o do gr√°fico
            plt.title('M√©tricas de Desempenho dos Modelos (Regress√£o)', fontsize=16)
            plt.ylabel('Valor', fontsize=14)
            plt.xlabel('Modelos', fontsize=14)
            plt.xticks(rotation=45, ha='right', fontsize=12)
            plt.legend(loc='upper right', fontsize=12)
            plt.grid(axis='y', linestyle='--', alpha=0.7)

        else:
            st.error("O DataFrame n√£o cont√©m m√©tricas v√°lidas para classifica√ß√£o ou regress√£o.")
            return  # Se n√£o h√° m√©tricas v√°lidas, encerra a fun√ß√£o

        # -------------------------------------
        # üìå Exibir o Gr√°fico no Streamlit
        # -------------------------------------

        st.pyplot(fig)

    except Exception as e:
        # Tratamento de erros gen√©rico para evitar falhas inesperadas
        st.error(f"Ocorreu um erro ao plotar as m√©tricas: {str(e)}")

    finally:
        # Limpar a figura para evitar sobreposi√ß√£o de gr√°ficos na interface do Streamlit
        plt.clf()

# -------------------------------------
# üìå FUN√á√ÉO PARA DEFINIR O GRID DE HIPERPAR√ÇMETROS PADR√ÉO PARA CADA MODELO
# -------------------------------------

def get_default_param_grid(model_name):
    """
    Retorna um dicion√°rio contendo os hiperpar√¢metros padr√£o para cada modelo de Machine Learning.

    Par√¢metros:
    - model_name: Nome do modelo para o qual se deseja obter o conjunto de hiperpar√¢metros.

    Retorno:
    - Dicion√°rio com os hiperpar√¢metros e os respetivos intervalos de valores para otimiza√ß√£o.
    """

    # -------------------------------------
    # üìå Configura√ß√£o do Grid Search para Support Vector Classification (SVC)
    # -------------------------------------
    if model_name == "Support Vector Classification (SVC)":
        return {
            'C': [0.1, 1, 10],  # Define a penaliza√ß√£o do erro
            'kernel': ['linear', 'rbf'],  # Tipos de kernel utilizados
            'gamma': ['scale', 'auto']  # Apenas utilizado quando kernel='rbf'
        }

    # -------------------------------------
    # üìå Configura√ß√£o do Grid Search para K-Nearest Neighbors (KNN)
    # -------------------------------------
    elif model_name == "K-Nearest Neighbors (KNN)":
        return {
            'n_neighbors': list(range(1, 21)),  # Testa todos os valores de 1 a 20 para o n√∫mero de vizinhos
            'weights': ['uniform', 'distance']  # Define a forma de pondera√ß√£o das dist√¢ncias
        }

    # -------------------------------------
    # üìå Configura√ß√£o do Grid Search para Random Forest
    # -------------------------------------
    elif model_name == "Random Forest":
        # Gera√ß√£o din√¢mica do par√¢metro `max_depth`
        max_depth_range = [None] + list(range(5, 21, 5))  # [None, 5, 10, 15, 20]
        return {
            'max_depth': max_depth_range,  # Profundidade m√°xima da √°rvore
            'n_estimators': [10, 50, 100]  # N√∫mero de √°rvores na floresta
        }

    # -------------------------------------
    # üìå Configura√ß√£o do Grid Search para Suporte de Vetores em Regress√£o (SVR)
    # -------------------------------------
    elif model_name == "Regress√£o por Vetores de Suporte (SVR)":
        return {
            'C': [1, 10],  # Penaliza√ß√£o do erro
            'epsilon': [0.1, 0.2],  # Margem de toler√¢ncia para erro
            'kernel': ['linear', 'rbf']  # Tipos de kernel utilizados
        }

    # -------------------------------------
    # üìå Configura√ß√£o para Regress√£o Linear Simples (RLS)
    # -------------------------------------
    elif model_name == "Regress√£o Linear Simples (RLS)":
        return {}  # A regress√£o linear geralmente n√£o requer ajuste de hiperpar√¢metros

    # -------------------------------------
    # üìå Retorno para modelos n√£o especificados
    # -------------------------------------
    else:
        return {}  # Se o modelo n√£o for reconhecido, retorna um dicion√°rio vazio

# -------------------------------------
# üìå FUN√á√ÉO PARA CONFIGURA√á√ÉO MANUAL DOS PAR√ÇMETROS DOS MODELOS
# -------------------------------------

import streamlit as st
import json

def configure_manual_params(model_key, param_grid, manual_params):
    """
    Permite a configura√ß√£o manual dos hiperpar√¢metros para o modelo selecionado, 
    exibindo intervalos personalizados para os par√¢metros num√©ricos.

    Par√¢metros:
    - model_key: Nome do modelo a ser ajustado.
    - param_grid: Dicion√°rio com os hiperpar√¢metros e op√ß√µes dispon√≠veis.
    - manual_params: Dicion√°rio onde os valores dos hiperpar√¢metros ser√£o armazenados.

    Retorno:
    - manual_params atualizado com os valores configurados pelo utilizador.
    """

    st.write(f"Configura√ß√µes manuais para o modelo: {model_key}")

    # -------------------------------------
    # üìå Limpar Par√¢metros Inv√°lidos no Estado Global
    # -------------------------------------

    # Remover 'gamma' do estado global se ele estiver presente
    if 'manual_params' in st.session_state and 'gamma' in st.session_state['manual_params']:
        del st.session_state['manual_params']['gamma']

    # -------------------------------------
    # üìå Defini√ß√£o de Intervalos Personalizados para Par√¢metros Num√©ricos
    # -------------------------------------

    param_ranges = {
        'C': {'min': 0.1, 'max': 100.0, 'step': 0.1, 'default': 1.0},  # Controle de penaliza√ß√£o do erro
        'epsilon': {'min': 0.01, 'max': 1.0, 'step': 0.01, 'default': 0.1},  # Toler√¢ncia ao erro em SVR
        'gamma': {'min': 0.01, 'max': 1.0, 'step': 0.01, 'default': 0.1},  # Par√¢metro do kernel 'rbf'
        'degree': {'min': 1, 'max': 5, 'step': 1, 'default': 3},  # Apenas para kernel 'poly'
    }

    # -------------------------------------
    # üìå Criar Widgets para Configura√ß√£o de Par√¢metros
    # -------------------------------------

    for param in param_grid:
        # Se o par√¢metro for categ√≥rico (exemplo: 'kernel', 'weights')
        if isinstance(param_grid[param][0], str):
            manual_params[param] = st.selectbox(
                f"{param} (Op√ß√µes: {', '.join(param_grid[param])}):",
                options=param_grid[param],
                index=0,
                key=f"{model_key}_{param}"
            )

        # Se o par√¢metro for num√©rico (inteiro ou float)
        elif isinstance(param_grid[param][0], (int, float)):
            param_type = float if any(isinstance(x, float) for x in param_grid[param]) else int

            # Verificar se o par√¢metro tem um intervalo personalizado definido
            if param in param_ranges:
                config = param_ranges[param]

                # Exibir informa√ß√£o sobre o intervalo aceito
                st.write(f"**{param}** (Intervalo: {config['min']} a {config['max']})")

                # Se for 'max_depth' (pode ser `None`), criar um selectbox
                if param == 'max_depth':
                    manual_params[param] = st.selectbox(
                        f"{param}:",
                        options=[None] + list(range(1, 21)),  # Permite selecionar `None`
                        index=0 if config['default'] is None else list(range(1, 21)).index(config['default']),
                        key=f"{model_key}_{param}"
                    )

                else:
                    # Criar um input num√©rico para outros par√¢metros
                    manual_params[param] = st.number_input(
                        f"{param}:",
                        min_value=config['min'],
                        max_value=config['max'],
                        value=config['default'],
                        step=config['step'],
                        key=f"{model_key}_{param}"
                    )

    # -------------------------------------
    # üìå Configura√ß√£o Din√¢mica do Par√¢metro 'gamma'
    # -------------------------------------

    # O par√¢metro 'gamma' s√≥ deve ser configurado se o kernel for 'rbf'
    if 'kernel' in manual_params and manual_params['kernel'] == 'rbf':
        config = param_ranges['gamma']
        st.write(f"**gamma** (Intervalo: {config['min']} a {config['max']})")
        manual_params['gamma'] = st.number_input(
            "gamma:",
            min_value=config['min'],
            max_value=config['max'],
            value=config['default'],
            step=config['step'],
            key=f"{model_key}_gamma"
        )
    else:
        # Se o kernel n√£o for 'rbf', remover 'gamma' do estado global e do dicion√°rio de par√¢metros
        manual_params.pop('gamma', None)
        if 'manual_params' in st.session_state and 'gamma' in st.session_state['manual_params']:
            del st.session_state['manual_params']['gamma']

    # -------------------------------------
    # üìå Atualizar Estado Global com Par√¢metros Configurados
    # -------------------------------------

    st.session_state['manual_params'] = manual_params
    st.session_state['best_params_str'] = json.dumps(manual_params, indent=2)  # Armazena como JSON formatado

    # Exibir os par√¢metros configurados
    st.write("Par√¢metros manuais salvos:", st.session_state['manual_params'])

    return manual_params

# -------------------------------------
# üìå DICION√ÅRIO DE PAR√ÇMETROS V√ÅLIDOS PARA CADA MODELO
# -------------------------------------

VALID_PARAMS = {
    "Random Forest": ["n_estimators", "max_depth"],  # Ajust√°veis para Random Forest
    "Support Vector Classification (SVC)": ["C", "kernel", "gamma"],  # Agora inclui "gamma"
    "K-Nearest Neighbors (KNN)": ["n_neighbors", "weights"],  # N√∫mero de vizinhos e peso das dist√¢ncias
    "Regress√£o Linear Simples (RLS)": [],  # Sem hiperpar√¢metros ajust√°veis
    "Regress√£o por Vetores de Suporte (SVR)": ["C", "epsilon", "kernel"],  # Hiperpar√¢metros t√≠picos do SVR
}



# -------------------------------------
# üìå FUN√á√ÉO PARA CONFIGURAR A VALIDA√á√ÉO CRUZADA COM BASE NA ESCOLHA DO UTILIZADOR
# -------------------------------------

def get_cv_strategy(cv_choice, X_train, y_train):
    """
    Retorna a estrat√©gia de valida√ß√£o cruzada com base na escolha do utilizador.

    Par√¢metros:
    - cv_choice: Tipo de valida√ß√£o cruzada selecionado pelo utilizador.
    - X_train: Dados de treino.
    - y_train: Labels do conjunto de treino.

    Retorno:
    - Objeto da estrat√©gia de valida√ß√£o cruzada correspondente.
    """
    
    if cv_choice == "K-Fold":
        return KFold(n_splits=5, shuffle=True, random_state=42)  # Divide os dados em 5 partes aleat√≥rias

    elif cv_choice == "Leave-One-Out":
        return LeaveOneOut()  # Usa cada amostra individualmente como conjunto de teste

    elif cv_choice == "Divis√£o em Treino e Teste":
        # Divide os dados de treino em 70% treino e 30% teste
        return train_test_split(X_train, y_train, test_size=0.3, random_state=42)

    elif cv_choice == "Holdout":
        # Funciona de forma semelhante ao treino-teste, com um conjunto adicional
        return train_test_split(X_train, y_train, test_size=0.3, random_state=42)

    else:
        # Se a escolha for inv√°lida, usa K-Fold como padr√£o
        return KFold(n_splits=5, shuffle=True, random_state=42)

# -------------------------------------
# üìå FUN√á√ÉO PARA CONFIGURAR MANUALMENTE O SVR (SUPPORT VECTOR REGRESSION)
# -------------------------------------

def configure_svr(model_key, manual_params):
    """
    Configura√ß√£o manual dos par√¢metros para o modelo Support Vector Regression (SVR).

    Par√¢metros:
    - model_key: Nome do modelo (SVR).
    - manual_params: Dicion√°rio para armazenar os hiperpar√¢metros configurados pelo utilizador.

    Retorno:
    - Dicion√°rio manual_params atualizado com os valores escolhidos pelo utilizador.
    """
    
    st.write("Configura√ß√£o de par√¢metros para Support Vector Regression (SVR)")

    # Configura√ß√£o dos hiperpar√¢metros principais
    c = st.number_input(
        "Par√¢metro C (Regulariza√ß√£o)", min_value=0.1, max_value=100.0, step=0.1, value=1.0
    )
    epsilon = st.number_input(
        "Par√¢metro epsilon", min_value=0.0, max_value=1.0, step=0.1, value=0.1
    )
    kernel = st.selectbox(
        "Escolha o kernel", options=["linear", "rbf", "poly", "sigmoid"], index=0
    )

    # Guardar os valores no dicion√°rio de par√¢metros
    manual_params['C'] = c
    manual_params['epsilon'] = epsilon
    manual_params['kernel'] = kernel

    # Configura√ß√£o extra para o kernel 'rbf'
    if kernel == "rbf":
        gamma = st.number_input(
            "Par√¢metro gamma", min_value=0.0, max_value=1.0, step=0.1, value=0.1
        )
        manual_params['gamma'] = gamma

    return manual_params

# -------------------------------------
# üìå FUN√á√ÉO PARA CONFIGURAR MANUALMENTE O SVC (SUPPORT VECTOR CLASSIFICATION)
# -------------------------------------

def configure_svc(model_key, manual_params):
    """
    Configura√ß√£o manual dos par√¢metros para o modelo Support Vector Classification (SVC).

    Par√¢metros:
    - model_key: Nome do modelo (SVC).
    - manual_params: Dicion√°rio para armazenar os hiperpar√¢metros configurados pelo utilizador.

    Retorno:
    - Dicion√°rio manual_params atualizado com os valores escolhidos pelo utilizador.
    """

    # Exibir o estado inicial dos par√¢metros (para depura√ß√£o)
    st.write("Estado inicial dos par√¢metros:", st.session_state.get('manual_params', {}))

    # Sele√ß√£o do tipo de kernel
    kernel_value = st.selectbox(
        "Escolha o valor para 'kernel'",
        options=["linear", "rbf"],  # Op√ß√µes dispon√≠veis
        index=0,  # Define 'linear' como padr√£o
        key="kernel_selectbox"
    )

    # Defini√ß√£o do valor de 'C' (Par√¢metro de regulariza√ß√£o)
    C_value = st.number_input(
        "Defina o valor para 'C'",
        min_value=0.01, step=0.01, value=1.0,
        key="C_input"
    )

    # Inicializar manual_params com os valores escolhidos
    manual_params = {
        "C": C_value,
        "kernel": kernel_value
    }

    # **Exibir 'gamma' apenas se o kernel for 'rbf'**
    if kernel_value == "rbf":
        gamma_value = st.selectbox(
            "Escolha o valor para 'gamma'",
            options=["scale", "auto"],  # Op√ß√µes dispon√≠veis
            index=0,
            key="gamma_selectbox"
        )
        manual_params["gamma"] = gamma_value  # Adiciona 'gamma' se necess√°rio

    else:
        # **Remover 'gamma' se o kernel for 'linear'**
        # Remover do manual_params local
        manual_params.pop('gamma', None)
        
        # Remover do estado global (caso tenha sido armazenado anteriormente)
        if 'manual_params' in st.session_state and 'gamma' in st.session_state['manual_params']:
            del st.session_state['manual_params']['gamma']  # Remove globalmente
            
        if 'best_params_str' in st.session_state:  # Remove dos par√¢metros guardados
            st.session_state['best_params_str'] = json.dumps(manual_params, indent=2)

    # Exibir os par√¢metros atualizados ap√≥s a sele√ß√£o manual
    st.write("Par√¢metros atualizados:", manual_params)

    # **Guardar os par√¢metros configurados no estado global**
    st.session_state['manual_params'] = manual_params
    st.session_state['best_params_str'] = json.dumps(manual_params, indent=2)

    # Exibir os par√¢metros guardados para depura√ß√£o
    st.write("Par√¢metros manuais salvos:", st.session_state['manual_params'])

    return manual_params


import pickle
import os

# -------------------------------------
# üìå FUN√á√ïES PARA GUARDAR E CARREGAR OS MELHORES PAR√ÇMETROS
# -------------------------------------

def save_best_params(params):
    """
    Guarda os melhores hiperpar√¢metros encontrados num ficheiro pickle.

    Par√¢metros:
    - params (dict): Dicion√°rio contendo os melhores hiperpar√¢metros.
    
    Retorno:
    - Nenhum (apenas salva os dados).
    """
    with open('best_params.pkl', 'wb') as f:
        pickle.dump(params, f)

def load_best_params():
    """
    Carrega os melhores hiperpar√¢metros previamente guardados, se existirem.

    Retorno:
    - dict: Dicion√°rio contendo os melhores hiperpar√¢metros, ou None se n√£o existirem par√¢metros guardados.
    """
    if os.path.exists('best_params.pkl'):
        with open('best_params.pkl', 'rb') as f:
            return pickle.load(f)
    return None


# -------------------------------------
# üìå FUN√á√ÉO PARA TREINAR UM MODELO SVR COM OU SEM GRID SEARCH
# -------------------------------------

def train_svr_with_gridsearch(X_train, y_train, X_test, y_test, use_grid_search=True, manual_params=None):
    """
    Treina um modelo de Support Vector Regression (SVR) com ou sem otimiza√ß√£o de hiperpar√¢metros via GridSearchCV.

    Par√¢metros:
    -----------
    - X_train : array-like
        Matriz de features do conjunto de treino.
    - y_train : array-like
        Vetor de r√≥tulos do conjunto de treino.
    - X_test : array-like
        Matriz de features do conjunto de teste.
    - y_test : array-like
        Vetor de r√≥tulos do conjunto de teste.
    - use_grid_search : bool (padr√£o=True)
        Define se ser√° utilizada a busca de hiperpar√¢metros via GridSearchCV.
    - manual_params : dict (opcional)
        Par√¢metros especificados manualmente para substituir o GridSearch.

    Retorno:
    --------
    - dict:
        Dicion√°rio contendo as m√©tricas de desempenho do modelo treinado e os melhores hiperpar√¢metros encontrados.
    """
    try:
        # -------------------------------------
        # üìå 1. Padronizar os dados de entrada (necess√°rio para SVR)
        # -------------------------------------
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)  # Ajusta e transforma os dados de treino
        X_test_scaled = scaler.transform(X_test)  # Apenas transforma os dados de teste com os mesmos par√¢metros

        # -------------------------------------
        # üìå 2. Definir o modelo base SVR
        # -------------------------------------
        svr = SVR()

        # -------------------------------------
        # üìå 3. Definir o grid de hiperpar√¢metros padr√£o para SVR
        # -------------------------------------
        param_grid = {
            'C': [0.1, 1, 10, 100],  # Par√¢metro de regulariza√ß√£o
            'epsilon': [0.01, 0.1, 0.2],  # Margem de erro permitida
            'kernel': ['linear', 'rbf'],  # Tipos de kernel suportados
            'gamma': ['scale', 'auto']  # Ajuste da largura da fun√ß√£o kernel
        }

        # Se o utilizador forneceu par√¢metros manuais, substituir os valores no grid
        if manual_params:
            for param, value in manual_params.items():
                # Garante que o valor seja uma lista para compatibilidade com o GridSearchCV
                param_grid[param] = [value] if not isinstance(value, list) else value

        # -------------------------------------
        # üìå 4. Definir a estrat√©gia de valida√ß√£o cruzada
        # -------------------------------------
        cv_strategy = KFold(n_splits=5, shuffle=True, random_state=42)  # Divide os dados em 5 partes

        # -------------------------------------
        # üìå 5. Escolher entre GridSearchCV ou par√¢metros manuais
        # -------------------------------------
        if use_grid_search:
            # Executar GridSearchCV para encontrar os melhores hiperpar√¢metros
            grid_search = GridSearchCV(
                estimator=svr, 
                param_grid=param_grid, 
                cv=cv_strategy, 
                scoring='neg_mean_squared_error',  # Crit√©rio de avalia√ß√£o (erro quadr√°tico m√©dio negativo)
                n_jobs=-1  # Utilizar todos os processadores dispon√≠veis
            )
            grid_search.fit(X_train_scaled, y_train)

            # Melhor modelo encontrado pelo GridSearch
            best_model = grid_search.best_estimator_
            best_params = grid_search.best_params_
        
        else:
            # Aplicar par√¢metros manuais, caso existam
            if manual_params:
                svr.set_params(**manual_params)

            # Treinar o modelo diretamente sem GridSearch
            best_model = svr.fit(X_train_scaled, y_train)
            best_params = manual_params or {}

        # -------------------------------------
        # üìå 6. Fazer previs√µes no conjunto de teste
        # -------------------------------------
        y_pred = best_model.predict(X_test_scaled)

        # -------------------------------------
        # üìå 7. Calcular m√©tricas de desempenho
        # -------------------------------------
        mse = mean_squared_error(y_test, y_pred)  # Erro Quadr√°tico M√©dio
        mae = mean_absolute_error(y_test, y_pred)  # Erro Absoluto M√©dio
        r2 = r2_score(y_test, y_pred)  # R¬≤ Score (coeficiente de determina√ß√£o)

        # -------------------------------------
        # üìå 8. Criar um dicion√°rio com as m√©tricas do modelo
        # -------------------------------------
        metrics = {
            "Modelo": "Support Vector Regression (SVR)",
            "R¬≤": r2,
            "MAE": mae,
            "MSE": mse,
            "Best Parameters": best_params  # Hiperpar√¢metros utilizados
        }

        return metrics  # Retorna as m√©tricas para an√°lise
    
    except Exception as e:
        st.error(f"Erro ao treinar o modelo SVR: {str(e)}")  # Exibir erro no Streamlit caso ocorra
        return None


def train_model_with_gridsearch(model, param_grid, X_train, y_train, use_grid_search, manual_params=None, cv_choice="K-Fold"):
    """
    Treina um modelo de Machine Learning com ou sem otimiza√ß√£o de hiperpar√¢metros via GridSearchCV.

    Par√¢metros:
    -----------
    - model : objeto do modelo
        Modelo de Machine Learning a ser treinado (ex: RandomForest, SVC, SVR, etc.).
    - param_grid : dict
        Dicion√°rio contendo os hiperpar√¢metros a serem ajustados.
    - X_train : array-like
        Matriz de features do conjunto de treino.
    - y_train : array-like
        Vetor de r√≥tulos do conjunto de treino.
    - use_grid_search : bool
        Define se ser√° utilizada a busca de hiperpar√¢metros via GridSearchCV.
    - manual_params : dict (opcional)
        Par√¢metros especificados manualmente para substituir o GridSearch.
    - cv_choice : str (padr√£o="K-Fold")
        M√©todo de valida√ß√£o cruzada a ser utilizado.

    Retorno:
    --------
    - best_model : objeto do modelo treinado
        Melhor modelo encontrado ap√≥s o treino.
    - best_params : dict
        Dicion√°rio com os melhores hiperpar√¢metros utilizados.
    """
    try:
        # -------------------------------------
        # üìå 1. Inicializar par√¢metros manuais, caso n√£o tenham sido fornecidos
        # -------------------------------------
        if manual_params is None:
            manual_params = {}

        # Obter o nome do modelo
        model_name = type(model).__name__

        # Diagn√≥stico: Exibir par√¢metros no estado global antes do treino
        st.write("üîç Par√¢metros no estado global antes do treino:")
        st.write("‚úÖ best_params:", st.session_state.get('best_params', {}))
        st.write("‚úÖ manual_params:", st.session_state.get('manual_params', {}))

        # -------------------------------------
        # üìå 2. Carregar par√¢metros previamente guardados, se existirem
        # -------------------------------------
        saved_params = st.session_state.get('best_params', None)

        # Se houver par√¢metros guardados e GridSearch n√£o for utilizado, aplicar os par√¢metros salvos
        if saved_params and not use_grid_search:
            st.info(f"‚ÑπÔ∏è Aplicando par√¢metros salvos ao modelo: {saved_params}")
            model.set_params(**saved_params)

        # -------------------------------------
        # üìå 3. Ajustar manualmente par√¢metros incompat√≠veis
        # -------------------------------------
        # Se o modelo for SVM e o kernel for 'linear', o par√¢metro 'gamma' n√£o √© necess√°rio
        if manual_params.get("kernel") == "linear" and "gamma" in manual_params:
            del manual_params["gamma"]

            # Remover 'gamma' do estado global, se presente
            if 'gamma' in st.session_state.get('manual_params', {}):
                del st.session_state['manual_params']['gamma']

        # -------------------------------------
        # üìå 4. Treinar modelo com GridSearchCV (se ativado)
        # -------------------------------------
        if use_grid_search:
            # Atualizar o grid de hiperpar√¢metros com os valores fornecidos manualmente
            if manual_params:
                for param, value in manual_params.items():
                    if not isinstance(value, list):  # Garantir que o valor seja uma lista para compatibilidade com GridSearch
                        manual_params[param] = [value]
                param_grid.update(manual_params)

            # Definir estrat√©gia de valida√ß√£o cruzada
            cv_strategy = get_cv_strategy(cv_choice, X_train, y_train)

            # Definir m√©trica de avalia√ß√£o (R¬≤ para regress√£o, accuracy para classifica√ß√£o)
            scoring = 'r2' if model_name == "SVR" else 'accuracy'

            # Configurar GridSearchCV para encontrar os melhores hiperpar√¢metros
            grid_search = GridSearchCV(
                estimator=model, 
                param_grid=param_grid, 
                cv=cv_strategy, 
                scoring=scoring, 
                n_jobs=-1  # Utilizar todos os processadores dispon√≠veis
            )
            grid_search.fit(X_train, y_train)

            # Extrair melhor modelo e hiperpar√¢metros encontrados
            best_model = grid_search.best_estimator_
            best_params = grid_search.best_params_

            # Guardar os melhores par√¢metros no estado global
            st.session_state['best_params'] = best_params
            st.success(f"üéØ Melhores par√¢metros encontrados: {best_params}")

            return best_model, best_params

        # -------------------------------------
        # üìå 5. Treinar modelo sem GridSearch (caso desativado)
        # -------------------------------------
        else:
            # Filtrar apenas os par√¢metros v√°lidos para o modelo
            valid_params = model.get_params().keys()
            manual_params = {k: v for k, v in manual_params.items() if k in valid_params}

            # Aplicar os par√¢metros escolhidos manualmente
            model.set_params(**manual_params)

            # Treinar o modelo diretamente sem GridSearch
            model.fit(X_train, y_train)

            # Guardar os par√¢metros manuais no estado global
            st.session_state['manual_params'] = manual_params
            st.success(f"üìù Par√¢metros manuais salvos: {manual_params}")

            return model, manual_params

    # -------------------------------------
    # üìå 6. Capturar e exibir erros, caso ocorram
    # -------------------------------------
    except Exception as e:
        st.error(f"‚ùå Ocorreu um erro ao treinar o modelo: {str(e)}")
        return None, None


# Fun√ß√£o para calcular o Gap Statistic para o Clustering Hier√°rquico
def calculate_gap_statistic_hierarchical(X, n_clusters_range, n_ref=10):
    """
    Calcula a estat√≠stica de Gap (Gap Statistic) para o algoritmo AgglomerativeClustering.

    Par√¢metros:
    -----------
    - X (ndarray): Dados de entrada no formato (n_samples x n_features).
    - n_clusters_range (tuple): Intervalo de n√∫meros de clusters a serem avaliados, ex: (2, 10).
    - n_ref (int, padr√£o=10): N√∫mero de amostras de refer√™ncia aleat√≥rias geradas para c√°lculo do Gap.

    Retorno:
    --------
    - gap_scores (list): Lista com os valores de Gap Statistic para cada n√∫mero de clusters avaliado.
    """
    # -------------------------------------
    # üìå 1. Normalizar os dados antes do clustering
    # -------------------------------------
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Lista para armazenar os valores de Gap Statistic
    gap_scores = []

    # -------------------------------------
    # üìå 2. Avaliar diferentes n√∫meros de clusters
    # -------------------------------------
    for n_clusters in range(n_clusters_range[0], n_clusters_range[1] + 1):
        # **Ajustar o modelo AgglomerativeClustering aos dados reais**
        model = AgglomerativeClustering(n_clusters=n_clusters)
        model.fit(X_scaled)
        labels = model.labels_

        # **Calcular a soma das dist√¢ncias intra-cluster**
        intra_cluster_dist = sum([
            np.sum(np.linalg.norm(X_scaled[labels == i] - X_scaled[labels == i].mean(axis=0), axis=1))
            for i in range(n_clusters)
        ])

        # -------------------------------------
        # üìå 3. Criar conjuntos de dados de refer√™ncia aleat√≥rios
        # -------------------------------------
        ref_inertias = []
        for _ in range(n_ref):
            # Gerar dados aleat√≥rios no mesmo espa√ßo dimensional
            random_data = np.random.random_sample(size=X_scaled.shape)

            # Aplicar AgglomerativeClustering nos dados aleat√≥rios
            random_model = AgglomerativeClustering(n_clusters=n_clusters)
            random_model.fit(random_data)
            ref_labels = random_model.labels_

            # **Calcular a soma das dist√¢ncias intra-cluster para os dados aleat√≥rios**
            ref_inertia = sum([
                np.sum(np.linalg.norm(random_data[ref_labels == i] - random_data[ref_labels == i].mean(axis=0), axis=1))
                for i in range(n_clusters)
            ])
            ref_inertias.append(ref_inertia)

        # -------------------------------------
        # üìå 4. Calcular a estat√≠stica de Gap
        # -------------------------------------
        # M√©dia e desvio padr√£o das in√©rcias dos clusters aleat√≥rios
        ref_inertia_mean = np.mean(ref_inertias)
        ref_inertia_std = np.std(ref_inertias)

        # Gap Statistic: diferen√ßa entre a in√©rcia real e a m√©dia das in√©rcias aleat√≥rias
        gap = np.log(ref_inertia_mean) - np.log(intra_cluster_dist)
        gap_scores.append(gap)

    return gap_scores


# Fun√ß√£o para a sele√ß√£o e treino de modelos
def model_selection():
    """
    Esta fun√ß√£o permite ao utilizador selecionar e treinar um modelo de Machine Learning 
    atrav√©s da interface do Streamlit.
    """
    st.subheader("Sele√ß√£o e Treino de Modelos")

    # üìå 1. Verifica√ß√£o se os dados est√£o dispon√≠veis
    if 'data' not in st.session_state or st.session_state.data is None:
        st.error("Dados n√£o encontrados. Por favor, carregue os dados primeiro.")
        return

    # Obter os dados e as colunas dispon√≠veis
    data = st.session_state.data
    columns = data.columns.tolist()

    # üìå 2. Inicializar vari√°veis de estado caso n√£o existam
    if 'target_column' not in st.session_state:
        st.session_state.target_column = None
    if 'target_column_confirmed' not in st.session_state:
        st.session_state.target_column_confirmed = False
    if 'validation_method' not in st.session_state:
        st.session_state.validation_method = None
    if 'validation_confirmed' not in st.session_state:
        st.session_state.validation_confirmed = False
    if 'model_type' not in st.session_state:
        st.session_state.model_type = None
    if 'model_type_confirmed' not in st.session_state:
        st.session_state.model_type_confirmed = False
    if 'X_train' not in st.session_state:
        st.session_state.X_train = None
    if 'X_test' not in st.session_state:
        st.session_state.X_test = None
    if 'y_train' not in st.session_state:
        st.session_state.y_train = None
    if 'y_test' not in st.session_state:
        st.session_state.y_test = None
    if 'feature_selection_done' not in st.session_state:
        st.session_state.feature_selection_done = False

    # üìå 3. Configura√ß√µes gerais
    st.write("### Configura√ß√µes")

    # üìå 4. Escolha do Tipo de Modelo
    if not st.session_state.model_type_confirmed:
        st.write("Escolha o Tipo de Modelo")
        model_types = ["Classifica√ß√£o", "Regress√£o", "Clustering"]
        st.session_state.model_type = st.selectbox("Selecione o tipo de modelo", model_types)

        if st.button("Confirmar Tipo de Modelo"):
            st.session_state.model_type_confirmed = True
            st.success("Tipo de modelo confirmado!")

    # üìå 5. Escolha do Modelo Espec√≠fico
    if st.session_state.model_type_confirmed and not st.session_state.selected_model_name:
        st.write("Selecione o(s) Modelo(s)")

        # Dicion√°rio com os modelos dispon√≠veis para cada tipo
        if st.session_state.model_type == "Classifica√ß√£o":
            models = {
                "Support Vector Classification (SVC)": SVC(),
                "K-Nearest Neighbors (KNN)": KNeighborsClassifier(),
                "Random Forest": RandomForestClassifier()
            }
        elif st.session_state.model_type == "Regress√£o":
            models = {
                "Regress√£o Linear Simples (RLS)": LinearRegression(),
                "Regress√£o por Vetores de Suporte (SVR)": SVR(),
            }
        elif st.session_state.model_type == "Clustering":
            models = {
                "KMeans": KMeans(),
                "Clustering Hier√°rquico": AgglomerativeClustering(linkage='ward'),
            }

        # Armazena os modelos no estado da sess√£o para uso posterior
        st.session_state.models = models

        # üìå 6. Criar lista de op√ß√µes de modelos dispon√≠veis
        model_options = list(models.keys())  # Lista com os nomes dos modelos

        # Definir o modelo predefinido para evitar erro de √≠ndice
        default_model_name = st.session_state.get("model_name", model_options[0])

        # Criar menu de sele√ß√£o do modelo
        model_name = st.selectbox(
            "Selecione o modelo", 
            options=model_options, 
            key="model_name_selectbox", 
            index=model_options.index(default_model_name)
        )

        # Atualizar o estado global do modelo selecionado
        st.session_state["model_name"] = model_name
        st.session_state.model_name = model_name

        # üìå 7. Bot√£o para confirmar o modelo selecionado
        if st.button("Confirmar Modelo"):
            if model_name:  # Verifica se um modelo foi selecionado
                st.session_state.selected_model_name = model_name
                st.success(f"Modelo selecionado: {st.session_state.selected_model_name}")
            else:
                st.warning("Selecione um modelo antes de continuar.")

    # Fun√ß√£o para a configura√ß√£o de Clustering
    import pandas as pd
    from sklearn.decomposition import PCA
    import numpy as np
    
    # Inicializar a vari√°vel best_n_clusters_retrain com um valor padr√£o
    best_n_clusters_retrain = None
    
    # Inicializar estados se ainda n√£o existirem
    if 'pca_configured' not in st.session_state:
        st.session_state.pca_configured = False
    if 'ready_for_clustering' not in st.session_state:
        st.session_state.ready_for_clustering = False
    
    # Verifica se o modelo selecionado √© de Clustering e se h√° um modelo escolhido
    if st.session_state.model_type == "Clustering" and st.session_state.selected_model_name:
        st.write("### Configura√ß√£o para Clustering")
    
        # Codificar vari√°veis categ√≥ricas para representa√ß√£o num√©rica
        X = pd.get_dummies(st.session_state.data)
    
        # Padronizar os dados para melhorar a efic√°cia dos algoritmos de clustering
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # ETAPA 1: Configura√ß√£o do PCA para Clustering Hier√°rquico
        if st.session_state.selected_model_name == "Clustering Hier√°rquico" and not st.session_state.pca_configured:
            st.write("### Redu√ß√£o de Dimensionalidade com PCA para Clustering Hier√°rquico")
            
            # Verificar se o dataset √© grande o suficiente para exigir PCA
            if X.shape[0] > 1000 or X.shape[1] > 10:
                st.warning(f"Aten√ß√£o: O seu dataset tem {X.shape[0]} registos e {X.shape[1]} dimens√µes. A aplica√ß√£o de PCA pode ser necess√°ria para otimizar o desempenho do Clustering Hier√°rquico.")
            
            # Permitir ao utilizador escolher o n√∫mero de componentes ou utilizar um valor autom√°tico
            use_auto_components = st.checkbox("Determinar automaticamente o n√∫mero de componentes", value=True, key="auto_comp_hierarch")
            
            if use_auto_components:
                # Calcular o PCA para determinar a vari√¢ncia explicada
                pca_full = PCA().fit(X_scaled)
                explained_variance_ratio = pca_full.explained_variance_ratio_
                cumulative_variance = np.cumsum(explained_variance_ratio)
                
                # Determinar o n√∫mero de componentes que explicam pelo menos 90% da vari√¢ncia
                n_components = np.argmax(cumulative_variance >= 0.9) + 1
                n_components = min(n_components, 10)  # Limitar a no m√°ximo 10 componentes
                
                st.write(f"N√∫mero de componentes selecionados automaticamente: {n_components} (explica aproximadamente {cumulative_variance[n_components-1]*100:.1f}% da vari√¢ncia)")
                
                # Criar um gr√°fico para visualizar a vari√¢ncia explicada
                fig, ax = plt.subplots(figsize=(8, 4))
                ax.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='-')
                ax.axhline(y=0.9, color='r', linestyle='--', label='90% Vari√¢ncia Explicada')
                ax.axvline(x=n_components, color='g', linestyle='--', label=f'{n_components} Componentes')
                ax.set_xlabel('N√∫mero de Componentes')
                ax.set_ylabel('Vari√¢ncia Explicada Acumulada')
                ax.set_title('Vari√¢ncia Explicada por Componentes do PCA')
                ax.legend()
                st.pyplot(fig)
                plt.clf()
            else:
                # Permitir que o utilizador escolha manualmente o n√∫mero de componentes
                max_components = min(X.shape[1], 20)  # Limitar ao n√∫mero de features ou 20, o que for menor
                n_components = st.slider("N√∫mero de componentes PCA para Hier√°rquico", 2, max_components, value=min(3, max_components), key="n_comp_hierarch")
            
            # Bot√£o para confirmar a configura√ß√£o do PCA
            if st.button("Confirmar Configura√ß√£o do PCA para Clustering Hier√°rquico"):
                # Aplicar PCA com o n√∫mero de componentes escolhido
                pca = PCA(n_components=n_components)
                X_pca = pca.fit_transform(X_scaled)
                
                # Guardar os dados transformados e as configura√ß√µes no estado da sess√£o
                st.session_state.X_pca = X_pca
                st.session_state.pca_n_components = n_components
                st.session_state.pca_configured = True
                st.session_state.pca_model = pca
                st.session_state.explained_variance = pca.explained_variance_ratio_
                
                st.success(f"PCA configurado com sucesso! Dimensionalidade reduzida de {X_scaled.shape[1]} para {X_pca.shape[1]} componentes.")
                
                # Visualizar os dados ap√≥s a aplica√ß√£o do PCA se tivermos pelo menos 2 componentes
                if n_components >= 2:
                    st.write("### Visualiza√ß√£o dos Dados Ap√≥s PCA")
                    
                    # Permitir ao utilizador escolher os componentes a visualizar
                    available_components = min(n_components, 10)  # Limitar a 10 para evitar sobrecarga
                    
                    component_x = st.selectbox(
                        "Escolha o componente para o eixo X:",
                        options=list(range(available_components)),
                        format_func=lambda x: f"Componente {x+1}",
                        index=0,
                        key="comp_x_hierarch"
                    )
                    
                    component_y = st.selectbox(
                        "Escolha o componente para o eixo Y:",
                        options=list(range(available_components)),
                        format_func=lambda x: f"Componente {x+1}",
                        index=1 if available_components > 1 else 0,
                        key="comp_y_hierarch"
                    )
                    
                    # Criar um gr√°fico de dispers√£o com os componentes escolhidos
                    fig, ax = plt.subplots(figsize=(10, 6))
                    scatter = ax.scatter(X_pca[:, component_x], X_pca[:, component_y], alpha=0.7)
                    ax.set_xlabel(f'Componente Principal {component_x+1}', fontsize=12)
                    ax.set_ylabel(f'Componente Principal {component_y+1}', fontsize=12)
                    ax.set_title(f'Visualiza√ß√£o 2D dos Componentes PCA {component_x+1} e {component_y+1}', fontsize=14, fontweight='bold')
                    ax.grid(True, linestyle='--', alpha=0.7)
                    
                    # Mostrar a vari√¢ncia explicada por cada componente escolhido
                    if hasattr(pca, 'explained_variance_ratio_'):
                        var_x = pca.explained_variance_ratio_[component_x] * 100
                        var_y = pca.explained_variance_ratio_[component_y] * 100
                        ax.set_xlabel(f'Componente Principal {component_x+1} ({var_x:.1f}% vari√¢ncia)', fontsize=12)
                        ax.set_ylabel(f'Componente Principal {component_y+1} ({var_y:.1f}% vari√¢ncia)', fontsize=12)
                    
                    plt.tight_layout()
                    st.pyplot(fig)
                    plt.clf()
    
            # Bot√£o para avan√ßar para a configura√ß√£o do clustering
            if st.button("Prosseguir para Clustering"):
                st.session_state.ready_for_clustering = True
                st.rerun()
           
        # ETAPA 1: Configura√ß√£o do PCA para KMeans
        if st.session_state.selected_model_name == "KMeans" and not st.session_state.pca_configured:
            st.write("### Redu√ß√£o de Dimensionalidade com PCA")
            
            # Verificar se o dataset √© grande e pode beneficiar do PCA
            if X.shape[0] > 1000 or X.shape[1] > 10:
                st.warning(f"Aten√ß√£o: O seu dataset tem {X.shape[0]} registos e {X.shape[1]} dimens√µes. A aplica√ß√£o de PCA √© altamente recomendada para melhorar a efici√™ncia do modelo.")
        
            # Permitir ao utilizador escolher entre uma determina√ß√£o autom√°tica ou manual do n√∫mero de componentes
            use_auto_components = st.checkbox("Determinar automaticamente o n√∫mero de componentes", value=True)
        
            if use_auto_components:
                # Calcular o PCA para determinar a vari√¢ncia explicada por cada componente
                pca_full = PCA().fit(X_scaled)
                explained_variance_ratio = pca_full.explained_variance_ratio_
                cumulative_variance = np.cumsum(explained_variance_ratio)
        
                # Determinar o n√∫mero de componentes necess√°rios para explicar pelo menos 90% da vari√¢ncia total
                n_components = np.argmax(cumulative_variance >= 0.9) + 1
                n_components = min(n_components, 10)  # Limitar a no m√°ximo 10 componentes
        
                st.write(f"N√∫mero de componentes selecionados automaticamente: {n_components} (explica aproximadamente {cumulative_variance[n_components-1]*100:.1f}% da vari√¢ncia)")
                
                # Criar um gr√°fico para visualizar a vari√¢ncia explicada pelos componentes do PCA
                fig, ax = plt.subplots(figsize=(8, 4))
                ax.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='-')
                ax.axhline(y=0.9, color='r', linestyle='--', label='90% Vari√¢ncia Explicada')
                ax.axvline(x=n_components, color='g', linestyle='--', label=f'{n_components} Componentes')
                ax.set_xlabel('N√∫mero de Componentes')
                ax.set_ylabel('Vari√¢ncia Explicada Acumulada')
                ax.set_title('Vari√¢ncia Explicada por Componentes do PCA')
                ax.legend()
                st.pyplot(fig)
                plt.clf()
            else:
                # Permitir ao utilizador selecionar manualmente o n√∫mero de componentes a utilizar
                max_components = min(X.shape[1], 20)  # Limitar ao n√∫mero de features ou 20, o que for menor
                n_components = st.slider("N√∫mero de componentes PCA", 2, max_components, value=min(3, max_components))
        
            # Bot√£o para confirmar a configura√ß√£o do PCA
            if st.button("Confirmar Configura√ß√£o do PCA"):
                # Aplicar o PCA com o n√∫mero de componentes escolhido
                pca = PCA(n_components=n_components)
                X_pca = pca.fit_transform(X_scaled)
        
                # Guardar os dados transformados e as configura√ß√µes no estado da sess√£o
                st.session_state.X_pca = X_pca
                st.session_state.pca_n_components = n_components
                st.session_state.pca_configured = True
                st.session_state.pca_model = pca
                st.session_state.explained_variance = pca.explained_variance_ratio_
        
                st.success(f"PCA configurado com sucesso! Dimensionalidade reduzida de {X_scaled.shape[1]} para {X_pca.shape[1]} componentes.")
        
                # Visualiza√ß√£o 2D dos dados ap√≥s PCA, caso tenhamos pelo menos 2 componentes
                if n_components >= 2:
                    st.write("### Visualiza√ß√£o dos Dados Ap√≥s PCA")
        
                    # Permitir ao utilizador escolher os componentes a visualizar
                    available_components = min(n_components, 10)  # Limitar a 10 para evitar sobrecarga
        
                    component_x = st.selectbox(
                        "Escolha o componente para o eixo X:",
                        options=list(range(available_components)),
                        format_func=lambda x: f"Componente {x+1}",
                        index=0
                    )
        
                    component_y = st.selectbox(
                        "Escolha o componente para o eixo Y:",
                        options=list(range(available_components)),
                        format_func=lambda x: f"Componente {x+1}",
                        index=1 if available_components > 1 else 0
                    )
        
                    # Criar um gr√°fico de dispers√£o com os componentes escolhidos
                    fig, ax = plt.subplots(figsize=(10, 6))
                    scatter = ax.scatter(X_pca[:, component_x], X_pca[:, component_y], alpha=0.7)
                    ax.set_xlabel(f'Componente Principal {component_x+1}', fontsize=12)
                    ax.set_ylabel(f'Componente Principal {component_y+1}', fontsize=12)
                    ax.set_title(f'Visualiza√ß√£o 2D dos Componentes PCA {component_x+1} e {component_y+1}', fontsize=14, fontweight='bold')
                    ax.grid(True, linestyle='--', alpha=0.7)
        
                    # Exibir a vari√¢ncia explicada pelos componentes selecionados, se dispon√≠vel
                    if hasattr(pca, 'explained_variance_ratio_'):
                        var_x = pca.explained_variance_ratio_[component_x] * 100
                        var_y = pca.explained_variance_ratio_[component_y] * 100
                        ax.set_xlabel(f'Componente Principal {component_x+1} ({var_x:.1f}% vari√¢ncia)', fontsize=12)
                        ax.set_ylabel(f'Componente Principal {component_y+1} ({var_y:.1f}% vari√¢ncia)', fontsize=12)
        
                    plt.tight_layout()
                    st.pyplot(fig)
                    plt.clf()
        
                # Bot√£o para avan√ßar para a configura√ß√£o do clustering
                if st.button("Prosseguir para Clustering"):
                    st.session_state.ready_for_clustering = True
                    st.rerun()
    
        # ETAPA 2: Configura√ß√£o do Clustering (ap√≥s o PCA para Hierarchical ou diretamente para K-means)
        elif st.session_state.selected_model_name == "KMeans" or (st.session_state.selected_model_name == "Clustering Hier√°rquico" and st.session_state.pca_configured):
            
            # Escolher o intervalo de clusters a explorar (reduzido para 2-10 por padr√£o para evitar processamento excessivo)
            num_clusters_range = st.slider("Intervalo de clusters para explorar (para an√°lise)", 2, 10, (2, 6))
        
            # Definir os dados de treino conforme o m√©todo de clustering escolhido
            if st.session_state.selected_model_name == "Clustering Hier√°rquico":
                # Se for Clustering Hier√°rquico, usar os dados transformados pelo PCA
                training_data = st.session_state.X_pca
            else:
                # Se for K-Means, utilizar os dados normalizados sem PCA
                training_data = X_scaled
        
            # Op√ß√£o para utilizar amostragem, permitindo uma an√°lise mais r√°pida
            use_sampling = st.checkbox("Usar amostragem dos dados para an√°lise mais r√°pida", value=True)
            if use_sampling:
                # Permitir ao utilizador selecionar o tamanho da amostra para an√°lise
                sample_size = st.slider("Tamanho da amostra", 
                                        min_value=min(100, training_data.shape[0]),
                                        max_value=min(2000, training_data.shape[0]),
                                        value=min(1000, training_data.shape[0]))
                
                # Realizar a amostragem aleat√≥ria dos dados
                np.random.seed(42)  # Para garantir reprodutibilidade dos resultados
                sample_indices = np.random.choice(training_data.shape[0], sample_size, replace=False)
                analysis_data = training_data[sample_indices]
                st.info(f"Usando {sample_size} pontos ({sample_size/training_data.shape[0]:.1%} dos dados) para an√°lise.")
            else:
                # Caso a amostragem n√£o seja ativada, utilizar todos os dados dispon√≠veis
                analysis_data = training_data
        
            # In√≠cio da an√°lise para determinar o n√∫mero ideal de clusters
            st.write("### An√°lise para Determina√ß√£o do N√∫mero de Clusters")
        
            # Inicializar listas para armazenar as m√©tricas de avalia√ß√£o dos clusters
            silhouette_scores = []
            davies_bouldin_scores = []
            calinski_harabasz_scores = []
        
            # Criar uma barra de progresso e um espa√ßo para atualizar o status do processamento
            progress_bar = st.progress(0)
            status_text = st.empty()
        
            # Calcular m√©tricas para cada n√∫mero de clusters dentro do intervalo selecionado
            total_iterations = num_clusters_range[1] - num_clusters_range[0] + 1
        
            # Loop para testar diferentes quantidades de clusters
            for i, n_clusters in enumerate(range(num_clusters_range[0], num_clusters_range[1] + 1)):
                # Atualizar a barra de progresso
                progress = (i + 1) / total_iterations
                progress_bar.progress(progress)
                status_text.text(f"Analisando com {n_clusters} clusters... ({i+1}/{total_iterations})")
        
                try:
                    # Verificar qual m√©todo de clustering foi escolhido
                    if st.session_state.selected_model_name == "KMeans":
                        # Para KMeans, otimizar os hiperpar√¢metros reduzindo n_init e max_iter
                        temp_model = KMeans(n_clusters=n_clusters, random_state=42, n_init=5, max_iter=100)
                    else:
                        # Para Clustering Hier√°rquico, utilizar o m√©todo de liga√ß√£o "ward"
                        temp_model = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')
        
                    # Treinar o modelo com os dados amostrados
                    temp_model.fit(analysis_data)
                    labels = temp_model.labels_
        
                    # Calcular e armazenar as m√©tricas de avalia√ß√£o do clustering
                    silhouette_scores.append(silhouette_score(analysis_data, labels))
                    davies_bouldin_scores.append(davies_bouldin_score(analysis_data, labels))
                    calinski_harabasz_scores.append(calinski_harabasz_score(analysis_data, labels))
        
                except Exception as e:
                    # Caso ocorra um erro durante a execu√ß√£o, mostrar mensagem ao utilizador
                    st.error(f"Erro ao processar {n_clusters} clusters: {str(e)}")
                    # Preencher com valores neutros para manter a estrutura do array
                    silhouette_scores.append(0)
                    davies_bouldin_scores.append(float('inf'))
                    calinski_harabasz_scores.append(0)
        
            # Limpar barra de progresso e status ap√≥s a conclus√£o
            status_text.empty()
            progress_bar.empty()
        
            # Criar um DataFrame com os resultados das m√©tricas calculadas
            metrics_df = pd.DataFrame({
                "N√∫mero de Clusters": range(num_clusters_range[0], num_clusters_range[1] + 1),
                "Silhouette Score": silhouette_scores,
                "Davies-Bouldin Index": davies_bouldin_scores,
                "Calinski-Harabasz Score": calinski_harabasz_scores,
            })
        
            # Exibir a tabela de m√©tricas no Streamlit
            st.write("#### Tabela de M√©tricas por N√∫mero de Clusters")
            st.dataframe(fix_dataframe_types(metrics_df.style.format({
                "Silhouette Score": "{:.2f}",
                "Davies-Bouldin Index": "{:.2f}",
                "Calinski-Harabasz Score": "{:.2f}",
            })))
        
            # Exibir gr√°ficos das m√©tricas para facilitar a interpreta√ß√£o visual
            st.write("#### Gr√°ficos das M√©tricas por N√∫mero de Clusters")
            
            # Criar colunas para organizar a exibi√ß√£o dos gr√°ficos
            col1, col2, col3 = st.columns(3)
        
            # Gr√°fico do Silhouette Score
            with col1:
                plt.figure(figsize=(6, 4))
                plt.plot(metrics_df["N√∫mero de Clusters"], metrics_df["Silhouette Score"], marker='o')
                plt.title("Silhouette Score por N√∫mero de Clusters")
                plt.xlabel("N√∫mero de Clusters")
                plt.ylabel("Silhouette Score")
                st.pyplot(plt.gcf())
                plt.clf()
        
            # Gr√°fico do Davies-Bouldin Index
            with col2:
                plt.figure(figsize=(6, 4))
                plt.plot(metrics_df["N√∫mero de Clusters"], metrics_df["Davies-Bouldin Index"], marker='o')
                plt.title("Davies-Bouldin Index por N√∫mero de Clusters")
                plt.xlabel("N√∫mero de Clusters")
                plt.ylabel("Davies-Bouldin Index")
                st.pyplot(plt.gcf())
                plt.clf()
        
            # Gr√°fico do Calinski-Harabasz Score
            with col3:
                plt.figure(figsize=(6, 4))
                plt.plot(metrics_df["N√∫mero de Clusters"], metrics_df["Calinski-Harabasz Score"], marker='o')
                plt.title("Calinski-Harabasz Score por N√∫mero de Clusters")
                plt.xlabel("N√∫mero de Clusters")
                plt.ylabel("Calinski-Harabasz Score")
                st.pyplot(plt.gcf())
                plt.clf()

            # Escolher o melhor n√∫mero de clusters com base no Silhouette Score
            if silhouette_scores and any(score > 0 for score in silhouette_scores):
                best_n_clusters = metrics_df.loc[metrics_df["Silhouette Score"].idxmax(), "N√∫mero de Clusters"]
                st.write(f"**Melhor N√∫mero de Clusters** (com base no Silhouette Score): {best_n_clusters}")
                best_n_clusters_retrain = best_n_clusters
            
            # Permitir ao utilizador escolher a abordagem para determinar o n√∫mero de clusters
            st.write("### Escolha a Abordagem para Determinar o N√∫mero de Clusters")
            method = st.radio("Selecione a abordagem:", ["Autom√°tico", "Manual"], key="initial_training_method")
            
            if method == "Autom√°tico":
                # Determinar automaticamente o melhor n√∫mero de clusters com base no Silhouette Score
                if silhouette_scores and any(score > 0 for score in silhouette_scores):
                    best_n_clusters = range(num_clusters_range[0], num_clusters_range[1] + 1)[np.argmax(silhouette_scores)]
                    best_n_clusters_retrain = best_n_clusters  # Atualizar o valor para re-treino
                else:
                    # Caso a determina√ß√£o autom√°tica falhe, exibir erro e atribuir um valor padr√£o
                    st.error("N√£o foi poss√≠vel determinar automaticamente o n√∫mero de clusters. Por favor, selecione manualmente.")
                    best_n_clusters_retrain = 3  # Valor padr√£o
            
            elif method == "Manual":
                # Permitir ao utilizador escolher manualmente o n√∫mero de clusters
                best_n_clusters = st.slider("Escolha o n√∫mero de clusters", num_clusters_range[0], num_clusters_range[1], value=3)
                best_n_clusters_retrain = best_n_clusters  # Atualizar o valor para re-treino
            
            # Garantir que `best_n_clusters_retrain` tenha um valor v√°lido antes de continuar
            if best_n_clusters_retrain is None:
                st.warning("Por favor, selecione uma abordagem para determinar o n√∫mero de clusters.")
            else:
                # Treinar o modelo inicial
                if st.button(f"Treinar Modelo Inicial"):
                    # Configurar o modelo de clustering escolhido
                    if st.session_state.selected_model_name == "Clustering Hier√°rquico":
                        model = st.session_state.models["Clustering Hier√°rquico"]
                        model.set_params(n_clusters=best_n_clusters_retrain, linkage='ward')
                    else:  # KMeans
                        model = st.session_state.models["KMeans"]
                        # Ajustar hiperpar√¢metros para otimiza√ß√£o no treino final
                        model.set_params(n_clusters=best_n_clusters_retrain, n_init=5, max_iter=300)
            
                    # Barra de progresso para o treino do modelo
                    with st.spinner(f"Treinando o modelo com {best_n_clusters_retrain} clusters..."):
                        model.fit(training_data)
                        st.session_state.clustering_labels = model.labels_
            
                    # Calcular m√©tricas de avalia√ß√£o do clustering
                    st.session_state.initial_metrics = {
                        "N√∫mero de Clusters": best_n_clusters_retrain,
                        "Silhouette Score": silhouette_score(training_data, st.session_state.clustering_labels),
                        "Davies-Bouldin Index": davies_bouldin_score(training_data, st.session_state.clustering_labels),
                        "Calinski-Harabasz Score": calinski_harabasz_score(training_data, st.session_state.clustering_labels)
                    }
            
                    # Guardar informa√ß√µes importantes no estado da sess√£o
                    st.session_state.training_data = training_data
                    st.session_state.training_completed = True
                    st.session_state.trained_model = model  # Guardar o modelo treinado
            
                    # Exibir mensagem de sucesso conforme o m√©todo escolhido
                    if st.session_state.selected_model_name == "Clustering Hier√°rquico":
                        st.success(f"Modelo hier√°rquico treinado com sucesso usando {best_n_clusters_retrain} clusters e {st.session_state.pca_n_components} componentes PCA!")
                    else:
                        st.success(f"Modelo K-Means treinado com sucesso usando {best_n_clusters_retrain} clusters!")
            
            # Exibir m√©tricas e visualiza√ß√£o apenas ap√≥s o treino do modelo
            if st.session_state.get("training_completed", False):
                st.write("### M√©tricas do Treino Inicial")
                st.table(fix_dataframe_types(pd.DataFrame([st.session_state.initial_metrics])))
            
                # Visualiza√ß√£o dos clusters treinados
                if 'clustering_labels' in st.session_state:
                    st.write("### Visualiza√ß√£o dos Clusters")
            
                    # Para K-Means, mostrar os centroides dos clusters
                    if st.session_state.selected_model_name == "KMeans":
                        if "trained_model" in st.session_state and hasattr(st.session_state.trained_model, 'cluster_centers_'):
                            st.write("#### Centroides dos Clusters")
                            centroids = st.session_state.trained_model.cluster_centers_
                            
                            # Exibir apenas as primeiras 10 dimens√µes, se existirem muitas dimens√µes
                            if centroids.shape[1] > 10:
                                st.write(f"(Mostrando apenas as primeiras 10 dimens√µes de {centroids.shape[1]})")
                                centroids_df = pd.DataFrame(centroids[:, :10])
                            else:
                                centroids_df = pd.DataFrame(centroids)
            
                            st.dataframe(fix_dataframe_types(centroids_df))
            
                    # Preparar dados para visualiza√ß√£o dos clusters
                    if st.session_state.selected_model_name == "Clustering Hier√°rquico":
                        # Para Clustering Hier√°rquico, utilizar os dados reduzidos pelo PCA
                        plot_data = st.session_state.X_pca
                    else:
                        # Para K-Means, reduzir dimensionalidade se necess√°rio
                        if X_scaled.shape[1] > 3:
                            pca_viz = PCA(n_components=3)
                            plot_data = pca_viz.fit_transform(X_scaled)
                            st.write("(Dados reduzidos via PCA para visualiza√ß√£o)")
                        else:
                            plot_data = X_scaled
            
                    # Obter o n√∫mero total de componentes dispon√≠veis para visualiza√ß√£o
                    total_components = plot_data.shape[1]
            
                    # Permitir ao utilizador escolher os componentes para visualiza√ß√£o
                    st.write("### Escolha os Componentes para Visualiza√ß√£o")
                    col1, col2 = st.columns(2)
            
                    with col1:
                        x_component = st.selectbox(
                            "Componente para o Eixo X",
                            list(range(total_components)),
                            index=0,
                            format_func=lambda x: f"Componente {x+1}",
                            key="initial_x_component"
                        )
            
                    with col2:
                        y_component = st.selectbox(
                            "Componente para o Eixo Y",
                            list(range(total_components)),
                            index=1 if total_components > 1 else 0,
                            format_func=lambda x: f"Componente {x+1}",
                            key="initial_y_component"
                        )
            
                    # Verificar se os componentes escolhidos s√£o diferentes
                    if x_component == y_component:
                        st.warning("Por favor, selecione componentes diferentes para X e Y.")
                    else:
                        # Criar gr√°fico de dispers√£o para visualiza√ß√£o dos clusters
                        fig, ax = plt.subplots(figsize=(10, 6))
                        scatter = ax.scatter(
                            plot_data[:, x_component],
                            plot_data[:, y_component],
                            c=st.session_state.clustering_labels,
                            cmap='viridis',
                            alpha=0.7
                        )
                        ax.set_title(f'Visualiza√ß√£o 2D dos Clusters ({best_n_clusters_retrain} clusters)')
                        ax.set_xlabel(f'Componente {x_component+1}')
                        ax.set_ylabel(f'Componente {y_component+1}')
                        legend = ax.legend(*scatter.legend_elements(), title="Clusters")
                        ax.add_artist(legend)
                        st.pyplot(fig)
                        plt.clf()
            
                # Op√ß√£o para o utilizador escolher a a√ß√£o seguinte
                next_action = st.selectbox(
                    "Selecione a pr√≥xima a√ß√£o:",
                    ["Re-Treinar o Modelo", "Finalizar"]
                )
            
                # Bot√£o para confirmar a escolha do utilizador
                if st.button("Confirmar Escolha"):
                    if next_action == "Finalizar":
                        st.session_state.step = 'clustering_final_page'
                        st.rerun()
                    elif next_action == "Re-Treinar o Modelo":
                        st.session_state.retrain_mode = True

            # Re-Treinar o Modelo (s√≥ aparece se o utilizador escolher esta op√ß√£o)
            if st.session_state.get("retrain_mode", False):
                st.write("### Re-Treino do Modelo")
                
                # Escolha do m√©todo para determinar o n√∫mero de clusters no re-treino
                retrain_method = st.radio(
                    "Escolha a Abordagem para Determinar o N√∫mero de Clusters no novo treino:",
                    ["Autom√°tico", "Manual"]
                )
            
                if retrain_method == "Manual":
                    # Permitir ao utilizador escolher manualmente o n√∫mero de clusters
                    st.session_state.num_clusters = st.slider(
                        "Selecione o n√∫mero de clusters para o re-treino",
                        min_value=2,
                        max_value=20,
                        value=st.session_state.num_clusters if "num_clusters" in st.session_state else 3
                    )
                    best_n_clusters_retrain = st.session_state.num_clusters
            
                elif retrain_method == "Autom√°tico":
                    # Determinar o melhor n√∫mero de clusters com base no Silhouette Score
                    if silhouette_scores and any(score > 0 for score in silhouette_scores):
                        best_n_clusters_retrain = range(num_clusters_range[0], num_clusters_range[1] + 1)[np.argmax(silhouette_scores)]
                    else:
                        # Caso a determina√ß√£o autom√°tica falhe, exibir erro e atribuir um valor padr√£o
                        st.error("N√£o foi poss√≠vel determinar automaticamente o n√∫mero de clusters. Por favor, selecione manualmente.")
                        best_n_clusters_retrain = 3  # Valor padr√£o
            
                # Bot√£o para executar o re-treino do modelo
                if st.button("Treinar Novamente"):
                    # Selecionar o modelo previamente escolhido pelo utilizador
                    model = st.session_state.models[st.session_state.selected_model_name]
                    
                    # Configurar o modelo com o novo n√∫mero de clusters
                    if st.session_state.selected_model_name == "Clustering Hier√°rquico":
                        model.set_params(n_clusters=best_n_clusters_retrain, linkage='ward')
                    else:
                        model.set_params(n_clusters=best_n_clusters_retrain, n_init=5, max_iter=300)
            
                    # Treinar o modelo com uma barra de progresso para indicar o progresso ao utilizador
                    with st.spinner(f"Realizando re-treino com {best_n_clusters_retrain} clusters..."):
                        model.fit(st.session_state.training_data)
            
                    # Calcular m√©tricas de avalia√ß√£o do clustering ap√≥s o re-treino
                    st.session_state.retrain_metrics = {
                        "N√∫mero de Clusters": best_n_clusters_retrain,
                        "Silhouette Score": silhouette_score(st.session_state.training_data, model.labels_),
                        "Davies-Bouldin Index": davies_bouldin_score(st.session_state.training_data, model.labels_),
                        "Calinski-Harabasz Score": calinski_harabasz_score(st.session_state.training_data, model.labels_)
                    }
            
                    # Atualizar r√≥tulos dos clusters no estado da sess√£o
                    st.session_state.retrain_labels = model.labels_
                    st.session_state.retrain_completed = True
            
                    # Exibir mensagem de sucesso com informa√ß√µes relevantes
                    if st.session_state.selected_model_name == "Clustering Hier√°rquico":
                        st.success(f"Re-treino conclu√≠do com sucesso com {best_n_clusters_retrain} clusters e {st.session_state.pca_n_components} componentes PCA!")
                    else:
                        st.success(f"Re-treino conclu√≠do com sucesso com {best_n_clusters_retrain} clusters!")
            
                # Exibir m√©tricas do re-treino ap√≥s a execu√ß√£o
                if st.session_state.get("retrain_completed", False):
                    st.write("### M√©tricas do Re-Treino")
                    st.table(fix_dataframe_types(pd.DataFrame([st.session_state.retrain_metrics])))
            
                    # Recuperar o modelo atualizado do estado da sess√£o
                    current_model = st.session_state.models[st.session_state.selected_model_name]
            
                    # Verificar centroides para KMeans e exibi-los
                    if st.session_state.selected_model_name == "KMeans":
                        if hasattr(current_model, 'cluster_centers_'):
                            st.write("#### Centroides dos Clusters")
                            centroids = current_model.cluster_centers_
                            if centroids.shape[1] > 10:
                                st.write(f"(Mostrando apenas as primeiras 10 dimens√µes de {centroids.shape[1]})")
                                centroids_df = pd.DataFrame(centroids[:, :10])
                            else:
                                centroids_df = pd.DataFrame(centroids)
                            
                            st.dataframe(fix_dataframe_types(centroids_df))
            
                    # Visualiza√ß√£o dos clusters ap√≥s o re-treino
                    if 'retrain_labels' in st.session_state:
                        st.write("### Visualiza√ß√£o dos Clusters do Re-Treino")
            
                        # Preparar dados para visualiza√ß√£o 2D
                        if st.session_state.selected_model_name == "Clustering Hier√°rquico":
                            # Para Clustering Hier√°rquico, utilizar os dados reduzidos pelo PCA
                            plot_data = st.session_state.X_pca
                        else:
                            # Para K-Means, aplicar PCA para reduzir os dados e facilitar a visualiza√ß√£o
                            X_for_viz = X_scaled  # Utilizar os dados originais normalizados
                            if X_for_viz.shape[1] > 3:
                                pca_viz = PCA(n_components=3)
                                plot_data = pca_viz.fit_transform(X_for_viz)
                                st.write("(Dados reduzidos via PCA para visualiza√ß√£o)")
                            else:
                                plot_data = X_for_viz
            
                        # Determinar o n√∫mero total de componentes dispon√≠veis para visualiza√ß√£o
                        total_components = plot_data.shape[1]
            
                        # Permitir ao utilizador escolher os componentes para visualiza√ß√£o
                        st.write("### Escolha os Componentes para Visualiza√ß√£o")
                        col1, col2 = st.columns(2)
            
                        with col1:
                            x_component = st.selectbox(
                                "Componente para o Eixo X", 
                                list(range(total_components)), 
                                index=0,
                                format_func=lambda x: f"Componente {x+1}",
                                key="retrain_x_component"  # Chave √∫nica para evitar conflitos no estado da sess√£o
                            )
            
                        with col2:
                            y_component = st.selectbox(
                                "Componente para o Eixo Y", 
                                list(range(total_components)), 
                                index=1 if total_components > 1 else 0,
                                format_func=lambda x: f"Componente {x+1}",
                                key="retrain_y_component"  # Chave √∫nica para evitar conflitos no estado da sess√£o
                            )
            
                        # Garantir que os componentes escolhidos s√£o diferentes antes da visualiza√ß√£o
                        if x_component == y_component:
                            st.warning("Por favor, selecione componentes diferentes para X e Y.")
                        else:
                            # Criar gr√°fico de dispers√£o dos clusters re-treinados
                            fig, ax = plt.subplots(figsize=(10, 6))
                            scatter = ax.scatter(
                                plot_data[:, x_component], 
                                plot_data[:, y_component], 
                                c=st.session_state.retrain_labels, 
                                cmap='viridis', 
                                alpha=0.7
                            )
                            ax.set_title(f'Visualiza√ß√£o 2D dos Clusters do Re-Treino ({best_n_clusters_retrain} clusters)')
                            ax.set_xlabel(f'Componente {x_component+1}')
                            ax.set_ylabel(f'Componente {y_component+1}')
                            legend = ax.legend(*scatter.legend_elements(), title="Clusters")
                            ax.add_artist(legend)
                            st.pyplot(fig)
                            plt.clf()
            
                # Finalizar o processo ap√≥s o re-treino bem-sucedido
                if st.session_state.get("retrain_completed", False):
                    st.write("## Concluir o Processo de Clustering")
                    if st.button("Seguir para o Relat√≥rio"):
                        st.session_state.step = 'clustering_final_page'
                        st.rerun()

    # 3. Sele√ß√£o da Coluna Alvo
    from sklearn.preprocessing import LabelEncoder
    import pandas as pd
    
    # Inicializar vari√°veis de estado no session_state se n√£o existirem
    if 'bins_confirmed' not in st.session_state:
        st.session_state['bins_confirmed'] = False  # Confirma√ß√£o da escolha dos bins
    if 'bins_value' not in st.session_state:
        st.session_state['bins_value'] = 3  # Definir um valor padr√£o para os bins
    
    # **Filtrar colunas dispon√≠veis para sele√ß√£o da vari√°vel alvo, dependendo do tipo de modelo**
    if st.session_state.model_type == "Classifica√ß√£o":
        # Para modelos de classifica√ß√£o: considerar colunas categ√≥ricas (object) e colunas num√©ricas com poucas categorias
        valid_columns = [col for col in columns if data[col].dtype in ['object', 'int64'] or data[col].nunique() <= 10]
    else:
        # Para modelos de regress√£o: considerar apenas colunas num√©ricas cont√≠nuas (float64 e int64) com muitas categorias
        valid_columns = [col for col in columns if data[col].dtype in ['float64', 'int64'] and data[col].nunique() > 10]
    
    # **Sele√ß√£o da Coluna Alvo**
    # Apenas necess√°rio para modelos de Classifica√ß√£o e Regress√£o (n√£o aplic√°vel a Clustering)
    if st.session_state.model_type != "Clustering" and st.session_state.selected_model_name and not st.session_state.target_column_confirmed:
        st.write("### Escolha a Coluna Alvo")
        
        # Criar um menu suspenso para o utilizador selecionar a coluna alvo
        target_column = st.selectbox(
            "Selecione a coluna alvo",
            options=valid_columns,  # Exibir apenas as colunas v√°lidas
            key='target_column_selectbox'
        )
    
        # **Bot√£o para confirmar a sele√ß√£o da coluna alvo**
        if st.button("Confirmar Coluna Alvo"):
            if target_column in columns:  # Verificar se a coluna selecionada est√° nos dados
                st.session_state.target_column = target_column
                st.session_state.target_column_confirmed = True  # Confirmar a sele√ß√£o
                st.session_state.validation_method = None  # Resetar m√©todo de valida√ß√£o
                st.session_state.validation_confirmed = False  # Resetar confirma√ß√£o de valida√ß√£o
    
                # Armazenar os valores da vari√°vel alvo
                y = data[st.session_state.target_column]
    
                # **Verificar o tipo de modelo**
                model_type = st.session_state.model_type
    
                # **Se o modelo for de Classifica√ß√£o**
                if model_type == "Classifica√ß√£o":
                    # Utilizar LabelEncoder para transformar colunas categ√≥ricas em valores num√©ricos
                    le = LabelEncoder()
                    y_encoded = le.fit_transform(y)
                    st.session_state['target_column_encoded'] = y_encoded
                    st.success("Coluna categ√≥rica detectada e codificada com LabelEncoder.")
    
                # **Se o modelo for de Regress√£o**
                elif model_type == "Regress√£o":
                    if y.dtype in ['float64', 'int64']:  # Verificar se a vari√°vel √© cont√≠nua
                        st.session_state['target_column_encoded'] = y  # Manter os valores originais
                        st.success("Coluna cont√≠nua detectada e pronta para regress√£o.")
                    else:
                        # Se a coluna n√£o for cont√≠nua, exibir um erro e interromper o processo
                        st.error("Modelos de regress√£o requerem uma coluna cont√≠nua como alvo.")
                        st.stop()  # Parar a execu√ß√£o para evitar erros futuros
    
    # **Exibir a Coluna Alvo Confirmada**
    if st.session_state.model_type != "Clustering" and st.session_state.target_column_confirmed:
        st.write(f"### Coluna Alvo Confirmada: {st.session_state.target_column}")
        st.write(f"Tipo: {st.session_state.get('target_column_type', 'N√£o definido')}")  # Mostrar tipo da vari√°vel alvo

        # 4. GridSearch - Ajuste de Hiperpar√¢metros
        # **Fun√ß√£o para limpar par√¢metros inv√°lidos no session_state**
        def limpar_parametros_invalidos():
            """Remove par√¢metros inv√°lidos do session_state."""
            if 'manual_params' in st.session_state:
                if 'gamma' in st.session_state['manual_params']:
                    del st.session_state['manual_params']['gamma']  # Remove 'gamma' se presente
        
        # **Definir modelos que n√£o possuem hiperpar√¢metros ajust√°veis**
        NO_HYPERPARAM_MODELS = ["Regress√£o Linear Simples (RLS)"]
        
        # **Verificar se o modelo foi selecionado e se o GridSearch ainda n√£o foi confirmado**
        if st.session_state.selected_model_name and not st.session_state.grid_search_confirmed:
        
            # **Caso o modelo n√£o tenha hiperpar√¢metros ajust√°veis**
            if st.session_state.selected_model_name in NO_HYPERPARAM_MODELS:
                st.write(f"O modelo {st.session_state.selected_model_name} n√£o possui hiperpar√¢metros ajust√°veis.")
                st.session_state.use_grid_search = "N√£o"
                param_grid = {}  # Nenhum par√¢metro para ajustar
                st.session_state.grid_search_confirmed = True
        
            else:
                # **Perguntar ao utilizador se quer usar GridSearch**
                use_grid_search = st.radio(
                    "Usar GridSearch?", 
                    ["Sim", "N√£o"], 
                    key='grid_search_radio', 
                    index=0 if st.session_state.get('use_grid_search', "Sim") == "Sim" else 1
                )
                st.session_state.use_grid_search = use_grid_search
        
                # **Inicializar param_grid como vazio**
                param_grid = {}  # Evita erros de vari√°vel n√£o definida
        
                if use_grid_search == "Sim":
                    # **Perguntar como os par√¢metros devem ser escolhidos**
                    param_choice = st.radio(
                        "Escolher os par√¢metros de GridSearch?",
                        ["Utilizar os melhores par√¢metros", "Escolher manualmente os par√¢metros de GridSearch"],
                        key='param_choice_radio',
                        index=0 if st.session_state.get('param_choice', "Utilizar os melhores par√¢metros") == "Utilizar os melhores par√¢metros" else 1
                    )
                    st.session_state.param_choice = param_choice
        
                    # **Inicializar par√¢metros manuais**
                    if 'manual_params' not in st.session_state:
                        st.session_state.manual_params = {}
        
                    manual_params = st.session_state.manual_params
        
                    # **Configura√ß√£o manual dos par√¢metros**
                    if param_choice == "Escolher manualmente os par√¢metros de GridSearch":
                        # **Recuperar o modelo selecionado**
                        model_key = st.session_state.selected_model_name
        
                        # **Obter os par√¢metros padr√£o para o modelo selecionado**
                        param_grid = get_default_param_grid(model_key)
        
                        # **Se n√£o houver par√¢metros padr√£o, informar o utilizador**
                        if not param_grid:
                            st.warning(f"Par√¢metros padr√£o n√£o definidos para o modelo {model_key}.")
                            param_grid = {}
        
                        # **Exibir os par√¢metros para o utilizador ajustar manualmente**
                        manual_params = {}
                        for param, values in param_grid.items():
                            # **Tratar par√¢metros espec√≠ficos como 'kernel'**
                            if param == "kernel":
                                manual_params[param] = st.selectbox(
                                    f"Escolha o valor para '{param}':",
                                    values,  # Lista de valores permitidos
                                    index=0,  # Primeiro valor como padr√£o
                                    key=f"{model_key}_{param}"
                                )
        
                            # **Mostrar 'gamma' apenas se o kernel for 'rbf'**
                            elif param == "gamma":
                                if "kernel" in manual_params and manual_params["kernel"] == "rbf":
                                    manual_params[param] = st.selectbox(
                                        f"Escolha o valor para '{param}':",
                                        values,  # Lista de valores permitidos
                                        index=0,  # Primeiro valor como padr√£o
                                        key=f"{model_key}_{param}"
                                    )
                                else:
                                    # **Remover 'gamma' se n√£o for necess√°rio**
                                    manual_params.pop(param, None)
                                    if 'manual_params' in st.session_state and param in st.session_state['manual_params']:
                                        del st.session_state['manual_params'][param]
        
                            # **Tratar par√¢metros num√©ricos (ex.: C, epsilon)**
                            elif isinstance(values[0], (int, float)):
                                st.write(f"Par√¢metro: **{param}** | Intervalo dispon√≠vel: [{min(values)}, {max(values)}]")
        
                                param_type = float if any(isinstance(v, float) for v in values) else int
        
                                manual_params[param] = st.number_input(
                                    f"Escolha o valor para '{param}':",
                                    min_value=float(min(values)) if param_type == float else int(min(values)),
                                    max_value=float(max(values)) if param_type == float else int(max(values)),
                                    value=float(values[0]) if param_type == float else int(values[0]),
                                    step=0.1 if param_type == float else 1,  
                                    key=f"{model_key}_{param}"
                                )
        
                            # **Tratar `max_depth` separadamente como um selectbox**
                            elif param == "max_depth":
                                st.write(f"Par√¢metro: **{param}** | Valores dispon√≠veis: {values}")
                                manual_params[param] = st.selectbox(
                                    f"Escolha o valor para '{param}':",
                                    values,
                                    index=0,  # Primeiro valor como padr√£o
                                    key=f"{model_key}_{param}"
                                )
        
                            # **Tratar par√¢metros categ√≥ricos (ex.: 'weights')**
                            elif isinstance(values[0], str):
                                st.write(f"Par√¢metro: **{param}** | Valores dispon√≠veis: {values}")
                                manual_params[param] = st.selectbox(
                                    f"Escolha o valor para '{param}':",
                                    values,  # Lista de valores permitidos
                                    index=0,  # Primeiro valor como padr√£o
                                    key=f"{model_key}_{param}"
                                )
        
                        # **Salvar os par√¢metros manuais no estado global**
                        st.session_state['manual_params'] = manual_params
                        st.write("Par√¢metros manuais salvos:", manual_params)
        
                # **Bot√£o para confirmar configura√ß√µes do GridSearch**
                if st.button("Confirmar GridSearch"):
                    st.session_state.grid_search_confirmed = True
                    st.success("Configura√ß√£o do GridSearch confirmada!")
        
                    # **Se o utilizador escolheu "Utilizar os melhores par√¢metros", armazenar um dicion√°rio vazio**
                    if st.session_state.use_grid_search == "Sim" and st.session_state.param_choice == "Utilizar os melhores par√¢metros":
                        st.session_state['manual_params'] = {}
                        st.session_state['best_params_str'] = "{}"
                        st.session_state['best_params'] = param_grid
                        st.session_state['best_params_selected'] = param_grid
                                
        # 5. Escolha do M√©todo de Valida√ß√£o
        
        # O m√©todo de valida√ß√£o s√≥ aparece ap√≥s a confirma√ß√£o do GridSearch
        if st.session_state.grid_search_confirmed and st.session_state.selected_model_name and not st.session_state.validation_method:
            
            st.write("### Escolha o M√©todo de Valida√ß√£o")
            
            # Lista dos m√©todos dispon√≠veis
            validation_methods = ["Divis√£o em Treino e Teste", "Holdout"]
        
            # Escolha do m√©todo pelo utilizador
            validation_method = st.radio(
                "Selecione o m√©todo de valida√ß√£o",
                validation_methods,
                key='validation_method_radio'
            )
        
            # Configura√ß√µes espec√≠ficas para cada m√©todo de valida√ß√£o
            if validation_method == "Divis√£o em Treino e Teste":
                # O utilizador escolhe a propor√ß√£o do conjunto de teste
                test_size = st.slider(
                    "Propor√ß√£o do conjunto de teste",
                    min_value=0.1, max_value=0.9, value=0.3, step=0.1
                )
                st.session_state.test_size = test_size
        
            elif validation_method == "Holdout":
                # O utilizador escolhe a propor√ß√£o do conjunto de treino
                train_size = st.slider(
                    "Propor√ß√£o do conjunto de treino",
                    min_value=0.1, max_value=0.9, value=0.7, step=0.1
                )
                st.session_state.train_size = train_size
        
            # **Bot√£o para confirmar a escolha do m√©todo de valida√ß√£o**
            if st.button("Confirmar Valida√ß√£o"):
                # Guardar o m√©todo de valida√ß√£o escolhido
                st.session_state.validation_method = validation_method  
        
                # **Prepara√ß√£o dos dados**
                # Remover a coluna alvo do conjunto de caracter√≠sticas
                X = data.drop(columns=[st.session_state.target_column])
                y = data[st.session_state.target_column]
        
                # **Convers√£o de vari√°veis categ√≥ricas para num√©ricas**
                X = pd.get_dummies(X)  # Cria√ß√£o de vari√°veis dummy para colunas categ√≥ricas
        
                try:
                    # **Divis√£o dos dados com base no m√©todo escolhido**
                    if st.session_state.validation_method == "Divis√£o em Treino e Teste":
                        # Divis√£o cl√°ssica em treino e teste
                        st.session_state.X_train, st.session_state.X_test, st.session_state.y_train, st.session_state.y_test = train_test_split(
                            X, y, test_size=st.session_state.test_size, random_state=42
                        )
                        st.success("Divis√£o dos dados realizada com sucesso!")
        
                    elif st.session_state.validation_method == "Holdout":
                        # Outro m√©todo de divis√£o treino-teste, baseado na propor√ß√£o de treino
                        st.session_state.X_train, st.session_state.X_test, st.session_state.y_train, st.session_state.y_test = train_test_split(
                            X, y, train_size=st.session_state.train_size, random_state=42
                        )
                        st.success("Divis√£o dos dados realizada com sucesso!")
        
                    # **Confirma que a valida√ß√£o foi conclu√≠da**
                    st.session_state.validation_confirmed = True
        
                except Exception as e:
                    st.error(f"Erro na divis√£o dos dados: {e}")
        
                # **Exibir o m√©todo de valida√ß√£o confirmado**
                if st.session_state.validation_confirmed:
                    st.write(f"**M√©todo de Valida√ß√£o Confirmado:** {st.session_state.validation_method}")

        # 6. Treino do Modelo
        
        # **Exibir o bot√£o para treinar o modelo apenas ap√≥s a valida√ß√£o ser confirmada**
        if st.session_state.validation_confirmed:
            if st.button("Treinar o Modelo"):
                st.session_state.validation_confirmed = False  # Resetar a valida√ß√£o ap√≥s o treino
                st.success("Treino iniciado com sucesso!")
        
                # **Recuperar o modelo selecionado**
                model_name = st.session_state.selected_model_name
                model = st.session_state.models.get(st.session_state.selected_model_name)
        
                # **Verificar se o modelo foi encontrado**
                if model is None:
                    st.error(f"Modelo {st.session_state.selected_model_name} n√£o encontrado.")
                    return  # Interrompe a execu√ß√£o caso o modelo n√£o seja encontrado
        
                # **Inicializar 'treinos_realizados' no estado global caso ainda n√£o exista**
                if 'treinos_realizados' not in st.session_state:
                    st.session_state['treinos_realizados'] = []
        
                # **Recolher as informa√ß√µes necess√°rias do estado global**
                target_column = st.session_state.target_column
                validation_method = st.session_state.validation_method
                use_grid_search = st.session_state.use_grid_search
                manual_params = st.session_state.manual_params
                X_train = st.session_state.X_train
                y_train = st.session_state.y_train
                X_test = st.session_state.X_test
                y_test = st.session_state.y_test
        
                # **Remover par√¢metros inv√°lidos antes do treino**
                if 'manual_params' in st.session_state:
                    if manual_params.get('kernel') == 'linear' and 'gamma' in manual_params:
                        del manual_params['gamma']  # Remover par√¢metro inv√°lido localmente
                    if 'gamma' in st.session_state['manual_params']:
                        del st.session_state['manual_params']['gamma']  # Remover do estado global
        
                # **Tratar valores ausentes antes do treino**
                from sklearn.impute import SimpleImputer
                imputer = SimpleImputer(strategy="mean")  # Estrat√©gia de imputa√ß√£o ("mean" pode ser alterado para "median")
                X_train = imputer.fit_transform(X_train)  # Aplicar imputa√ß√£o no conjunto de treino
                X_test = imputer.transform(X_test)        # Aplicar imputa√ß√£o no conjunto de teste
        
                # **Exibir resumo das escolhas feitas pelo utilizador**
                st.write("### Resumo das Escolhas Feitas:")
                st.write(f"**Modelo Selecionado**: {model_name}")
                st.write(f"**Coluna Alvo**: {target_column}")
                st.write(f"**M√©todo de Valida√ß√£o**: {validation_method}")
                st.write(f"**GridSearch Ativado?** {use_grid_search}")  # Informa√ß√£o adicional para depura√ß√£o
        
                # **Iniciar o treino do modelo**
                param_grid = get_default_param_grid(model_name) if use_grid_search == "Sim" else {}
                resultado = train_and_evaluate(
                    model, param_grid, X_train, y_train, X_test, y_test, use_grid_search, manual_params
                )
        
                # **Guardar os melhores par√¢metros no estado global ap√≥s o treino**
                if 'Best Parameters' in resultado:
                    st.session_state['best_params'] = resultado['Best Parameters']
                    st.session_state['best_params_selected'] = resultado['Best Parameters']
                    st.session_state['best_params_str'] = json.dumps(st.session_state['best_params'], indent=2)
                    st.write("Par√¢metros salvos no estado global:", st.session_state['best_params'])
                else:
                    st.warning("Nenhum par√¢metro encontrado para salvar.")
        
                # **Guardar os resultados ap√≥s o primeiro treino**
                if resultado:
                    st.session_state['resultado_sem_selecao'] = resultado  # Salvar resultado sem sele√ß√£o de features
                    st.session_state['treinos_realizados'].append(resultado)
        
                    # **Criar um DataFrame com as m√©tricas do modelo treinado**
                    df_resultado = pd.DataFrame([resultado])
        
                    # **Corrigir os tipos de dados antes de exibir**
                    df_corrigido = fix_dataframe_types(df_resultado)
        
                    # **Exibir m√©tricas do modelo**
                    st.write("### M√©tricas do Modelo Treinado:")
                    formatted_display = df_corrigido.style.format(
                        {col: "{:.4f}" for col in df_corrigido.select_dtypes(include=['float', 'float64']).columns}
                    )
                    st.dataframe(formatted_display)
        
                    # **Gerar gr√°fico com as m√©tricas do modelo**
                    plot_metrics(df_corrigido)
        
                    # **Marcar o treino como conclu√≠do**
                    st.session_state['treino_concluido'] = True
                else:
                    st.error("O treino do modelo falhou.")
        
        # **Avan√ßar para Sele√ß√£o de Features APENAS ap√≥s a exibi√ß√£o das m√©tricas**
        if st.session_state.get('treino_concluido', False):
            st.write("### Avan√ßar para Sele√ß√£o de Features")
        
            # **Verificar se h√° treinos realizados**
            if 'treinos_realizados' in st.session_state and st.session_state['treinos_realizados']:
                
                # **Identificar o melhor modelo com base na m√©trica apropriada**
                if st.session_state.model_type == "Classifica√ß√£o":
                    melhores_metricas = sorted(
                        st.session_state['treinos_realizados'], 
                        key=lambda x: x.get('Accuracy', 0),  # Ordena√ß√£o pela m√©trica Accuracy
                        reverse=True
                    )[0]  # Seleciona o melhor modelo
                elif st.session_state.model_type == "Regress√£o":
                    melhores_metricas = sorted(
                        st.session_state['treinos_realizados'], 
                        key=lambda x: x.get('R¬≤', 0),  # Ordena√ß√£o pela m√©trica R¬≤
                        reverse=True
                    )[0]  # Seleciona o melhor modelo
        
                # **Permitir ao utilizador escolher um modelo manualmente ou manter o melhor**
                model_options = [resultado['Modelo'] for resultado in st.session_state['treinos_realizados']]
                default_index = model_options.index(melhores_metricas['Modelo']) if melhores_metricas['Modelo'] in model_options else 0
        
                selected_model_temp = st.selectbox(
                    "Escolha um modelo para avan√ßar para a Sele√ß√£o de Features:",
                    options=model_options,
                    index=default_index
                )
        
                # **Bot√£o para avan√ßar para a pr√≥xima etapa**
                if st.button("Avan√ßar para Sele√ß√£o de Features"):
                    st.session_state.selected_model_name = selected_model_temp  # Atualiza o modelo selecionado
                    st.session_state.step = 'feature_selection'  # Atualiza a etapa do fluxo
                    st.session_state['treino_concluido'] = False  # Reseta o estado do treino
                    st.rerun()
            else:
                st.error("Nenhum modelo foi treinado. Execute o treino primeiro.")

# Fun√ß√£o para treinar e avaliar os modelos de clustering
def train_clustering_model(model, X_data, model_name):
    """
    Treina um modelo de clustering (KMeans ou Clustering Hier√°rquico) e armazena os r√≥tulos dos clusters.

    Par√¢metros:
    - model: Modelo de clustering selecionado (KMeans ou Clustering Hier√°rquico).
    - X_data: Dados de entrada para treino do modelo.
    - model_name: Nome do modelo a ser treinado.

    """
    try:
        # **Padronizar os dados para melhor desempenho dos modelos**
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X_data)  # Normaliza os dados

        # **Treinar o modelo conforme o tipo de clustering selecionado**
        if model_name == "KMeans":
            model.set_params(n_clusters=st.session_state.kmeans_clusters)  # Definir o n√∫mero de clusters
            model.fit(X_scaled)  # Ajustar o modelo aos dados normalizados
            st.session_state['labels'] = model.labels_  # Armazenar os r√≥tulos dos clusters
        
        elif model_name == "Clustering Hier√°rquico":
            # Configurar todos os par√¢metros necess√°rios para o modelo Hier√°rquico
            model.set_params(n_clusters=st.session_state.kmeans_clusters, linkage="ward")
            model.fit(X_scaled)  # Ajustar o modelo aos dados
            st.session_state['labels'] = model.labels_  # Armazenar os r√≥tulos dos clusters
        
        # **Exibir mensagem de sucesso**
        st.write(f"Clusteriza√ß√£o realizada com sucesso usando o modelo {model_name}!")

    except Exception as e:
        # **Capturar e exibir erros, caso ocorram**
        st.error(f"Erro ao treinar o modelo {model_name}: {str(e)}")


# Fun√ß√£o para visualiza√ß√£o dos clusters usando PCA
def visualize_clusters(X_data):
    """
    Gera uma visualiza√ß√£o dos clusters em 2D usando PCA para reduzir a dimensionalidade dos dados.

    Par√¢metros:
    - X_data: Dados de entrada que ser√£o projetados em 2D para visualiza√ß√£o dos clusters.

    """
    if 'labels' in st.session_state:  # Verifica se os r√≥tulos dos clusters j√° foram gerados
        # **Aplicar PCA para reduzir os dados para 2 dimens√µes**
        pca = PCA(n_components=2)
        X_pca = pca.fit_transform(X_data)

        # **Criar gr√°fico de dispers√£o dos clusters**
        plt.figure(figsize=(8, 6))
        plt.scatter(X_pca[:, 0], X_pca[:, 1], c=st.session_state['labels'], cmap='viridis', alpha=0.7)
        plt.xlabel('Componente Principal 1')
        plt.ylabel('Componente Principal 2')
        plt.title('Visualiza√ß√£o dos Clusters em 2D')

        # **Exibir o gr√°fico no Streamlit**
        st.pyplot(plt.gcf())


from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import GridSearchCV, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVR, SVC
from sklearn.linear_model import LinearRegression

def evaluate_regression_model(y_true, y_pred):
    """
    Avalia um modelo de regress√£o com base em tr√™s m√©tricas principais:
    - R¬≤: Coeficiente de determina√ß√£o (quanto maior, melhor).
    - MAE: Erro absoluto m√©dio (quanto menor, melhor).
    - MSE: Erro quadr√°tico m√©dio (quanto menor, melhor).

    Par√¢metros:
    - y_true: Valores reais da vari√°vel de sa√≠da.
    - y_pred: Valores previstos pelo modelo.

    Retorna:
    - Um dicion√°rio com as m√©tricas calculadas.
    """
    r2 = r2_score(y_true, y_pred)
    mae = mean_absolute_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    return {"R¬≤": r2, "MAE": mae, "MSE": mse}

def train_and_evaluate(model, param_grid, X_train, y_train, X_test, y_test, use_grid_search, manual_params=None):
    """
    Treina e avalia um modelo de Machine Learning utilizando GridSearch para otimiza√ß√£o dos hiperpar√¢metros.

    Par√¢metros:
    - model: O modelo de Machine Learning a ser treinado (ex.: SVR, SVC, LinearRegression).
    - param_grid: Dicion√°rio contendo os par√¢metros para GridSearchCV (se ativado).
    - X_train: Conjunto de treino para as vari√°veis preditoras.
    - y_train: Conjunto de treino para a vari√°vel alvo.
    - X_test: Conjunto de teste para as vari√°veis preditoras.
    - y_test: Conjunto de teste para a vari√°vel alvo.
    - use_grid_search: Booleano que indica se o GridSearchCV deve ser utilizado.
    - manual_params: Par√¢metros fornecidos manualmente pelo utilizador (se houver).

    Retorna:
    - Um dicion√°rio com as m√©tricas de avalia√ß√£o do modelo treinado.
    """
    try:
        # **Verificar o tipo de modelo**
        is_svr = isinstance(model, SVR)  # Identifica se o modelo √© uma regress√£o por vetores de suporte (SVR)
        is_svc = isinstance(model, SVC)  # Identifica se o modelo √© um classificador SVC
        is_regression = is_svr or isinstance(model, LinearRegression)  # Identifica se o modelo √© de regress√£o

        # **Escalonamento dos dados apenas para SVR (necess√°rio para otimizar o desempenho)**
        if is_svr:
            scaler = StandardScaler()
            X_train = scaler.fit_transform(X_train)
            X_test = scaler.transform(X_test)

        # **Configura√ß√£o do GridSearchCV**
        if use_grid_search:
            cv = KFold(n_splits=5, shuffle=True, random_state=42)  # Valida√ß√£o cruzada com 5 divis√µes
            scoring = 'r2' if is_regression else 'accuracy'  # Define a m√©trica de avalia√ß√£o conforme o tipo de problema
            
            # **Otimiza√ß√£o para modelos SVC (Classifica√ß√£o por Vetores de Suporte)**
            if is_svc:
                # Reduz o n√∫mero de par√¢metros testados para acelerar o GridSearch
                simplified_grid = {
                    'C': [1],            # Apenas um valor para C
                    'kernel': ['rbf'],   # Apenas um tipo de kernel
                    'gamma': ['scale']   # Apenas uma configura√ß√£o de gamma
                }
                
                # **Aplicar par√¢metros manuais, se fornecidos pelo utilizador**
                if manual_params:
                    for param, value in manual_params.items():
                        simplified_grid[param] = [value]  # Garante que os valores sejam listas para GridSearch
                
                actual_grid = simplified_grid  # Usa o grid simplificado para SVC
                cv = KFold(n_splits=3, shuffle=True, random_state=42)  # Reduz o n√∫mero de folds para otimizar tempo
                
            else:
                actual_grid = param_grid  # Para outros modelos, usa o grid normal
                
                # **Se o utilizador forneceu par√¢metros manuais, incorpor√°-los ao grid**
                if manual_params:
                    actual_grid.update({k: [v] for k, v in manual_params.items()})

            # **Executar o GridSearch para encontrar os melhores hiperpar√¢metros**
            grid_search = GridSearchCV(
                model, 
                actual_grid,
                cv=cv,
                scoring=scoring,
                n_jobs=-1  # Usa todos os n√∫cleos dispon√≠veis para acelerar a busca
            )
            grid_search.fit(X_train, y_train)
            
            best_model = grid_search.best_estimator_  # Melhor modelo encontrado pelo GridSearch
            best_params = grid_search.best_params_  # Melhores hiperpar√¢metros identificados

        else:
            # **Se n√£o usar GridSearch, aplicar os par√¢metros manualmente (se fornecidos)**
            if manual_params:
                model.set_params(**manual_params)

            model.fit(X_train, y_train)  # Treinar o modelo com os dados de treino
            best_model = model  # O modelo treinado sem otimiza√ß√£o
            best_params = manual_params or {}  # Se n√£o houver par√¢metros manuais, define um dicion√°rio vazio

        # **Fazer previs√µes com o modelo treinado**
        y_pred = best_model.predict(X_test)

        # **Calcular m√©tricas de desempenho**
        metrics = {
            "Modelo": model.__class__.__name__,
            **(
                # **Se for um modelo de regress√£o**
                {
                    "R¬≤": r2_score(y_test, y_pred),
                    "MAE": mean_absolute_error(y_test, y_pred),
                    "MSE": mean_squared_error(y_test, y_pred)
                } if is_regression else 
                # **Se for um modelo de classifica√ß√£o**
                {
                    "Accuracy": accuracy_score(y_test, y_pred),
                    "Precision": precision_score(y_test, y_pred, average='weighted'),
                    "Recall": recall_score(y_test, y_pred, average='weighted'),
                    "F1-Score": f1_score(y_test, y_pred, average='weighted')
                }
            ),
            "Best Parameters": best_params
        }

        return metrics  # Retorna as m√©tricas do modelo treinado

    except Exception as e:
        # **Capturar erros e exibir no Streamlit**
        st.error(f"Erro ao treinar o modelo: {str(e)}")
        return None

# **Fun√ß√£o para selecionar o m√©todo de avalia√ß√£o (Scoring)**
def select_scoring():
    """
    Permite ao utilizador selecionar a m√©trica de avalia√ß√£o a ser usada na sele√ß√£o de features.
    A escolha √© armazenada no session_state para ser utilizada posteriormente.

    - Se o utilizador j√° tiver feito uma escolha anteriormente, ela ser√° mantida.
    - Se for a primeira vez, a m√©trica padr√£o ser√° "F1-Score".
    - A escolha pode ser guardada num ficheiro para persist√™ncia.

    Retorna:
    - Nenhum valor expl√≠cito, mas a m√©trica escolhida √© armazenada no session_state.
    """
    # Verifica se a m√©trica j√° foi selecionada; se n√£o, define "F1-Score" como padr√£o
    if 'selected_scoring' not in st.session_state:
        st.session_state.selected_scoring = 'F1-Score'

    # Criar a caixa de sele√ß√£o para escolha da m√©trica
    st.session_state.selected_scoring = st.selectbox(
        "Escolha o scoring para a sele√ß√£o de features:",
        ['Accuracy', 'Precision', 'Recall', 'F1-Score'],
        index=['Accuracy', 'Precision', 'Recall', 'F1-Score'].index(st.session_state.selected_scoring)
    )

    # Exibir a escolha feita
    st.write("Scoring selecionado:", st.session_state.selected_scoring)

    # Op√ß√£o para guardar a escolha num ficheiro
    if st.button("Salvar escolha"):
        with open("scoring_choice.txt", "w") as file:
            file.write(st.session_state.selected_scoring)  # Grava a escolha no ficheiro
        st.success("Escolha salva com sucesso!")


# **Fun√ß√£o para remover features altamente correlacionadas**
def remove_highly_correlated_features(df, threshold=0.9):
    """
    Remove colunas do DataFrame que apresentam uma correla√ß√£o superior a um determinado limiar.

    Par√¢metros:
    - df (DataFrame): Conjunto de dados de entrada.
    - threshold (float): Limiar de correla√ß√£o acima do qual as colunas ser√£o removidas (padr√£o: 0.9).

    Retorna:
    - DataFrame sem as colunas altamente correlacionadas.
    """
    # **1. Calcular a matriz de correla√ß√£o absoluta**
    corr_matrix = df.corr().abs()  # Calcula os coeficientes de correla√ß√£o absolutos

    # **2. Criar uma matriz triangular superior**
    # Esta matriz exclui a diagonal principal e os valores abaixo dela, para evitar redund√¢ncias
    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))

    # **3. Identificar as colunas a serem removidas**
    # Se qualquer valor na matriz for superior ao threshold, removemos a coluna correspondente
    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]

    # **4. Informar ao utilizador quais colunas foram removidas**
    if to_drop:
        st.info(f"Features removidas por alta correla√ß√£o: {to_drop}")

    # **5. Retornar o DataFrame sem as colunas altamente correlacionadas**
    return df.drop(columns=to_drop)

# **Fun√ß√£o para selecionar features importantes com RandomForest**
def select_important_features(X, y, threshold=0.01, model_type=None):
    """
    Seleciona as features mais relevantes utilizando RandomForest.

    Par√¢metros:
    - X: Matriz de features (DataFrame).
    - y: Vetor alvo (s√©rie de labels ou valores num√©ricos).
    - threshold: Limiar m√≠nimo de import√¢ncia (padr√£o = 0.01).
    - model_type: Tipo de modelo ("Classifica√ß√£o" ou "Regress√£o").

    Retorna:
    - DataFrame contendo apenas as features selecionadas.
    """

    # **1. Definir o modelo conforme o tipo de problema**
    if model_type == "Classifica√ß√£o":
        model = RandomForestClassifier(n_estimators=100, random_state=42)
    elif model_type == "Regress√£o":
        model = RandomForestRegressor(n_estimators=100, random_state=42)
    else:
        raise ValueError("O tipo de modelo deve ser 'Classifica√ß√£o' ou 'Regress√£o'.")

    # **2. Tratar valores ausentes utilizando SimpleImputer**
    imputer = SimpleImputer(strategy='mean')  # Substitui valores ausentes pela m√©dia
    X_imputed = imputer.fit_transform(X)

    # **3. Treinar o modelo RandomForest**
    model.fit(X_imputed, y)

    # **4. Obter a import√¢ncia de cada feature**
    importances = model.feature_importances_

    # **5. Criar um DataFrame com as import√¢ncias ordenadas**
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': importances
    }).sort_values('importance', ascending=False)

    # **6. Selecionar apenas as features que ultrapassam o threshold**
    important_features = feature_importance[feature_importance['importance'] > threshold]['feature']

    # **7. Exibir as features selecionadas ao utilizador**
    st.info(f"Features selecionadas: {list(important_features)}")

    # **8. Retornar o DataFrame contendo apenas as features selecionadas**
    return X[important_features]


# **Fun√ß√£o principal para sele√ß√£o de features**
def feature_selection():
    """
    Interface para a sele√ß√£o de features em modelos de Machine Learning.
    
    - Permite ao utilizador escolher a m√©trica de scoring.
    - D√° a op√ß√£o de selecionar as features automaticamente ou manualmente.
    - Mostra um DataFrame com as import√¢ncias das features.
    """

    st.header("Sele√ß√£o de Features")

    # Inicializar o estado de sele√ß√£o de features
    if 'feature_selection_done' not in st.session_state:
        st.session_state.feature_selection_done = False

    # Obter o tipo de modelo armazenado na sess√£o (Classifica√ß√£o ou Regress√£o)
    model_type = st.session_state.get('model_type', 'Classifica√ß√£o')

    # Definir op√ß√µes de scoring dispon√≠veis conforme o tipo de modelo
    scoring_options = {
        "Classifica√ß√£o": ['Accuracy', 'Precision', 'Recall', 'F1-Score'],
        "Regress√£o": ['R¬≤', 'MAE', 'MSE']
    }

    # **1. Escolha da m√©trica de avalia√ß√£o**
    selected_scoring = st.selectbox(
        "Escolha a m√©trica de scoring:",
        scoring_options.get(model_type, [])  # Exibe op√ß√µes conforme o tipo de modelo
    )

    # **Confirmar a escolha da m√©trica**
    if st.button("Confirmar Scoring"):
        st.session_state.selected_scoring = selected_scoring
        st.session_state.scoring_confirmed = True
        st.success(f"M√©trica de scoring {selected_scoring} confirmada!")

    # **2. Escolha do m√©todo de sele√ß√£o de features**
    if st.session_state.scoring_confirmed:
        method_selection = st.radio(
            "Escolha o m√©todo de sele√ß√£o de features:",
            ["Autom√°tico", "Manual"]
        )

        # **Confirmar m√©todo escolhido**
        if st.button("Confirmar M√©todo"):
            st.session_state.method_selection = method_selection
            st.success(f"M√©todo {method_selection} confirmado!")

        # Obter os dados de treino e teste da sess√£o
        X_train, X_test, y_train, y_test = (
            st.session_state.X_train, 
            st.session_state.X_test, 
            st.session_state.y_train, 
            st.session_state.y_test
        )

        # **3. Sele√ß√£o Autom√°tica de Features**
        if method_selection == "Autom√°tico":
            feature_selector = (
                RandomForestClassifier(n_estimators=100, random_state=42)
                if model_type == "Classifica√ß√£o"
                else RandomForestRegressor(n_estimators=100, random_state=42)
            )

            # Treinar o modelo para obter import√¢ncias
            feature_selector.fit(X_train, y_train)

            # Criar DataFrame com as import√¢ncias ordenadas
            feature_importances = pd.DataFrame({
                'feature': X_train.columns,
                'importance': feature_selector.feature_importances_
            }).sort_values('importance', ascending=False)

            # Exibir o DataFrame com as import√¢ncias das features
            st.dataframe(feature_importances)

            # **Selecionar as features mais importantes com threshold > 0.01**
            selected_features = feature_importances[feature_importances['importance'] > 0.01]['feature'].tolist()

        # **4. Sele√ß√£o Manual de Features**
        else:
            feature_selector = (
                RandomForestClassifier(n_estimators=100, random_state=42)
                if model_type == "Classifica√ß√£o"
                else RandomForestRegressor(n_estimators=100, random_state=42)
            )

            # Treinar o modelo para obter import√¢ncias
            feature_selector.fit(X_train, y_train)

            # Criar DataFrame com as import√¢ncias ordenadas
            feature_importances = pd.DataFrame({
                'feature': X_train.columns,
                'importance': feature_selector.feature_importances_
            }).sort_values('importance', ascending=False)

            # Exibir o DataFrame com as import√¢ncias das features
            st.dataframe(feature_importances)

            # **Permitir ao utilizador escolher quantas features deseja manter**
            num_features = st.slider(
                "N√∫mero de Features a Selecionar",
                1, X_train.shape[1], min(5, X_train.shape[1])
            )

            # Selecionar as top-N features com base na escolha do utilizador
            selected_features = feature_importances['feature'].head(num_features).tolist()

        # **5. Atualizar o estado global com as features selecionadas**
        st.session_state.X_train_selected = X_train[selected_features]
        st.session_state.X_test_selected = X_test[selected_features]
        st.session_state.selected_features = selected_features
        st.session_state.feature_selection_done = True

        # **6. Bot√£o para treinar o modelo com as features selecionadas**
        if st.button("Treinar Modelo com Features Selecionadas"):
            st.session_state.step = 'train_with_selected_features'
            st.rerun()

# **Fun√ß√£o para treinar o modelo com as features selecionadas**
def train_with_selected_features_page():
    st.title("Treino do Modelo com Features Selecionadas")
    
    # **Mapeamento de modelos para evitar inconsist√™ncias nos nomes**
    model_name_map = {
        "SVC": "Support Vector Classification (SVC)",
        "KNeighborsClassifier": "K-Nearest Neighbors (KNN)",
        "RandomForestClassifier": "Random Forest",
        "LinearRegression": "Regress√£o Linear Simples (RLS)",
        "SVR": "Regress√£o por Vetores de Suporte (SVR)",
        "Support Vector Classification (SVC)": "SVC",
        "K-Nearest Neighbors (KNN)": "KNeighborsClassifier", 
        "Random Forest": "RandomForestClassifier",
        "Regress√£o Linear Simples (RLS)": "LinearRegression",
        "Regress√£o por Vetores de Suporte (SVR)": "SVR"
    }
    
    # **Verificar se h√° modelos dispon√≠veis**
    if 'models' not in st.session_state or not st.session_state.models:
        st.error("Erro: Nenhum modelo foi treinado ou selecionado.")
        return

    if 'selected_model_name' not in st.session_state or not st.session_state.selected_model_name:
        st.error("Nenhum modelo foi selecionado. Por favor, selecione um modelo antes de continuar.")
        return

    # **Obter o nome do modelo selecionado e verificar a sua exist√™ncia**
    selected_model_name = st.session_state.selected_model_name.strip()
    model_class_name = model_name_map.get(selected_model_name, selected_model_name)

    if model_class_name not in st.session_state.models:
        st.error(f"O modelo '{selected_model_name}' n√£o foi encontrado na sess√£o.")
        st.write("Modelos dispon√≠veis:", list(st.session_state.models.keys()))
        return

    # **Recuperar o modelo**
    model = st.session_state.models[model_class_name]
    
    # **Recuperar os dados selecionados**
    X_train_selected, X_test_selected = st.session_state.X_train_selected, st.session_state.X_test_selected
    y_train, y_test = st.session_state.y_train, st.session_state.y_test
    
    st.write(f"Treinando o modelo {selected_model_name} com {len(st.session_state.selected_features)} features selecionadas...")
    
    # **Treinar e armazenar m√©tricas**
    selected_metrics = train_and_store_metrics(
        model, X_train_selected, y_train, X_test_selected, y_test, "Com Sele√ß√£o", False
    )
    
    # **Exibir m√©tricas se o treino for bem-sucedido**
    if selected_metrics:
        st.session_state['resultado_com_selecao'] = selected_metrics
        st.success("Treinamento conclu√≠do!")
        
        st.subheader("M√©tricas do Modelo com Features Selecionadas")
        metrics_df = pd.DataFrame([selected_metrics])
        metrics_df.insert(0, "Modelo", "Com Sele√ß√£o de Features")
        st.table(metrics_df)
    
    # **Bot√£o para comparar modelos**
    if st.button("Comparar Modelos"):
        st.session_state.step = 'evaluate_and_compare_models'
        st.rerun()


# **Fun√ß√£o para treinar o modelo e armazenar m√©tricas**
def train_and_store_metrics(model, X_train, y_train, X_test, y_test, metric_type, use_grid_search=False, manual_params=None):
    """
    Treina o modelo e armazena as m√©tricas de desempenho.
    
    Par√¢metros:
    - model: Modelo a ser treinado.
    - X_train, y_train: Dados de treino.
    - X_test, y_test: Dados de teste.
    - metric_type: Tipo de treino ("Com Sele√ß√£o" ou "Sem Sele√ß√£o").
    - use_grid_search: Se True, aplica GridSearchCV.
    - manual_params: Par√¢metros manuais a serem aplicados.
    
    Retorna:
    - Dicion√°rio com m√©tricas do modelo.
    """
    try:
        # **1. Tratar valores ausentes**
        imputer = SimpleImputer(strategy="mean")  # Preenche valores ausentes com a m√©dia
        X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)
        X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)

        # **2. Converter vari√°veis categ√≥ricas**
        if y_train.dtype == 'object':
            from sklearn.preprocessing import LabelEncoder
            le = LabelEncoder()
            y_train = le.fit_transform(y_train)
            y_test = le.transform(y_test)
        else:
            y_train = y_train.fillna(y_train.mean())
            y_test = y_test.fillna(y_test.mean())

        # **3. Aplicar par√¢metros salvos ao modelo, se existirem**
        if metric_type == "Com Sele√ß√£o":
            saved_params = st.session_state.get('best_params_selected', None) or st.session_state.get('best_params', None)
        else:
            saved_params = st.session_state.get('best_params', None)

        if saved_params and hasattr(model, 'get_params') and all(param in model.get_params() for param in saved_params):
            st.info(f"Aplicando par√¢metros salvos ao modelo: {saved_params}")
            model.set_params(**saved_params)

        # **4. Treinar o modelo com ou sem GridSearch**
        if use_grid_search and metric_type == "Sem Sele√ß√£o":
            param_grid = st.session_state.get('param_grid', {
                'n_neighbors': [3, 5, 7, 9],
                'weights': ['uniform', 'distance']
            })

            cv_strategy = KFold(n_splits=5, shuffle=True, random_state=42)
            scoring = 'accuracy' if st.session_state.model_type == "Classifica√ß√£o" else 'r2'

            grid_search = GridSearchCV(model, param_grid, scoring=scoring, cv=cv_strategy, n_jobs=-1)
            grid_search.fit(X_train, y_train)

            best_model = grid_search.best_estimator_
            best_params = grid_search.best_params_

            # **Salvar os melhores par√¢metros no estado global**
            st.session_state['best_params'] = best_params
            st.session_state['best_params_selected'] = best_params

        else:
            model.fit(X_train, y_train)
            best_model = model
            best_params = saved_params if saved_params else {}

        # **5. Armazenar o modelo treinado na sess√£o**
        st.session_state['trained_model'] = best_model
        st.session_state['trained_model_name'] = best_model.__class__.__name__
        
        # **6. Fazer previs√µes**
        y_pred = best_model.predict(X_test)

        # **7. Calcular as m√©tricas de desempenho**
        if st.session_state.model_type == "Classifica√ß√£o":
            metrics = {
                'F1-Score': f1_score(y_test, y_pred, average='weighted'),
                'Precision': precision_score(y_test, y_pred, average='weighted'),
                'Recall': recall_score(y_test, y_pred, average='weighted'),
                'Accuracy': accuracy_score(y_test, y_pred),
                'Best Parameters': best_params
            }
        else:
            metrics = {
                'R¬≤': r2_score(y_test, y_pred),
                'MSE': mean_squared_error(y_test, y_pred),
                'MAE': mean_absolute_error(y_test, y_pred),
                'Best Parameters': best_params
            }

        # **8. Armazenar m√©tricas no estado global**
        if 'metrics' not in st.session_state:
            st.session_state['metrics'] = {}
        st.session_state['metrics'][metric_type] = metrics

        return metrics

    except Exception as e:
        st.error(f"Erro ao treinar o modelo: {str(e)}")
        return None
        
def evaluate_and_compare_models():
    st.title("Compara√ß√£o dos Resultados do Treino dos Modelos")

    # **Mapeamento de modelos para garantir compatibilidade de nomenclatura**
    model_name_map = {
        "SVC": "Support Vector Classification (SVC)",
        "KNeighborsClassifier": "K-Nearest Neighbors (KNN)",
        "RandomForestClassifier": "Random Forest",
        "LinearRegression": "Regress√£o Linear Simples (RLS)",
        "SVR": "Regress√£o por Vetores de Suporte (SVR)",
        "Support Vector Classification (SVC)": "SVC",
        "K-Nearest Neighbors (KNN)": "KNeighborsClassifier", 
        "Random Forest": "RandomForestClassifier",
        "Regress√£o Linear Simples (RLS)": "LinearRegression",
        "Regress√£o por Vetores de Suporte (SVR)": "SVR"
    }

    # **Verifica√ß√µes preliminares para garantir que todas as etapas anteriores foram conclu√≠das**
    if 'selected_features' not in st.session_state:
        st.error("Nenhuma feature foi selecionada. Por favor, volte √† etapa de sele√ß√£o de features.")
        return

    if 'models' not in st.session_state or not st.session_state.models:
        st.error("Configura√ß√£o de modelos n√£o encontrada. Por favor, reinicie o processo de sele√ß√£o de modelos.")
        return

    # **Obter tipo de modelo e m√©trica escolhida**
    model_type = st.session_state.get('model_type', 'Indefinido')
    scoring_metric = st.session_state.get("selected_scoring", None)
    
    if not scoring_metric:
        st.error("Nenhuma m√©trica de avalia√ß√£o foi escolhida. Por favor, volte √† etapa de sele√ß√£o de m√©tricas.")
        return

    # **Recuperar o nome do modelo selecionado**
    model_name = st.session_state.get('selected_model_name')
    if not model_name:
        st.error("Nenhum modelo foi selecionado. Por favor, volte √† etapa de sele√ß√£o de modelos.")
        return

    # **Verificar se o modelo est√° no mapeamento**
    model_class_name = model_name_map.get(model_name)
    if model_class_name is None:
        st.error(f"O modelo {model_name} n√£o foi encontrado na lista de modelos dispon√≠veis.")
        st.write("Modelos dispon√≠veis:", list(model_name_map.keys()))
        return

    # **Recuperar o modelo treinado**
    model = st.session_state.models.get(model_class_name)
    if model is None:
        st.error(f"O modelo {model_class_name} n√£o foi encontrado na sess√£o.")
        st.write("Modelos dispon√≠veis:", list(st.session_state.models.keys()))
        return

    # **Obter as m√©tricas dos modelos treinados**
    original_metrics = st.session_state.get('resultado_sem_selecao', {}) 
    selected_metrics = st.session_state.get('resultado_com_selecao', {})

    if not original_metrics:
        st.error("N√£o foi poss√≠vel encontrar as m√©tricas originais. Por favor, refa√ßa o treinamento.")
        return
        
    if not selected_metrics:
        st.error("N√£o foi poss√≠vel encontrar as m√©tricas com sele√ß√£o de features. Por favor, execute o treino com features selecionadas.")
        return

    # **Criar DataFrame de compara√ß√£o**
    if model_type == "Classifica√ß√£o":
        comparison_df = pd.DataFrame({
            'Modelo': ['Sem Sele√ß√£o de Features', 'Com Sele√ß√£o de Features'],
            'Accuracy': [original_metrics.get('Accuracy', 0), selected_metrics.get('Accuracy', 0)],
            'Precision': [original_metrics.get('Precision', 0), selected_metrics.get('Precision', 0)],
            'Recall': [original_metrics.get('Recall', 0), selected_metrics.get('Recall', 0)],
            'F1-Score': [original_metrics.get('F1-Score', 0), selected_metrics.get('F1-Score', 0)],
            'Best Parameters': [original_metrics.get('Best Parameters', 'N/A'), selected_metrics.get('Best Parameters', 'N/A')]
        })
    elif model_type == "Regress√£o":
        comparison_df = pd.DataFrame({
            'Modelo': ['Sem Sele√ß√£o de Features', 'Com Sele√ß√£o de Features'],
            'R¬≤': [original_metrics.get('R¬≤', 0), selected_metrics.get('R¬≤', 0)],
            'MAE': [original_metrics.get('MAE', 0), selected_metrics.get('MAE', 0)],
            'MSE': [original_metrics.get('MSE', 0), selected_metrics.get('MSE', 0)],
            'Best Parameters': [original_metrics.get('Best Parameters', 'N/A'), selected_metrics.get('Best Parameters', 'N/A')]
        })
    else:
        st.error(f"Tipo de modelo n√£o reconhecido: {model_type}")
        return

    # **Exibir tabela de compara√ß√£o**
    st.subheader("üìà Compara√ß√£o dos Resultados:")
    
    format_dict = {}
    for col in comparison_df.columns:
        if col != 'Modelo' and col != 'Best Parameters':
            format_dict[col] = '{:.4f}'
    
    st.dataframe(
        comparison_df.style.format(format_dict).set_table_styles(
            [{'selector': 'th', 'props': [('font-size', '18px')]}, 
             {'selector': 'td', 'props': [('font-size', '12px')]},  
             {'selector': 'table', 'props': [('width', '100%')]},    
            ]
        )
    )
    
    # **Criar gr√°fico de compara√ß√£o com base na m√©trica selecionada**
    if scoring_metric in comparison_df.columns:
        fig, ax = plt.subplots(figsize=(10, 6))

        x_pos = [0, 1]
        width = 0.4

        bars1 = ax.bar(x_pos[0], comparison_df[scoring_metric].iloc[0], width=width, label="Sem Sele√ß√£o de Features", color='#90EE90', align='center')
        bars2 = ax.bar(x_pos[1], comparison_df[scoring_metric].iloc[1], width=width, label="Com Sele√ß√£o de Features", color='#006400', align='center')

        for bar in bars1:
            ax.annotate(f'{bar.get_height():.4f}', xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()), 
                        xytext=(0, 3), textcoords="offset points", ha='center', fontsize=12, color='black')

        for bar in bars2:
            ax.annotate(f'{bar.get_height():.4f}', xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()), 
                        xytext=(0, 3), textcoords="offset points", ha='center', fontsize=12, color='black')

        ax.set_title(f'Compara√ß√£o de {scoring_metric}', fontsize=16, fontweight='bold')
        ax.set_ylabel(scoring_metric, fontsize=14)
        ax.set_xlabel("Modelos", fontsize=14)

        plt.xticks(x_pos, ['Sem Sele√ß√£o de Features', 'Com Sele√ß√£o de Features'], fontsize=12)
        plt.yticks(fontsize=12)

        ax.legend()
        plt.tight_layout()

        st.pyplot(fig)

    # **Determinar o melhor modelo com base na m√©trica escolhida**
    score_without = comparison_df[scoring_metric].iloc[0]
    score_with = comparison_df[scoring_metric].iloc[1]

    better_model = "Com Sele√ß√£o de Features" if score_with > score_without else "Sem Sele√ß√£o de Features"
    better_score = max(score_with, score_without)

    st.success(f"üèÜ **Melhor modelo:** {better_model} com {scoring_metric} = {better_score:.4f}")
    
    # **Bot√£o para avan√ßar para a pr√≥xima etapa**
    if st.button("Seguir para Resumo Final", key="btn_resumo_final"):
        st.session_state.step = 'final_page'
        st.rerun()

# Fun√ß√£o para gerar interpreta√ß√£o personalizada das m√©tricas de classifica√ß√£o
def generate_metrics_interpretation(metrics):
    """Gera interpreta√ß√£o personalizada para m√©tricas de modelos de classifica√ß√£o."""
    interpretacao = []

    # **Verificar se as m√©tricas est√£o no formato esperado**
    if not isinstance(metrics, dict):
        return "Formato de m√©tricas inv√°lido."

    # **Interpreta√ß√£o para Acur√°cia (Accuracy)**
    if 'Accuracy' in metrics:
        try:
            accuracy = float(metrics['Accuracy'])
            if accuracy > 0.9:
                interpretacao.append(f"- Acur√°cia: {accuracy:.4f} - Excelente! O modelo tem uma taxa de acerto muito elevada.")
            elif accuracy > 0.75:
                interpretacao.append(f"- Acur√°cia: {accuracy:.4f} - Boa. O modelo est√° a funcionar bem, mas pode ser otimizado.")
            elif accuracy > 0.5:
                interpretacao.append(f"- Acur√°cia: {accuracy:.4f} - Moderada. O modelo apresenta um n√≠vel de erro consider√°vel.")
            else:
                interpretacao.append(f"- Acur√°cia: {accuracy:.4f} - Fraca. O modelo falha em muitas previs√µes e precisa de ajustes.")
        except (ValueError, TypeError):
            interpretacao.append("- Acur√°cia: N√£o dispon√≠vel ou inv√°lida.")

    # **Interpreta√ß√£o para Precis√£o (Precision)**
    if 'Precision' in metrics:
        try:
            precision = float(metrics['Precision'])
            if precision > 0.9:
                interpretacao.append(f"- Precis√£o: {precision:.4f} - Excelente! O modelo evita falsos positivos com alta confian√ßa.")
            elif precision > 0.75:
                interpretacao.append(f"- Precis√£o: {precision:.4f} - Boa. O modelo √© confi√°vel, mas pode ser mais rigoroso na sele√ß√£o.")
            elif precision > 0.5:
                interpretacao.append(f"- Precis√£o: {precision:.4f} - Moderada. O modelo ainda produz muitos falsos positivos.")
            else:
                interpretacao.append(f"- Precis√£o: {precision:.4f} - Fraca. Muitos falsos positivos comprometem a confiabilidade.")
        except (ValueError, TypeError):
            interpretacao.append("- Precis√£o: N√£o dispon√≠vel ou inv√°lida.")

    # **Interpreta√ß√£o para Recall (Sensibilidade)**
    if 'Recall' in metrics:
        try:
            recall = float(metrics['Recall'])
            if recall > 0.9:
                interpretacao.append(f"- Recall: {recall:.4f} - Excelente! O modelo deteta quase todos os casos positivos.")
            elif recall > 0.75:
                interpretacao.append(f"- Recall: {recall:.4f} - Bom. A maioria dos positivos s√£o identificados.")
            elif recall > 0.5:
                interpretacao.append(f"- Recall: {recall:.4f} - Moderado. O modelo est√° a perder muitos casos positivos.")
            else:
                interpretacao.append(f"- Recall: {recall:.4f} - Fraco. O modelo falha em detetar muitos casos positivos.")
        except (ValueError, TypeError):
            interpretacao.append("- Recall: N√£o dispon√≠vel ou inv√°lido.")

    # **Interpreta√ß√£o para F1-Score**
    if 'F1-Score' in metrics:
        try:
            f1_score = float(metrics['F1-Score'])
            if f1_score > 0.9:
                interpretacao.append(f"- F1-Score: {f1_score:.4f} - Excelente equil√≠brio entre precis√£o e recall.")
            elif f1_score > 0.75:
                interpretacao.append(f"- F1-Score: {f1_score:.4f} - Bom, mas pode ser melhorado.")
            elif f1_score > 0.5:
                interpretacao.append(f"- F1-Score: {f1_score:.4f} - Moderado. Ajustes podem melhorar o desempenho.")
            else:
                interpretacao.append(f"- F1-Score: {f1_score:.4f} - Fraco. Ajustes profundos s√£o necess√°rios.")
        except (ValueError, TypeError):
            interpretacao.append("- F1-Score: N√£o dispon√≠vel ou inv√°lido.")

    # **Conclus√£o Geral**
    if all(key in metrics for key in ['F1-Score', 'Precision', 'Recall']):
        try:
            f1_score = float(metrics['F1-Score'])
            precision = float(metrics['Precision'])
            recall = float(metrics['Recall'])

            if f1_score > 0.9 and precision > 0.9 and recall > 0.9:
                interpretacao.append("\nConclus√£o: üéâ O modelo tem um desempenho excecional!")
            elif f1_score > 0.75 and precision > 0.75 and recall > 0.75:
                interpretacao.append("\nConclus√£o: üëç O modelo tem um bom desempenho geral.")
            elif f1_score > 0.5 or precision > 0.5 or recall > 0.5:
                interpretacao.append("\nConclus√£o: ‚ö†Ô∏è O modelo √© funcional, mas pode ser melhorado.")
            else:
                interpretacao.append("\nConclus√£o: ‚ùó O modelo apresenta desempenho insatisfat√≥rio.")
        except (ValueError, TypeError):
            pass

    return "\n".join(interpretacao)


# Fun√ß√£o para gerar interpreta√ß√£o personalizada das m√©tricas de regress√£o
def generate_regression_interpretation(metrics):
    """Gera interpreta√ß√£o personalizada para m√©tricas de regress√£o."""
    interpretation = []

    # **Verificar se as m√©tricas est√£o no formato esperado**
    if not isinstance(metrics, dict):
        return "Formato de m√©tricas inv√°lido."

    # **Interpreta√ß√£o para R¬≤ (Coeficiente de Determina√ß√£o)**
    if 'R¬≤' in metrics:
        try:
            r2 = float(metrics['R¬≤'])
            if r2 > 0.9:
                interpretation.append(f"- R¬≤: {r2:.4f} - Excelente! O modelo explica quase toda a variabilidade dos dados.")
            elif r2 > 0.75:
                interpretation.append(f"- R¬≤: {r2:.4f} - Muito bom! O modelo tem um √≥timo ajuste.")
            elif r2 > 0.5:
                interpretation.append(f"- R¬≤: {r2:.4f} - Moderado. O modelo precisa de ajustes para melhor explica√ß√£o dos dados.")
            else:
                interpretation.append(f"- R¬≤: {r2:.4f} - Fraco. O modelo tem um ajuste insatisfat√≥rio.")
        except (ValueError, TypeError):
            interpretation.append("- R¬≤: N√£o dispon√≠vel ou inv√°lido.")

    # **Interpreta√ß√£o para MAE (Erro Absoluto M√©dio)**
    if 'MAE' in metrics:
        try:
            mae = float(metrics['MAE'])
            if mae < 0.1:
                interpretation.append(f"- MAE: {mae:.4f} - Excelente! As previs√µes est√£o muito pr√≥ximas dos valores reais.")
            elif mae < 1:
                interpretation.append(f"- MAE: {mae:.4f} - Bom. O erro √© aceit√°vel, mas pode ser reduzido.")
            else:
                interpretation.append(f"- MAE: {mae:.4f} - Alto. O modelo apresenta desvios significativos.")
        except (ValueError, TypeError):
            interpretation.append("- MAE: N√£o dispon√≠vel ou inv√°lido.")

    # **Interpreta√ß√£o para MSE (Erro Quadr√°tico M√©dio)**
    if 'MSE' in metrics:
        try:
            mse = float(metrics['MSE'])
            if mse < 0.1:
                interpretation.append(f"- MSE: {mse:.4f} - Excelente! As previs√µes t√™m erros m√≠nimos.")
            elif mse < 1:
                interpretation.append(f"- MSE: {mse:.4f} - Bom. O erro est√° sob controlo, mas pode ser otimizado.")
            else:
                interpretation.append(f"- MSE: {mse:.4f} - Alto. As previs√µes est√£o significativamente afastadas dos valores reais.")
        except (ValueError, TypeError):
            interpretation.append("- MSE: N√£o dispon√≠vel ou inv√°lido.")

    # **Conclus√£o Geral**
    if all(key in metrics for key in ['R¬≤', 'MAE', 'MSE']):
        try:
            r2 = float(metrics['R¬≤'])
            mse = float(metrics['MSE'])
            mae = float(metrics['MAE'])

            if r2 > 0.9 and mse < 0.1 and mae < 0.1:
                interpretation.append("\nConclus√£o: üéâ O modelo apresenta um desempenho excecional!")
            elif r2 > 0.75 and mse < 1 and mae < 1:
                interpretation.append("\nConclus√£o: üëç O modelo tem um bom desempenho geral.")
            elif r2 > 0.5 or mse < 1 or mae < 1:
                interpretation.append("\nConclus√£o: ‚ö†Ô∏è O modelo precisa de melhorias.")
            else:
                interpretation.append("\nConclus√£o: ‚ùó O modelo apresenta um desempenho insatisfat√≥rio.")
        except (ValueError, TypeError):
            pass

    return "\n".join(interpretation)


import joblib

# Fun√ß√£o para salvar o modelo treinado com um nome din√¢mico
def save_best_model(model, with_feature_selection=True):
    """
    Salva o modelo treinado em um ficheiro .pkl, permitindo a recupera√ß√£o posterior.

    Par√¢metros:
    - model: Modelo treinado a ser salvo.
    - with_feature_selection (bool): Se True, indica que o modelo foi treinado com sele√ß√£o de features.

    Retorna:
    - str: Nome do ficheiro onde o modelo foi salvo, ou None em caso de erro.
    """
    try:
        # Determinar o nome do ficheiro dependendo se houve sele√ß√£o de features
        if with_feature_selection:
            model_filename = "best_model_com_selecao_features.pkl"
        else:
            model_filename = "best_model_sem_selecao_features.pkl"

        # Salvar o modelo utilizando joblib
        joblib.dump(model, model_filename)
        
        # Mensagem de sucesso
        st.success(f"Modelo salvo com sucesso como {model_filename}")
        
        return model_filename
    except Exception as e:
        # Exibir erro caso ocorra alguma falha no processo de salvamento
        st.error(f"Erro ao salvar o modelo: {str(e)}")
        return None


# Fun√ß√£o para executar o treino e avan√ßar para a etapa final
def execute_training():
    """
    Executa o treino do modelo armazenado no session_state e avan√ßa para a p√°gina final.

    Esta fun√ß√£o:
    - Recupera o modelo selecionado pelo utilizador.
    - Treina o modelo e armazena as m√©tricas resultantes.
    - Exibe informa√ß√µes de depura√ß√£o.
    - Redireciona para a p√°gina final ap√≥s o treino.
    """
    if st.session_state.step == 'train_and_store_metrics':
        # Recuperar o modelo selecionado
        model = st.session_state.models[st.session_state.selected_model_name]

        # Treinar o modelo e armazenar as m√©tricas
        metrics = train_and_store_metrics(
            model,
            st.session_state.X_train,
            st.session_state.y_train,
            st.session_state.X_test,
            st.session_state.y_test,
            metric_type="sem_selecao_features"
        )

        # **Depura√ß√£o**: Exibir as m√©tricas armazenadas no session_state ap√≥s o treino
        st.write("Conte√∫do de metrics ap√≥s treino:", st.session_state.get('metrics', {}))

        # Avan√ßar para a p√°gina final ap√≥s o treino ser conclu√≠do
        st.session_state.step = 'final_page'
        st.rerun()


## Relat√≥rio Final para Classifica√ß√£o/Regressao ##

# Fun√ß√£o para gerar o relat√≥rio em PDF
from fpdf import FPDF
import requests
import tempfile
from datetime import datetime
from io import BytesIO
import matplotlib.pyplot as plt
from reportlab.lib import colors
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.platypus import Paragraph

# Classe personalizada para a gera√ß√£o de PDFs
class CustomPDF(FPDF):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        
        # Inicializar a vari√°vel do caminho do log√≥tipo
        self.logo_path = None
        
        # URL do log√≥tipo institucional
        logo_url = 'https://www.ipleiria.pt/normasgraficas/wp-content/uploads/sites/80/2017/09/estg_v-01.jpg'
        
        try:
            # Tentar fazer o download do log√≥tipo
            response = requests.get(logo_url)
            if response.status_code == 200:
                with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as tmpfile:
                    tmpfile.write(response.content)
                    self.logo_path = tmpfile.name
        except Exception as e:
            print(f"Erro ao baixar o log√≥tipo: {e}")

    def header(self):
        """Cabe√ßalho do relat√≥rio"""
        
        # Posicionar o cabe√ßalho no topo da p√°gina
        self.set_y(10)
        
        # Inserir o log√≥tipo se foi baixado com sucesso
        if self.logo_path:
            self.image(self.logo_path, 10, 10, 25)
        
        # Definir a fonte e tamanho do t√≠tulo
        self.set_font('Arial', 'B', 12)
        
        # Adicionar o t√≠tulo centralizado
        self.cell(25)  # Criar espa√ßo para o log√≥tipo
        self.cell(0, 10, 'MLCase - Plataforma de Machine Learning', 0, 0, 'C')
        
        # Criar uma linha horizontal para separar o cabe√ßalho do conte√∫do
        self.ln(15)
        self.ln(5)  # Criar espa√ßo ap√≥s o cabe√ßalho

    def footer(self):
        """Rodap√© do relat√≥rio"""
        
        # Posicionar o rodap√© a 1.5 cm da parte inferior
        self.set_y(-20)
        
        # Adicionar uma linha horizontal acima do rodap√©
        self.line(10, self.get_y(), 200, self.get_y())
        self.ln(3)
        
        # Definir a fonte do rodap√©
        self.set_font('Arial', 'I', 8)
        
        # Obter a data atual
        current_date = datetime.now().strftime('%d/%m/%Y')
        
        # Adicionar a data e n√∫mero da p√°gina
        self.cell(0, 10, f'{current_date} - P√°gina {self.page_no()}  |  Autora da Plataforma: Bruna Sousa', 0, 0, 'C')


# Classe respons√°vel pela gera√ß√£o do relat√≥rio da performance do modelo
class MLCaseModelReportGenerator:
    def __init__(self, output_path='model_performance_report.pdf', logo_url=None):
        """
        Inicializa o gerador de relat√≥rios de performance do modelo.

        Par√¢metros:
        - output_path (str): Caminho para salvar o PDF.
        - logo_url (str, opcional): URL do log√≥tipo da institui√ß√£o.
        """
        self.output_path = output_path
        
        # Definir a URL padr√£o do log√≥tipo se n√£o for especificada
        self.logo_url = logo_url or 'https://www.ipleiria.pt/normasgraficas/wp-content/uploads/sites/80/2017/09/estg_v-01.jpg'
        
        # Fazer o download do log√≥tipo
        self.logo_path = self._fetch_logo()
        
        # Preparar estilos personalizados
        self.styles = getSampleStyleSheet()
        self._create_custom_styles()
    
    def _fetch_logo(self):
        """Faz o download do log√≥tipo e armazena temporariamente."""
        try:
            response = requests.get(self.logo_url)
            if response.status_code == 200:
                with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as tmpfile:
                    tmpfile.write(response.content)
                    return tmpfile.name
            return None
        except Exception:
            return None
    
    def _create_custom_styles(self):
        """Define estilos personalizados para os textos do relat√≥rio."""
        
        # Estilo do t√≠tulo
        self.styles.add(ParagraphStyle(
            name='MLCaseTitle',
            parent=self.styles['Title'],
            fontSize=18,
            textColor=colors.HexColor('#2C3E50'),
            alignment=1,  # Centralizado
            spaceAfter=12
        ))
        
        # Estilo do subt√≠tulo
        self.styles.add(ParagraphStyle(
            name='MLCaseSubtitle',
            parent=self.styles['Heading2'],
            fontSize=14,
            textColor=colors.HexColor('#34495E'),
            spaceAfter=6
        ))
        
        # Estilo do texto normal
        self.styles.add(ParagraphStyle(
            name='MLCaseNormal',
            parent=self.styles['Normal'],
            fontSize=10,
            textColor=colors.HexColor('#2C3E50'),
            leading=14  # Espa√ßamento entre linhas
        ))
    
    def create_bar_chart(self, data, labels, title):
        """
        Gera um gr√°fico de barras para exibi√ß√£o no relat√≥rio.

        Par√¢metros:
        - data (list): Valores das m√©tricas.
        - labels (list): Nome das m√©tricas.
        - title (str): T√≠tulo do gr√°fico.

        Retorna:
        - Objeto de buffer com o gr√°fico gerado.
        """
        
        # Criar o gr√°fico de barras com tamanho definido
        plt.figure(figsize=(6, 4), dpi=100)
        
        # Criar barras com cores diferenciadas
        plt.bar(labels, data, color=['#3498DB', '#2980B9'])
        
        # Definir t√≠tulo e r√≥tulos do gr√°fico
        plt.title(title, fontsize=12, color='#2C3E50')
        plt.ylabel('Valor', color='#2C3E50')
        
        # Rotacionar os r√≥tulos do eixo X para melhor visualiza√ß√£o
        plt.xticks(rotation=45, ha='right', color='#2C3E50')
        
        # Ajustar automaticamente o layout para evitar sobreposi√ß√£o
        plt.tight_layout()
        
        # Criar um buffer de mem√≥ria para armazenar a imagem
        buf = BytesIO()
        plt.savefig(buf, format='png', bbox_inches='tight')
        buf.seek(0)
        
        # Fechar a figura para evitar consumo de mem√≥ria
        plt.close()
        
        return buf
    
def gerar_relatorio_pdf(comparison_df, best_model, session_state):
    """
    Gera um relat√≥rio PDF com os resultados da compara√ß√£o de modelos.

    Args:
        comparison_df: DataFrame contendo as m√©tricas comparativas dos modelos.
        best_model: Nome do melhor modelo identificado.
        session_state: Estado da sess√£o do Streamlit com informa√ß√µes do treino.

    Returns:
        BytesIO: Buffer contendo o PDF gerado.
    """

    # Inicializa√ß√£o do PDF com cabe√ßalho e rodap√© personalizados
    pdf = CustomPDF(format='A4')
    pdf.set_margins(10, 30, 10)  # Margens: esquerda, topo, direita
    pdf.set_auto_page_break(auto=True, margin=30)  # Margem inferior para o rodap√©
    pdf.add_page()
    
    # Fun√ß√£o auxiliar para limpar texto e evitar erros de codifica√ß√£o Latin-1
    def clean_text(text):
        if not isinstance(text, str):
            return str(text)
        return text.encode('latin-1', errors='ignore').decode('latin-1')
    

    # T√≠tulo do Relat√≥rio
    pdf.set_font("Arial", style="B", size=16)
    pdf.cell(0, 10, txt=clean_text("Relat√≥rio Final do Modelo Treinado"), ln=True, align="C")
    pdf.ln(10)
    
    # Tipo de Modelo Utilizado
    model_type = session_state.get('model_type', 'Indefinido')
    pdf.set_font("Arial", style="B", size=12)
    pdf.cell(60, 10, txt=clean_text("Tipo de Modelo:"), ln=False)
    pdf.set_font("Arial", size=12)
    pdf.cell(0, 10, txt=clean_text(model_type), ln=True)
    
    # Modelo Selecionado pelo Utilizador
    selected_model_name = session_state.get('selected_model_name', 'N√£o Selecionado')
    pdf.set_font("Arial", style="B", size=12)
    pdf.cell(60, 10, txt=clean_text("Modelo Selecionado:"), ln=False)
    pdf.set_font("Arial", size=12)
    pdf.cell(0, 10, txt=clean_text(selected_model_name), ln=True)
    
    # Melhor Modelo Identificado com Base nas M√©tricas
    pdf.set_font("Arial", style="B", size=12)
    pdf.cell(60, 10, txt=clean_text("Melhor Modelo:"), ln=False)
    pdf.set_font("Arial", size=12)
    pdf.cell(0, 10, txt=clean_text(best_model), ln=True)
    pdf.ln(10)
    
    # Informa√ß√µes sobre os Conjuntos de Dados Utilizados no Treino
    if 'X_train' in session_state and 'X_test' in session_state:
        X_train = session_state.X_train
        X_test = session_state.X_test
        
        # Calcular percentagem de amostras de treino e teste
        total_samples = X_train.shape[0] + X_test.shape[0]
        train_percent = (X_train.shape[0] / total_samples) * 100
        test_percent = (X_test.shape[0] / total_samples) * 100
        
        pdf.set_font("Arial", style="B", size=14)
        pdf.cell(0, 10, txt=clean_text("Informa√ß√µes dos Conjuntos de Dados"), ln=True)
        pdf.ln(5)
        
        # Criar tabela com informa√ß√µes do conjunto de dados
        data_info = [
            ["Amostras de Treino", f"{X_train.shape[0]} ({train_percent:.1f}%)"],
            ["Amostras de Teste", f"{X_test.shape[0]} ({test_percent:.1f}%)"],
            ["Features Originais", f"{X_train.shape[1]}"]
        ]
        
        # Adicionar n√∫mero de features ap√≥s a sele√ß√£o, se dispon√≠vel
        if 'X_train_selected' in session_state:
            data_info.append(["Features Ap√≥s Sele√ß√£o", f"{session_state.X_train_selected.shape[1]}"])
        
        # Formatar e adicionar a tabela ao PDF
        pdf.set_font("Arial", size=10)
        pdf.set_fill_color(144, 238, 144)  # Cor de fundo do cabe√ßalho
        
        for i, (label, value) in enumerate(data_info):
            if i % 2 == 0:  # Linhas alternadas para melhor leitura
                pdf.set_fill_color(240, 240, 240)
            else:
                pdf.set_fill_color(255, 255, 255)
            
            pdf.cell(70, 8, txt=clean_text(label), border=1, ln=0, fill=True)
            pdf.cell(0, 8, txt=clean_text(value), border=1, ln=1, fill=True)
        
        pdf.ln(10)
    
    # Features Selecionadas no Processo de Sele√ß√£o de Features
    if 'selected_features' in session_state:
        pdf.set_font("Arial", style="B", size=14)
        pdf.cell(0, 10, txt=clean_text("Features Selecionadas"), ln=True)
        
        # Listar todas as features utilizadas ap√≥s a sele√ß√£o
        features = session_state.selected_features
        pdf.set_font("Arial", size=10)
        for i, feature in enumerate(features):
            pdf.cell(0, 6, txt=clean_text(f"‚Ä¢ {feature}"), ln=True)
        
        pdf.ln(10)
    
    # Compara√ß√£o de M√©tricas entre Modelos
    pdf.set_font("Arial", style="B", size=14)
    pdf.cell(0, 10, txt=clean_text("Compara√ß√£o de M√©tricas"), ln=True)
    
    # Determinar o tipo de modelo (Regress√£o ou Classifica√ß√£o) para escolher as m√©tricas adequadas
    is_regression = model_type == "Regress√£o"
    metric_columns = ['R¬≤', 'MAE', 'MSE'] if is_regression else ['Accuracy', 'Precision', 'Recall', 'F1-Score']
    
    # Criar tabela de m√©tricas no relat√≥rio
    pdf.set_font("Arial", style="B", size=10)
    pdf.set_fill_color(144, 238, 144)  # Definir cor do cabe√ßalho
    
    # Definir largura das colunas
    column_width = 30
    first_column_width = 60
    
    # Criar cabe√ßalho da tabela
    pdf.cell(first_column_width, 10, "Modelo", 1, 0, 'C', True)
    for col in metric_columns:
        pdf.cell(column_width, 10, clean_text(col), 1, 0, 'C', True)
    pdf.ln()
    
    # Preencher as linhas da tabela com os valores das m√©tricas
    pdf.set_font("Arial", size=10)
    for _, row in comparison_df.iterrows():
        model_name = row['Modelo']
        pdf.cell(first_column_width, 10, clean_text(model_name), 1, 0, 'L')
        
        for col in metric_columns:
            if col in row:
                # Formatar valores num√©ricos para 4 casas decimais
                if isinstance(row[col], (int, float)):
                    value = f"{row[col]:.4f}"
                else:
                    value = str(row[col])
                pdf.cell(column_width, 10, clean_text(value), 1, 0, 'C')
        
        pdf.ln()
    
    pdf.ln(10)

    # Gr√°ficos de M√©tricas
    for metric in metric_columns:
        if metric in comparison_df.columns:
            # Criar o gr√°fico com tamanho adequado
            plt.figure(figsize=(10, 6))
            
            # Obter os modelos e os valores da m√©trica atual
            models = comparison_df['Modelo'].tolist()
            values = comparison_df[metric].tolist()
            
            # Criar gr√°fico de barras com cores diferenciadas para melhor visualiza√ß√£o
            plt.bar(models, values, color=['#90EE90', '#006400'], width=0.4)
            
            # Adicionar valores sobre as barras para melhor compreens√£o
            for i, v in enumerate(values):
                if isinstance(v, (int, float)):  # Garantir que o valor √© num√©rico
                    plt.text(i, v + 0.01, f"{v:.4f}", ha='center', fontsize=10)
            
            # Configura√ß√£o do eixo X sem rota√ß√£o para manter alinhamento claro
            plt.xticks(rotation=0, ha='center', fontsize=8)  # Antes era rotation=45, alterado para 0
            
            # Estiliza√ß√£o do gr√°fico
            plt.title(f"Compara√ß√£o de {metric}", fontsize=14, pad=15)  # Aumentar o espa√ßo acima do t√≠tulo
            plt.ylabel(metric, fontsize=12)
            
            # Ajustar espa√ßo do gr√°fico para garantir melhor apresenta√ß√£o
            plt.subplots_adjust(bottom=0.2, left=0.15)  # Aumentar margem inferior e lateral esquerda
            
            # Ajustar a altura do gr√°fico para evitar cortes no eixo Y
            plt.ylim(0, max(values) * 1.2)  # Aumenta o limite superior em 20% para evitar sobrecarga visual
            
            plt.tight_layout()  # Ajustar automaticamente o layout para evitar sobreposi√ß√µes
            
            # Guardar o gr√°fico num ficheiro tempor√°rio com DPI superior para melhor qualidade
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.png')
            plt.savefig(temp_file.name, bbox_inches='tight', dpi=150)  # DPI aumentado para evitar pixeliza√ß√£o
            plt.close()
        
            # Adicionar o gr√°fico ao PDF
            pdf.add_page()
            pdf.set_font("Arial", style="B", size=14)
            pdf.cell(0, 10, txt=clean_text(f"Gr√°fico de Compara√ß√£o - {metric}"), ln=True, align="C")
            
            # Posicionar o gr√°fico mais abaixo para evitar sobreposi√ß√£o com o cabe√ßalho
            pdf.image(temp_file.name, x=10, y=45, w=180)  # Posi√ß√£o Y ajustada para evitar cortes
            
            # Fechar e eliminar o ficheiro tempor√°rio ap√≥s utiliza√ß√£o
            temp_file.close()
            try:
                os.remove(temp_file.name)  # Remover o ficheiro tempor√°rio para evitar acumula√ß√£o de arquivos
            except:
                pass  # Se houver erro ao eliminar, ignorar e seguir em frente
    
    # Adicionar uma nova p√°gina ao PDF para interpreta√ß√£o das m√©tricas
    pdf.add_page()
    pdf.set_font("Arial", style="B", size=14)
    pdf.cell(0, 10, txt=clean_text("Interpreta√ß√£o das M√©tricas"), ln=True, align="C")

    # Fun√ß√£o para gerar interpreta√ß√£o de m√©tricas
    def generate_metrics_interpretation(metrics, model_type):
        """
        Gera uma interpreta√ß√£o personalizada das m√©tricas do modelo.
        
        Args:
            metrics (dict): Dicion√°rio contendo as m√©tricas do modelo.
            model_type (str): Tipo do modelo ('Classifica√ß√£o' ou 'Regress√£o').
        
        Returns:
            list: Lista de strings com a interpreta√ß√£o das m√©tricas.
        """
        interpretacao = []
        
        # Caso o modelo seja de Classifica√ß√£o
        if model_type == "Classifica√ß√£o":
            # Interpretar a Acur√°cia (Accuracy)
            accuracy = float(metrics.get('Accuracy', 0))
            if accuracy > 0.9:
                interpretacao.append(f"Acur√°cia: {accuracy:.4f} - Excelente! O modelo tem uma taxa de acerto muito elevada.")
            elif accuracy > 0.75:
                interpretacao.append(f"Acur√°cia: {accuracy:.4f} - Boa, mas ainda h√° margem para otimiza√ß√£o.")
            elif accuracy > 0.5:
                interpretacao.append(f"Acur√°cia: {accuracy:.4f} - Moderada. O modelo apresenta erros significativos.")
            else:
                interpretacao.append(f"Acur√°cia: {accuracy:.4f} - Fraca. O modelo precisa ser revisto e melhorado.")
            
            # Interpretar a Precis√£o (Precision)
            precision = float(metrics.get('Precision', 0))
            if precision > 0.9:
                interpretacao.append(f"Precis√£o: {precision:.4f} - Excelente! Poucos falsos positivos.")
            elif precision > 0.75:
                interpretacao.append(f"Precis√£o: {precision:.4f} - Bom, mas ainda pode melhorar.")
            elif precision > 0.5:
                interpretacao.append(f"Precis√£o: {precision:.4f} - Moderada. O modelo tem um n√∫mero significativo de falsos positivos.")
            else:
                interpretacao.append(f"Precis√£o: {precision:.4f} - Fraca. Muitos falsos positivos prejudicam o desempenho.")
    
            # Interpretar o Recall (Sensibilidade)
            recall = float(metrics.get('Recall', 0))
            if recall > 0.9:
                interpretacao.append(f"Recall: {recall:.4f} - Excelente! A maioria dos positivos verdadeiros s√£o identificados.")
            elif recall > 0.75:
                interpretacao.append(f"Recall: {recall:.4f} - Bom. O modelo capta a maioria dos casos positivos.")
            elif recall > 0.5:
                interpretacao.append(f"Recall: {recall:.4f} - Moderado. Alguns positivos verdadeiros n√£o est√£o a ser reconhecidos.")
            else:
                interpretacao.append(f"Recall: {recall:.4f} - Fraco. O modelo perde muitos casos positivos.")
    
            # Interpretar o F1-Score
            f1_score = float(metrics.get('F1-Score', 0))
            if f1_score > 0.9:
                interpretacao.append(f"F1-Score: {f1_score:.4f} - Excelente equil√≠brio entre precis√£o e recall.")
            elif f1_score > 0.75:
                interpretacao.append(f"F1-Score: {f1_score:.4f} - Bom, mas ainda h√° margem para melhorias.")
            elif f1_score > 0.5:
                interpretacao.append(f"F1-Score: {f1_score:.4f} - Moderado.")
            else:
                interpretacao.append(f"F1-Score: {f1_score:.4f} - Fraco.")
    
        # Caso o modelo seja de Regress√£o
        elif model_type == "Regress√£o":
            # Interpretar o Coeficiente de Determina√ß√£o R¬≤
            r2 = float(metrics.get('R¬≤', 0))
            if r2 > 0.9:
                interpretacao.append(f"R¬≤: {r2:.4f} - Excelente! O modelo explica quase toda a variabilidade dos dados.")
            elif r2 > 0.75:
                interpretacao.append(f"R¬≤: {r2:.4f} - Muito bom! Explica a maioria da variabilidade dos dados.")
            elif r2 > 0.5:
                interpretacao.append(f"R¬≤: {r2:.4f} - Moderado. Ainda h√° limita√ß√µes no ajuste do modelo.")
            else:
                interpretacao.append(f"R¬≤: {r2:.4f} - Fraco. O modelo n√£o est√° a explicar bem a variabilidade dos dados.")
    
            # Interpretar o Erro Absoluto M√©dio (MAE)
            mae = float(metrics.get('MAE', 0))
            if mae < 0.1:
                interpretacao.append(f"MAE: {mae:.4f} - Excelente! O erro m√©dio √© muito pequeno.")
            elif mae < 1:
                interpretacao.append(f"MAE: {mae:.4f} - Bom. O erro m√©dio √© aceit√°vel.")
            else:
                interpretacao.append(f"MAE: {mae:.4f} - Alto. As previs√µes desviam-se significativamente dos valores reais.")
    
            # Interpretar o Erro Quadr√°tico M√©dio (MSE)
            mse = float(metrics.get('MSE', 0))
            if mse < 0.1:
                interpretacao.append(f"MSE: {mse:.4f} - Excelente! O erro quadr√°tico m√©dio √© muito baixo.")
            elif mse < 1:
                interpretacao.append(f"MSE: {mse:.4f} - Bom. O erro √© relativamente baixo.")
            else:
                interpretacao.append(f"MSE: {mse:.4f} - Alto. O modelo tem um erro significativo.")
    
        return interpretacao
    
    # Gerar interpreta√ß√µes para os modelos com e sem sele√ß√£o de features
    pdf.set_font("Arial", style="B", size=12)
    pdf.cell(0, 10, txt=clean_text("Modelo Sem Sele√ß√£o de Features"), ln=True)
    pdf.set_font("Arial", size=10)
    
    # Adicionar interpreta√ß√£o do modelo sem sele√ß√£o de features
    for line in generate_metrics_interpretation(original_metrics, model_type):
        pdf.multi_cell(0, 8, txt=clean_text(f"‚Ä¢ {line}"))
    
    pdf.ln(5)
    pdf.set_font("Arial", style="B", size=12)
    pdf.cell(0, 10, txt=clean_text("Modelo Com Sele√ß√£o de Features"), ln=True)
    pdf.set_font("Arial", size=10)
    
    # Adicionar interpreta√ß√£o do modelo com sele√ß√£o de features
    for line in generate_metrics_interpretation(selected_metrics, model_type):
        pdf.multi_cell(0, 8, txt=clean_text(f"‚Ä¢ {line}"))
    
    # Conclus√£o
    pdf.ln(10)
    pdf.set_font("Arial", style="B", size=14)
    pdf.cell(0, 10, txt=clean_text("Conclus√£o"), ln=True)
    
    # Escolher a m√©trica principal para avalia√ß√£o do modelo
    scoring_metric = session_state.get("selected_scoring", None)
    if not scoring_metric or scoring_metric not in metric_columns:
        main_metric = 'R¬≤' if model_type == "Regress√£o" else 'F1-Score'
    else:
        main_metric = scoring_metric
    
    # Obter valores da m√©trica principal
    original_value = original_metrics.get(main_metric, 0)
    selected_value = selected_metrics.get(main_metric, 0)
    
    # Conclus√£o baseada no desempenho
    pdf.set_font("Arial", size=10)
    conclusion_text = f"Com base na m√©trica principal ({main_metric}), o modelo {best_model} apresentou o melhor desempenho."
    pdf.multi_cell(0, 8, txt=clean_text(conclusion_text))
    
    if original_value > selected_value:
        recommendation_text = "Recomenda-se utilizar o modelo sem sele√ß√£o de features, pois apresentou melhor desempenho geral."
    else:
        feature_reduction = session_state.X_train.shape[1] - session_state.X_train_selected.shape[1]
        recommendation_text = f"Recomenda-se utilizar o modelo com sele√ß√£o de features, pois al√©m de melhorar o desempenho, reduziu a dimensionalidade em {feature_reduction} features."
    
    pdf.multi_cell(0, 8, txt=clean_text(recommendation_text))
    
    # Guardar o PDF
    pdf_buffer = BytesIO()
    pdf_output = pdf.output(dest='S').encode('latin1', errors='ignore')
    pdf_buffer.write(pdf_output)
    pdf_buffer.seek(0)
    return pdf_buffer


# Fun√ß√£o para exibir a p√°gina final com o relat√≥rio
# Mapeamento de nomes de m√©tricas para as colunas do DataFrame
METRIC_MAPPING = {
    "accuracy": "Accuracy",
    "precision": "Precision", 
    "recall": "Recall",
    "f1-score": "F1-Score",
    "r2": "R¬≤",
    "R¬≤": "R¬≤",  # Adicionar mapeamento direto para R¬≤
    "r-squared": "R¬≤",
    "coefficient_of_determination": "R¬≤",
    "mean_squared_error": "MSE",
    "mse": "MSE",  # Adicionar vers√£o min√∫scula de MSE
    "mean_absolute_error": "MAE",
    "mae": "MAE"  # Adicionar vers√£o min√∫scula de MAE
}

def get_metric_mapping(metric):
    """
    Fun√ß√£o para obter o nome da m√©trica de forma mais flex√≠vel
    
    Args:
        metric (str): Nome da m√©trica a ser mapeada
    
    Returns:
        str: Nome da m√©trica mapeado ou None se n√£o encontrado
    """
    # Garantir que seja uma string
    if not isinstance(metric, str):
        st.write(f"Metric n√£o √© uma string: {metric}, tipo: {type(metric)}")
        return None
    
    # Converter para min√∫sculas, remover espa√ßos, acentos
    import unidecode # Normaliza caracteres acentuados, √∫til para lidar com strings em diferentes idiomas.
    metric_clean = unidecode.unidecode(metric.lower().replace(' ', '').replace('-', '').replace('_', ''))
    
    # Verificar se a m√©trica j√° est√° diretamente no formato esperado
    if metric in METRIC_MAPPING.values():
        return metric
    
    # Dicion√°rio expandido de mapeamentos
    extended_mapping = {
        **METRIC_MAPPING,
        "r2score": "R¬≤",
        "rsquared": "R¬≤",
        "determinacao": "R¬≤",
        "coeficienteajuste": "R¬≤",
        "mae": "MAE",
        "erro_absoluto_medio": "MAE",
        "mean_absolute_error": "MAE",
        "erro_absoluto": "MAE",
        "mse": "MSE",
        "erro_quadratico_medio": "MSE",
        "mean_squared_error": "MSE",
        "erro_quadratico": "MSE"
    }
    
    # Tentar mapear
    mapped_metric = extended_mapping.get(metric_clean)
    
    # Se n√£o encontrou, verificar diretamente nas chaves do METRIC_MAPPING
    if mapped_metric is None and metric in METRIC_MAPPING:
        mapped_metric = METRIC_MAPPING[metric]
        
    # Adicionar debug
    st.write(f"M√©trica original: {metric}, limpa: {metric_clean}, mapeada: {mapped_metric}")
    
    return mapped_metric
    
def final_page():
    st.title("Resumo Final dos Modelos Treinados")

    # **CONFIGURA√á√ïES UTILIZADAS**
    st.subheader("Configura√ß√µes Utilizadas")

    # Tipo de Modelo
    model_type = st.session_state.get('model_type', 'Indefinido')
    st.write(f"**Tipo de Modelo:** {model_type}")

    # Modelo Selecionado
    selected_model_name = st.session_state.get('selected_model_name', 'N√£o Selecionado')
    st.write(f"**Modelo Selecionado:** {selected_model_name}")

    # Recupera m√©tricas salvas (sem re-treinar)
    original_metrics = st.session_state.get('resultado_sem_selecao', {})
    selected_metrics = st.session_state.get('resultado_com_selecao', {})

    # Exibir estat√≠sticas sobre os conjuntos de dados
    if 'X_train' in st.session_state and 'X_train_selected' in st.session_state:
        X_train_original = st.session_state.X_train
        X_train_selected = st.session_state.X_train_selected
        
        # Calcular percentuais
        total_samples = X_train_original.shape[0] + st.session_state.X_test.shape[0]
        train_percent = (X_train_original.shape[0] / total_samples) * 100
        test_percent = (st.session_state.X_test.shape[0] / total_samples) * 100
        
        st.subheader("üìä Informa√ß√µes dos Conjuntos de Dados")
        st.write(f"‚Ä¢ Amostras de Treino: {X_train_original.shape[0]} ({train_percent:.1f}% do total)")
        st.write(f"‚Ä¢ Amostras de Teste: {st.session_state.X_test.shape[0]} ({test_percent:.1f}% do total)")
        st.write(f"‚Ä¢ Features Originais: {st.session_state.X_train_original.shape[1] if 'X_train_original' in st.session_state else X_train_original.shape[1]}")
        st.write(f"‚Ä¢ Features Ap√≥s Sele√ß√£o: {X_train_selected.shape[1]}")

    # Recuperar features selecionadas
    if 'selected_features' in st.session_state:
        st.subheader("‚úÖ Features Selecionadas")
        st.write(st.session_state.selected_features)

    # Recupera a m√©trica escolhida para sele√ß√£o de features
    scoring_metric = st.session_state.get("selected_scoring", None)

    # Validar se a m√©trica foi definida
    if not scoring_metric:
        st.error("Nenhuma m√©trica foi selecionada. Volte para a etapa de Sele√ß√£o de Features.")
        return

    # Obter o nome capitalizado da m√©trica com base no mapeamento
    scoring_metric_capitalized = get_metric_mapping(scoring_metric)
    if not scoring_metric_capitalized:
        st.error(f"A m√©trica '{scoring_metric}' n√£o √© v√°lida ou n√£o est√° dispon√≠vel.")
        return

    # **COMPARA√á√ÉO DE M√âTRICAS**
    st.subheader("Compara√ß√£o de M√©tricas")

    # Formatar valores com 4 casas decimais
    def format_metric(value):
        try:
            return float(f"{float(value):.4f}")
        except (ValueError, TypeError):
            return None

    # Criar tabela de m√©tricas
    if model_type == "Classifica√ß√£o":
        comparison_df = pd.DataFrame({
            'Modelo': ['Sem Sele√ß√£o de Features', 'Com Sele√ß√£o de Features'],
            'Accuracy': [
                format_metric(original_metrics.get('Accuracy', 'N/A')),
                format_metric(selected_metrics.get('Accuracy', 'N/A'))
            ],
            'Precision': [
                format_metric(original_metrics.get('Precision', 'N/A')),
                format_metric(selected_metrics.get('Precision', 'N/A'))
            ],
            'Recall': [
                format_metric(original_metrics.get('Recall', 'N/A')),
                format_metric(selected_metrics.get('Recall', 'N/A'))
            ],
            'F1-Score': [
                format_metric(original_metrics.get('F1-Score', 'N/A')),
                format_metric(selected_metrics.get('F1-Score', 'N/A'))
            ],
            'Best Parameters': [
                st.session_state.get('best_params', 'N/A'),
                st.session_state.get('best_params_selected', 'N/A')
            ]
        })
    elif model_type == "Regress√£o":
        comparison_df = pd.DataFrame({
            'Modelo': ['Sem Sele√ß√£o de Features', 'Com Sele√ß√£o de Features'],
            'R¬≤': [
                format_metric(original_metrics.get('R¬≤', 'N/A')),
                format_metric(selected_metrics.get('R¬≤', 'N/A'))
            ],
            'MAE': [
                format_metric(original_metrics.get('MAE', 'N/A')),
                format_metric(selected_metrics.get('MAE', 'N/A'))
            ],
            'MSE': [
                format_metric(original_metrics.get('MSE', 'N/A')),
                format_metric(selected_metrics.get('MSE', 'N/A'))
            ],
            'Best Parameters': [
                st.session_state.get('best_params', 'N/A'),
                st.session_state.get('best_params_selected', 'N/A')
            ]
        })
    else:
        st.error("Tipo de modelo n√£o reconhecido. N√£o √© poss√≠vel gerar a tabela de m√©tricas.")
        return

    # Exibir tabela de m√©tricas com ajustes finos
    st.dataframe(comparison_df.style.format({
        'Accuracy': '{:.4f}' if 'Accuracy' in comparison_df.columns else None,
        'Precision': '{:.4f}' if 'Precision' in comparison_df.columns else None,
        'Recall': '{:.4f}' if 'Recall' in comparison_df.columns else None,
        'F1-Score': '{:.4f}' if 'F1-Score' in comparison_df.columns else None,
        'R¬≤': '{:.4f}' if 'R¬≤' in comparison_df.columns else None,
        'MAE': '{:.4f}' if 'MAE' in comparison_df.columns else None,
        'MSE': '{:.4f}' if 'MSE' in comparison_df.columns else None,
    }).set_table_styles([
        {'selector': 'th', 'props': [('font-size', '14px'), ('background-color', '#f0f0f0'), ('text-align', 'center'), ('font-weight', 'bold')]},  # Cabe√ßalho
        {'selector': 'td', 'props': [('font-size', '14px'), ('text-align', 'center')]},  # Tamanho das c√©lulas e alinhamento
        {'selector': 'table', 'props': [('width', '100%'), ('border-collapse', 'collapse')]},  # Largura da tabela e bordas
        {'selector': 'tr:nth-child(even)', 'props': [('background-color', '#f9f9f9')]},  # Cor de fundo alternada para as linhas
        {'selector': 'tr:nth-child(odd)', 'props': [('background-color', '#ffffff')]},  # Cor de fundo para linhas √≠mpares
    ]))

    # Verificar se a m√©trica escolhida existe no DataFrame
    if scoring_metric_capitalized not in comparison_df.columns:
        st.error(f"A m√©trica '{scoring_metric}' n√£o est√° dispon√≠vel no DataFrame.")
        return


    # **GR√ÅFICOS DAS M√âTRICAS**
    st.subheader("Gr√°fico Interativo de Compara√ß√£o de M√©tricas")

    # Determinar as m√©tricas dispon√≠veis com base no tipo de modelo
    if model_type == "Classifica√ß√£o":
        metric_columns = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
    elif model_type == "Regress√£o":
        metric_columns = ['R¬≤', 'MAE', 'MSE']
    else:
        st.error("Tipo de modelo n√£o reconhecido. N√£o √© poss√≠vel gerar gr√°ficos.")
        return

    # Adicionar um filtro interativo para a sele√ß√£o da m√©trica
    selected_metric = st.selectbox(
        "Selecione a m√©trica para visualizar:",
        metric_columns,
        index=0  # M√©trica padr√£o exibida no in√≠cio
    )

    # Criar o gr√°fico apenas para a m√©trica selecionada
    if selected_metric in comparison_df.columns:
        fig, ax = plt.subplots(figsize=(10, 6))
        
        # Dados para o gr√°fico
        bars = ax.bar(
            comparison_df['Modelo'],
            comparison_df[selected_metric],
            color=['#9ACD32', '#006400']  # Verde claro e verde escuro
        )
        
        # Adicionar valores nas barras
        for bar in bars:
            height = bar.get_height()
            ax.annotate(f'{height:.4f}',
                        xy=(bar.get_x() + bar.get_width() / 2, height),
                        xytext=(0, 3),
                        textcoords="offset points",
                        ha='center', va='bottom',
                        fontsize=12)
        
        ax.set_title(f"Compara√ß√£o de {selected_metric}", fontsize=14)
        ax.set_ylabel(selected_metric, fontsize=12)
        ax.set_xlabel("Modelo", fontsize=12)
        
        # Ajustar altura para caber os valores
        plt.ylim(0, max(comparison_df[selected_metric]) * 1.1)
        
        # Exibir gr√°fico no Streamlit
        st.pyplot(fig)
    else:
        st.error(f"A m√©trica selecionada '{selected_metric}' n√£o est√° dispon√≠vel.")

    # **DETERMINAR O MELHOR MODELO BASEADO NA M√âTRICA ESCOLHIDA**
    scoring_values = comparison_df[scoring_metric_capitalized].values  # Recupera os valores da m√©trica na tabela
    if len(scoring_values) == 2:  # Certifique-se de que existem dois valores (sem e com sele√ß√£o)
        score_without_selection = scoring_values[0]
        score_with_selection = scoring_values[1]

        # Determina o melhor modelo
        if score_with_selection > score_without_selection:
            best_model = "Com Sele√ß√£o de Features"
            best_score = score_with_selection
        else:
            best_model = "Sem Sele√ß√£o de Features"
            best_score = score_without_selection
    else:
        st.warning("Erro na determina√ß√£o das m√©tricas na tabela.")
        return

    # Exibir mensagem com o melhor modelo
    st.success(f"üéâ **O melhor modelo √©:** {best_model} com base na m√©trica: {scoring_metric_capitalized} ({best_score:.4f})")

    # **INTERPRETA√á√ÉO DAS M√âTRICAS**
    st.subheader("Interpreta√ß√£o das M√©tricas")
    try:
        # Gerar interpreta√ß√£o para cada modelo
        if model_type == "Classifica√ß√£o":
            interpretation_without = generate_metrics_interpretation(original_metrics)
            interpretation_with = generate_metrics_interpretation(selected_metrics)
        elif model_type == "Regress√£o":
            interpretation_without = generate_regression_interpretation(original_metrics)
            interpretation_with = generate_regression_interpretation(selected_metrics)
        else:
            raise ValueError("Tipo de modelo desconhecido para interpreta√ß√£o.")

        # Exibir interpreta√ß√µes
        st.write("### Sem Sele√ß√£o de Features")
        st.write(interpretation_without)

        st.write("### Com Sele√ß√£o de Features")
        st.write(interpretation_with)
    except Exception as e:
        st.error(f"Erro ao gerar a interpreta√ß√£o das m√©tricas: {e}")
        
    # **DOWNLOAD DO MODELO TREINADO**
    st.subheader("Download do Melhor Modelo Treinado")
    model = st.session_state.models.get(st.session_state.selected_model_name)
    model_filename = save_best_model(model, with_feature_selection=(best_model == "Com Sele√ß√£o de Features"))

    if model_filename:
        with open(model_filename, "rb") as file:
            st.download_button(
                label="üíæ Download Melhor Modelo",
                data=file,
                file_name=model_filename,
                mime="application/octet-stream",
            )

    # **DOWNLOAD DO RELAT√ìRIO EM PDF**
    try:
        pdf_buffer = gerar_relatorio_pdf(comparison_df, best_model, st.session_state)
        pdf_file_name = f"relatorio_final_{st.session_state.get('selected_model_name', 'modelo')}.pdf"
        st.download_button(
            label="üíæ Download Relat√≥rio PDF",
            data=pdf_buffer,
            file_name=pdf_file_name,
            mime="application/pdf",
        )
    except Exception as e:
        st.error(f"Erro ao gerar relat√≥rio em PDF: {e}")

    # **CONCLUIR**
    if st.button("Concluir"):
        st.session_state.clear()  # Limpa o cache do Streamlit
        st.rerun()

############ Relat√≥rio Final para Clustering ###################
# Classe personalizada para PDF
class CustomPDF(FPDF):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # Baixar o logo no in√≠cio para reutiliz√°-lo
        self.logo_path = None
        logo_url = 'https://www.ipleiria.pt/normasgraficas/wp-content/uploads/sites/80/2017/09/estg_v-01.jpg'
        try:
            response = requests.get(logo_url)
            if response.status_code == 200:
                with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as tmpfile:
                    tmpfile.write(response.content)
                    self.logo_path = tmpfile.name
        except Exception as e:
            print(f"Erro ao baixar o logo: {e}")

    def header(self):
        # Posicionar o cabe√ßalho no topo da p√°gina
        self.set_y(10)
        
        # Adicionar a imagem no cabe√ßalho se o logo foi baixado com sucesso
        if self.logo_path:
            self.image(self.logo_path, 10, 10, 25)
        
        # Configurar fonte para o t√≠tulo
        self.set_font('Arial', 'B', 12)
        
        # Adicionar o t√≠tulo centralizado
        # Deixar espa√ßo para o logo
        self.cell(25)  # Espa√ßo para o logo
        self.cell(0, 10, 'MLCase - Plataforma de Machine Learning', 0, 0, 'C')
        
        # Adicionar uma linha horizontal ap√≥s o cabe√ßalho
        self.ln(15)
        self.ln(5)  # Espa√ßo ap√≥s o cabe√ßalho

    def footer(self):
        # Ir para 1.5 cm da parte inferior
        self.set_y(-20)
        
        # Adicionar uma linha horizontal antes do rodap√©
        self.line(10, self.get_y(), 200, self.get_y())
        self.ln(3)
        
        # Definir fonte para o rodap√©
        self.set_font('Arial', 'I', 8)
        
        # Data atual
        current_date = datetime.now().strftime('%d/%m/%Y')
        
        # Adicionar rodap√© com a data e n√∫mero da p√°gina
        self.cell(0, 10, f'{current_date} - P√°gina {self.page_no()}  |  Autora da Plataforma: Bruna Sousa', 0, 0, 'C')
# Fun√ß√£o para gerar o relat√≥rio PDF
import os
import matplotlib.pyplot as plt
from fpdf import FPDF
from io import BytesIO

def gerar_relatorio_clustering_pdf(initial_metrics, retrain_metrics, best_model_type, st_session):
    pdf = CustomPDF(format='A4')
    pdf.set_auto_page_break(auto=True, margin=15)
    pdf.add_page()

    # T√≠tulo
    pdf.set_font("Arial", style="B", size=16)
    pdf.cell(0, 10, txt="Relat√≥rio Final do Modelo Treinados", ln=True, align="C")
    pdf.ln(10)

    # Modelo Selecionado
    pdf.set_font("Arial", style="B", size=12)
    pdf.cell(50, 10, txt="Modelo Selecionado:", ln=False)
    pdf.set_font("Arial", size=12)
    model_info = st_session['selected_model_name']
    
    # Adicionar informa√ß√£o de componentes para KMeans e Clustering Hier√°rquico
    if st_session['selected_model_name'] in ["KMeans", "Clustering Hier√°rquico"]:
        model_info += f" (PCA: {st_session.get('pca_n_components', 'N/A')} componentes)"
    
    pdf.cell(0, 10, txt=model_info, ln=True)
    pdf.ln(5)

    # Melhor Modelo
    pdf.set_font("Arial", style="B", size=12)
    pdf.cell(50, 10, txt="Melhor Modelo Treinado:", ln=False)
    pdf.set_font("Arial", size=12)
    pdf.cell(0, 10, txt=best_model_type, ln=True)
    pdf.ln(10)

    # Adicionar m√©tricas do treino inicial e re-treino
    pdf.set_font("Arial", style="B", size=12)
    pdf.cell(0, 10, txt="M√©tricas Obtidas", ln=True)
    pdf.set_font("Arial", size=9)
    pdf.set_fill_color(200, 220, 255)

    # Cabe√ßalho da tabela
    pdf.set_fill_color(144, 238, 144)  # Verde claro (light green)
    col_widths = [40, 40, 40, 40]
    pdf.cell(col_widths[0], 10, "Treino", 1, 0, 'C', True)
    pdf.cell(col_widths[1], 10, "Silhouette Score", 1, 0, 'C', True)
    pdf.cell(col_widths[2], 10, "Davies-Bouldin Index", 1, 0, 'C', True)
    pdf.cell(col_widths[3], 10, "Calinski-Harabasz Score", 1, 1, 'C', True)

    # Dados da tabela
    pdf.set_font("Arial", size=8)
    pdf.cell(col_widths[0], 10, "Treino Inicial", 1, 0, 'C')
    pdf.cell(col_widths[1], 10, f"{initial_metrics['Silhouette Score']:.2f}", 1, 0, 'C')
    pdf.cell(col_widths[2], 10, f"{initial_metrics['Davies-Bouldin Index']:.2f}", 1, 0, 'C')
    pdf.cell(col_widths[3], 10, f"{initial_metrics['Calinski-Harabasz Score']:.2f}", 1, 1, 'C')

    if retrain_metrics:
        pdf.cell(col_widths[0], 10, "Re-Treino", 1, 0, 'C')
        pdf.cell(col_widths[1], 10, f"{retrain_metrics['Silhouette Score']:.2f}", 1, 0, 'C')
        pdf.cell(col_widths[2], 10, f"{retrain_metrics['Davies-Bouldin Index']:.2f}", 1, 0, 'C')
        pdf.cell(col_widths[3], 10, f"{retrain_metrics['Calinski-Harabasz Score']:.2f}", 1, 1, 'C')

    pdf.ln(10)

    # Adicionar interpreta√ß√µes
    def add_interpretation(metrics, treino_name):
        pdf.set_font("Arial", size=10)
        pdf.cell(0, 10, txt=f"{treino_name}:", ln=True)
        pdf.multi_cell(0, 10, txt=f"  Silhouette Score: {metrics['Silhouette Score']:.2f} - "
                                  f"{'Excelente' if metrics['Silhouette Score'] > 0.75 else 'Bom' if metrics['Silhouette Score'] > 0.5 else 'Moderado' if metrics['Silhouette Score'] > 0.25 else 'Fraco'} separa√ß√£o entre clusters.")
        pdf.multi_cell(0, 10, txt=f"  Davies-Bouldin Index: {metrics['Davies-Bouldin Index']:.2f} - "
                                  f"{'Muito bom' if metrics['Davies-Bouldin Index'] < 0.5 else 'Bom' if metrics['Davies-Bouldin Index'] < 1.0 else 'Moderado' if metrics['Davies-Bouldin Index'] < 2.0 else 'Fraco'} compacta√ß√£o e separa√ß√£o.")
        pdf.multi_cell(0, 10, txt=f"  Calinski-Harabasz Score: {metrics['Calinski-Harabasz Score']:.2f} - "
                                  f"{'Excelente' if metrics['Calinski-Harabasz Score'] > 2500 else 'Bom' if metrics['Calinski-Harabasz Score'] > 1500 else 'Moderado' if metrics['Calinski-Harabasz Score'] > 500 else 'Fraco'} densidade e separa√ß√£o.")
        pdf.ln(5)

    add_interpretation(initial_metrics, "Treino Inicial")
    if retrain_metrics:
        add_interpretation(retrain_metrics, "Re-Treino")

    # Adicionar gr√°ficos
    metrics_to_plot = ["Silhouette Score", "Davies-Bouldin Index", "Calinski-Harabasz Score"]
    graphs = []
    for metric in metrics_to_plot:
        plt.figure(figsize=(6, 4))
        labels = ["Treino Inicial"]
        values = [initial_metrics[metric]]
        if retrain_metrics:
            labels.append("Re-Treino")
            values.append(retrain_metrics[metric])
        plt.bar(labels, values, color=['#90EE90', '#006400'], edgecolor='black')
        plt.title(f"{metric} por Treino")
        plt.ylabel(metric)
        plt.xlabel("Treino")
        graph_path = f"temp_{metric.replace(' ', '_')}.png"
        plt.savefig(graph_path)
        plt.close()
        graphs.append(graph_path)

    # Inserir gr√°ficos no PDF
    pdf.add_page()
    pdf.set_font("Arial", style="B", size=12)
    pdf.cell(0, 10, txt="Gr√°ficos das M√©tricas", ln=True, align='C')
    pdf.ln(10)

    x_offset = 10
    y_offset = pdf.get_y()
    for i, graph in enumerate(graphs):
        pdf.image(graph, x=x_offset, y=y_offset, w=90, h=70)
        x_offset += 100
        if (i + 1) % 2 == 0:  # Nova linha a cada dois gr√°ficos
            x_offset = 10
            y_offset += 75
        os.remove(graph)  # Remover o arquivo tempor√°rio ap√≥s us√°-lo

    # Salvar o PDF no buffer
    pdf_buffer = BytesIO()
    pdf_output = pdf.output(dest='S').encode('latin1')
    pdf_buffer.write(pdf_output)
    pdf_buffer.seek(0)

    return pdf_buffer

# P√°gina final para clustering
def clustering_final_page():
    st.title("Relat√≥rio Final do Clustering")

    # Verificar se os dados est√£o dispon√≠veis
    if "selected_model_name" not in st.session_state or "initial_metrics" not in st.session_state:
        st.error("Nenhuma informa√ß√£o de clustering dispon√≠vel. Por favor, execute o treino primeiro.")
        return

    # Mostrar o modelo selecionado
    st.subheader("Modelo Selecionado")
    st.write(f"**Modelo:** {st.session_state.selected_model_name}")

    # Adicionar informa√ß√£o sobre o n√∫mero de componentes
    if st.session_state.selected_model_name in ["KMeans", "Clustering Hier√°rquico"]:
        st.write(f"**N√∫mero de Componentes PCA:** {st.session_state.get('pca_n_components', 'N/A')}")
    
# Exibir m√©tricas do treino inicial
    st.subheader("M√©tricas do Treino Inicial")
    st.table(fix_dataframe_types(pd.DataFrame([st.session_state.initial_metrics])))

    # Interpreta√ß√£o personalizada para o treino inicial
    initial_metrics = st.session_state.initial_metrics
    st.write("**Interpreta√ß√£o do Treino Inicial:**")
    st.markdown(f"""
    - **Silhouette Score:** {initial_metrics["Silhouette Score"]:.2f} - {"Excelente" if initial_metrics["Silhouette Score"] > 0.75 else "Bom" if initial_metrics["Silhouette Score"] > 0.5 else "Moderado" if initial_metrics["Silhouette Score"] > 0.25 else "Fraco"} separa√ß√£o entre clusters.
    - **Davies-Bouldin Index:** {initial_metrics["Davies-Bouldin Index"]:.2f} - {"Muito bom" if initial_metrics["Davies-Bouldin Index"] < 0.5 else "Bom" if initial_metrics["Davies-Bouldin Index"] < 1.0 else "Moderado" if initial_metrics["Davies-Bouldin Index"] < 2.0 else "Fraco"} compacta√ß√£o e separa√ß√£o.
    - **Calinski-Harabasz Score:** {initial_metrics["Calinski-Harabasz Score"]:.2f} - {"Excelente" if initial_metrics["Calinski-Harabasz Score"] > 2500 else "Bom" if initial_metrics["Calinski-Harabasz Score"] > 1500 else "Moderado" if initial_metrics["Calinski-Harabasz Score"] > 500 else "Fraco"} densidade e separa√ß√£o.
    """)

    # Exibir m√©tricas do re-treino (se dispon√≠veis)
    retrain_silhouette_score = None  # Inicializa como None para evitar erros
    if "retrain_metrics" in st.session_state:
        st.subheader("M√©tricas do Re-Treino")
        st.table(fix_dataframe_types(pd.DataFrame([st.session_state.retrain_metrics])))

        # Interpreta√ß√£o personalizada para o re-treino
        retrain_metrics = st.session_state.retrain_metrics
        retrain_silhouette_score = retrain_metrics["Silhouette Score"]
        st.write("**Interpreta√ß√£o do Re-Treino:**")
        st.markdown(f"""
        - **Silhouette Score:** {retrain_metrics["Silhouette Score"]:.2f} - {"Excelente" if retrain_metrics["Silhouette Score"] > 0.75 else "Bom" if retrain_metrics["Silhouette Score"] > 0.5 else "Moderado" if retrain_metrics["Silhouette Score"] > 0.25 else "Fraco"} separa√ß√£o entre clusters.
        - **Davies-Bouldin Index:** {retrain_metrics["Davies-Bouldin Index"]:.2f} - {"Muito bom" if retrain_metrics["Davies-Bouldin Index"] < 0.5 else "Bom" if retrain_metrics["Davies-Bouldin Index"] < 1.0 else "Moderado" if retrain_metrics["Davies-Bouldin Index"] < 2.0 else "Fraco"} compacta√ß√£o e separa√ß√£o.
        - **Calinski-Harabasz Score:** {retrain_metrics["Calinski-Harabasz Score"]:.2f} - {"Excelente" if retrain_metrics["Calinski-Harabasz Score"] > 2500 else "Bom" if retrain_metrics["Calinski-Harabasz Score"] > 1500 else "Moderado" if retrain_metrics["Calinski-Harabasz Score"] > 500 else "Fraco"} densidade e separa√ß√£o.
        """)

    # Determinar o melhor modelo considerando apenas o Silhouette Score
    if retrain_silhouette_score is not None and retrain_silhouette_score > initial_metrics["Silhouette Score"]:
        melhor_modelo = "Re-Treino"
        best_metrics = st.session_state.retrain_metrics
        best_model = st.session_state.models[st.session_state.selected_model_name]
    else:
        melhor_modelo = "Treino Inicial"
        best_metrics = st.session_state.initial_metrics
        best_model = st.session_state.models[st.session_state.selected_model_name]

    st.subheader("Melhor Modelo")
    st.success(f"üéâ **{melhor_modelo}** com Silhouette Score: {max(initial_metrics['Silhouette Score'], retrain_silhouette_score or 0):.4f}")


    # **Gr√°ficos Interativos das M√©tricas**
    st.subheader("Gr√°fico Interativo de M√©tricas")
    metrics_to_plot = ["Silhouette Score", "Davies-Bouldin Index", "Calinski-Harabasz Score"]
    selected_metric = st.selectbox("Selecione a m√©trica para visualizar:", metrics_to_plot)
    
    # Criar o gr√°fico
    if selected_metric:
        # Verificar se os dados do re-treino est√£o presentes
        if "retrain_metrics" in st.session_state:
            # Se o re-treino foi realizado, exibe "Treino Inicial" e "Re-Treino"
            data_to_plot = pd.DataFrame({
                "Treino": ["Treino Inicial", "Re-Treino"],
                selected_metric: [
                    initial_metrics[selected_metric],
                    retrain_metrics[selected_metric]
                ]
            })
        else:
            # Se o re-treino n√£o foi realizado, exibe todas as m√©tricas para "Treino Inicial"
            data_to_plot = pd.DataFrame({
                "Treino": ["Treino Inicial"] * len(metrics_to_plot),
                selected_metric: [initial_metrics[metric] for metric in metrics_to_plot]
            })
    
        # Criar gr√°fico com base nos dados dispon√≠veis
        fig, ax = plt.subplots(figsize=(6, 4))
        ax.bar(data_to_plot["Treino"], data_to_plot[selected_metric], color=['#a8ddb5', '#005a32'], edgecolor='black')
        ax.set_title(f"Compara√ß√£o de {selected_metric}", fontsize=14, fontweight='bold')
        ax.set_ylabel(selected_metric, fontsize=12)
        ax.set_xlabel("Treino", fontsize=12)
        ax.tick_params(axis='both', which='major', labelsize=10)
        st.pyplot(fig) 


    # Gerar o relat√≥rio PDF
    pdf_buffer = gerar_relatorio_clustering_pdf(
        initial_metrics,
        st.session_state.get("retrain_metrics"),
        melhor_modelo,
        st.session_state
    )

    # Bot√£o para download do relat√≥rio em PDF
    pdf_filename = f"Relatorio__{st.session_state.selected_model_name}_{st.session_state.model_type}_{melhor_modelo.replace(' ', '_').lower()}.pdf"
    st.download_button(
        label="Baixar Relat√≥rio em PDF",
        data=pdf_buffer,
        file_name=pdf_filename,
        mime="application/pdf"
    )

    # Bot√£o para download do melhor modelo treinado
    model_buffer = BytesIO()
    pickle.dump(best_model, model_buffer)
    model_buffer.seek(0)

    st.download_button(
        label="Baixar Melhor Modelo Treinado",
        data=model_buffer,
        file_name=f"melhor_modelo_{melhor_modelo.replace(' ', '_').lower()}.pkl",
        mime="application/octet-stream"
    )

    # Bot√£o para concluir o processo
    if st.button("Concluir"):
        st.info("Clustering finalizado. Redirecionando para o in√≠cio...")
        st.session_state.clear()
        st.session_state.step = 'file_upload'
        st.rerun()

def initialize_session_state():
    # Inicializando as vari√°veis principais de estado
    default_values = {
        'step': 'file_upload',
        'page': 'file_upload',
        'data': None,
        'target_column': None,
        'target_column_confirmed': False,
        'validation_method': None,
        'validation_confirmed': False,
        'model_type': None,
        'model_type_confirmed': False,
        'X_train': None,
        'X_test': None,
        'y_train': None,
        'y_test': None,
        'knn_neighbors': 5,
        'rf_estimators': 100,
        'svc_kernel': 'rbf',
        'kmeans_clusters': 3,
        'feature_selection_done': False,
        'X_train_selected': None,
        'X_test_selected': None,
        'model_name': None,
        'selected_model': None,
        'selected_model_name': None,
        'models': {
            "Support Vector Classification (SVC)": SVC(),
            "K-Nearest Neighbors (KNN)": KNeighborsClassifier(),
            "Random Forest": RandomForestClassifier(),
            "KMeans": KMeans(),
            "Clustering Hier√°rquico": AgglomerativeClustering(linkage='ward'),
            "Regress√£o Linear Simples (RLS)": LinearRegression(),
            "Regress√£o por Vetores de Suporte (SVR)": SVR(),
        },
        'model_trained': False,
        'clustering_final_page': False,  # P√°gina do relat√≥rio final de clustering
        'grid_search_confirmed': False,  # Adicionando grid_search_confirmed ao estado
        'manual_params': {}, # Inicializando manual_params como um dicion√°rio vazio
        'best_params_str': {},
        'treinos_realizados': [],  # Inicializando a lista de treinos realizados
        'scoring_confirmed': False,  # Inicializando scoring_confirmed
        'target_column_type': None,  # Adiciona tipo da coluna alvo
        'selected_scoring': 'F1-Score'  # Inicializando 'selected_scoring' com o valor 'f1'
    }

    # Usando valores padr√£o para inicializar session_state
    for key, value in default_values.items():
        if key not in st.session_state:
            st.session_state[key] = value

    # Adicionalmente, voc√™ pode garantir que os par√¢metros do modelo sejam v√°lidos:
    if st.session_state.knn_neighbors < 1:
        st.session_state.knn_neighbors = 5  # Default para KNN
    if st.session_state.kmeans_clusters < 1:
        st.session_state.kmeans_clusters = 3  # Default para KMeans

# Fun√ß√£o principal
def main():
    # Inicializa√ß√£o das vari√°veis de estado da sess√£o
    initialize_session_state()

    # Exibir estado atual para depura√ß√£o
    #st.write(f"üìå Estado atual: {st.session_state.step}")

    # Roteamento baseado no estado atual
    if st.session_state.step == 'file_upload':
        upload_file()
    elif st.session_state.step == 'data_preview':
        data_preview()
    elif st.session_state.step == 'missing_values':
        handle_missing_values()
    elif st.session_state.step == 'outlier_detection':
        outlier_detection()
    elif st.session_state.step == 'data_summary':
        data_summary()
    elif st.session_state.step == 'model_selection':
        model_selection()
    elif st.session_state.step == 'feature_selection':
        feature_selection()
    elif st.session_state.step == 'train_with_selected_features':  
        train_with_selected_features_page()
    elif st.session_state.step == 'evaluate_and_compare_models':
        evaluate_and_compare_models()
    elif st.session_state.step == 'clustering_final_page':  # ‚úÖ Adicionado!
        clustering_final_page()  # ‚úÖ Chama a fun√ß√£o do relat√≥rio final de clustering
    elif st.session_state.step == 'final_page':
        final_page()
    else:
        st.error(f"‚ö† Etapa desconhecida: {st.session_state.step}. Reiniciando a aplica√ß√£o.")
        st.session_state.step = 'file_upload'
        st.rerun()
        
    # Exibir o estado ap√≥s a execu√ß√£o para depura√ß√£o
    #st.write(f"Estado final: {st.session_state.step}")


if __name__ == "__main__":
    main()
