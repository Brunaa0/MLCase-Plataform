########### Bibliotecas Necessﾃ｡rias ###########

# -------------------------------------
# 東 Bibliotecas para Interface com Utilizador (Streamlit)
# -------------------------------------
import streamlit as st  # Framework para criaﾃｧﾃ｣o de interfaces web interativas
import streamlit.components.v1 as components  # Permite adicionar componentes HTML/CSS personalizados

# -------------------------------------
# 東 Manipulaﾃｧﾃ｣o e Anﾃ｡lise de Dados
# -------------------------------------
import pandas as pd  # Manipulaﾃｧﾃ｣o de DataFrames e sﾃｩries temporais
import numpy as np  # Operaﾃｧﾃｵes numﾃｩricas e matrizes eficientes

# -------------------------------------
# 東 Visualizaﾃｧﾃ｣o de Dados
# -------------------------------------
import matplotlib.pyplot as plt  # Criaﾃｧﾃ｣o de grﾃ｡ficos estﾃ｡ticos
import seaborn as sns  # Grﾃ｡ficos estatﾃｭsticos avanﾃｧados baseados no Matplotlib
import plotly.express as px  # Grﾃ｡ficos interativos e visualizaﾃｧﾃｵes dinﾃ｢micas

# -------------------------------------
# 東 Modelos de Machine Learning
# -------------------------------------
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor  # Modelos baseados em ﾃ｡rvores de decisﾃ｣o
from sklearn.linear_model import LogisticRegression, LinearRegression  # Modelos lineares para classificaﾃｧﾃ｣o e regressﾃ｣o
from sklearn.svm import SVC, SVR  # Modelos de Support Vector Machine (SVM) para classificaﾃｧﾃ｣o e regressﾃ｣o
from sklearn.cluster import KMeans, AgglomerativeClustering  # Algoritmos de clustering
from sklearn.neighbors import KNeighborsClassifier  # Modelo de vizinhos mais prﾃｳximos (KNN)
from sklearn import svm, tree, neighbors  # Modelos adicionais do sklearn

# -------------------------------------
# 東 Seleﾃｧﾃ｣o de Features (Atributos)
# -------------------------------------
from mlxtend.feature_selection import SequentialFeatureSelector  # Seleﾃｧﾃ｣o sequencial de variﾃ｡veis para otimizar modelos

# -------------------------------------
# 東 Mﾃｩtricas de Avaliaﾃｧﾃ｣o
# -------------------------------------
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,  # Mﾃｩtricas para classificaﾃｧﾃ｣o
    confusion_matrix, classification_report, roc_auc_score,  # Matriz de confusﾃ｣o e anﾃ｡lise ROC
    mean_squared_error, mean_absolute_error, r2_score,  # Mﾃｩtricas para regressﾃ｣o
    silhouette_score, davies_bouldin_score, calinski_harabasz_score  # Mﾃｩtricas para clustering
)

# -------------------------------------
# 東 Prﾃｩ-Processamento e Pipeline
# -------------------------------------
from sklearn.model_selection import (
    train_test_split,  # Separaﾃｧﾃ｣o entre dados de treino e teste
    KFold, LeaveOneOut, cross_val_score,  # Validaﾃｧﾃ｣o cruzada para avaliar modelos
    GridSearchCV  # Procura de melhores hiperparﾃ｢metros usando Grid Search
)
from sklearn.preprocessing import StandardScaler, LabelEncoder  # Normalizaﾃｧﾃ｣o e codificaﾃｧﾃ｣o de variﾃ｡veis categﾃｳricas
from sklearn.impute import SimpleImputer  # Tratamento de valores ausentes

# -------------------------------------
# 東 Utilitﾃ｡rios Diversos
# -------------------------------------
import os  # Operaﾃｧﾃｵes no sistema de arquivos (criaﾃｧﾃ｣o de pastas, leitura de arquivos)
import joblib  # Guardar e carregar  modelos treinados
import pickle  # Serializaﾃｧﾃ｣o e desserializaﾃｧﾃ｣o de objetos Python
import json  # Manipulaﾃｧﾃ｣o de arquivos JSON
import requests  # Requisiﾃｧﾃｵes HTTP para acesso a APIs externas
import unidecode  # Remoﾃｧﾃ｣o de acentos e normalizaﾃｧﾃ｣o de caracteres especiais

# -------------------------------------
# 東 Manipulaﾃｧﾃ｣o de Arquivos e Dados Binﾃ｡rios
# -------------------------------------
from io import BytesIO  # Manipulaﾃｧﾃ｣o de streams binﾃ｡rios para arquivos em memﾃｳria
import tempfile  # Criaﾃｧﾃ｣o de arquivos e diretﾃｳrios temporﾃ｡rios

# -------------------------------------
# 東 Manipulaﾃｧﾃ｣o de Datas e Cﾃ｡lculos Matemﾃ｡ticos
# -------------------------------------
from datetime import datetime  # Manipulaﾃｧﾃ｣o de datas e horas
from decimal import Decimal  # Precisﾃ｣o extra em cﾃ｡lculos decimais
from fractions import Fraction  # Trabalha com fraﾃｧﾃｵes matemﾃ｡ticas exatas
from scipy.sparse import csr_matrix  # Representaﾃｧﾃ｣o eficiente de matrizes esparsas
import scipy  # Biblioteca cientﾃｭfica para estatﾃｭsticas, ﾃ｡lgebra linear e otimizaﾃｧﾃ｣o
import time  # Mediﾃｧﾃ｣o do tempo de execuﾃｧﾃ｣o de processos

# -------------------------------------
# 東 Bibliotecas para Geraﾃｧﾃ｣o de Relatﾃｳrios
# -------------------------------------
from fpdf import FPDF  # Criaﾃｧﾃ｣o de documentos PDF programaticamente
from reportlab.lib.pagesizes import letter  # Definiﾃｧﾃ｣o do tamanho da pﾃ｡gina nos relatﾃｳrios
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle  # Estilos para formataﾃｧﾃ｣o de texto
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, Image  # Estruturaﾃｧﾃ｣o de documentos PDF
from reportlab.lib import colors  # Definiﾃｧﾃ｣o de cores em relatﾃｳrios
from reportlab.lib.units import inch  # Unidades de medida para layout de documentos



##############################################
# -------------------------------------
# 東 Funﾃｧﾃ｣o JavaScript para voltar ao topo da pﾃ｡gina
# -------------------------------------

# Script JavaScript que permite rolar automaticamente para o topo da pﾃ｡gina
scroll_to_top_js = """
<script>
    function scrollToTop() {
        window.scrollTo(0, 0);  // Move a pﾃ｡gina para o topo (coordenadas 0,0)
    }
</script>
"""

# Insere o JavaScript na pﾃ｡gina com Streamlit
# Definiﾃｧﾃ｣o de height=0 e width=0 para evitar que o cﾃｳdigo ocupe espaﾃｧo visﾃｭvel na interface
components.html(scroll_to_top_js, height=0, width=0)  

# -------------------------------------
# 東 Ajustes de Exibiﾃｧﾃ｣o do Pandas Styler
# -------------------------------------

# Define o nﾃｺmero mﾃ｡ximo de elementos a serem renderizados no Styler do Pandas
pd.set_option("styler.render.max_elements", 2000000)  # Ajustar se necessﾃ｡rio para grandes DataFrames

# Configura a exibiﾃｧﾃ｣o de todas as linhas e colunas de um DataFrame
pd.set_option("display.max_rows", None)  # Permite visualizar todas as linhas sem truncamento
pd.set_option("display.max_columns", None)  # Permite visualizar todas as colunas sem truncamento


##############################################
def fix_dataframe_types(df):
    """Corrigir tipos de dados num DataFrame para compatibilidade com PyArrow"""

    # Verificar se o objeto ﾃｩ um Styler e extrair o DataFrame
    if hasattr(df, 'data'):  # Objetos Styler possuem um atributo .data
        df = df.data
    elif hasattr(df, 'render') and not hasattr(df, 'copy'):  # Outra forma de identificar um Styler
        # Para versﾃｵes mais recentes do pandas
        if hasattr(df, '_data'):
            df = df._data
        # Para versﾃｵes ainda mais recentes do pandas, onde a estrutura pode ser diferente
        elif hasattr(df, 'data'):
            df = df.data
        # Se ainda nﾃ｣o for possﾃｭvel extrair o DataFrame
        else:
            # Tentar converter primeiro para dicionﾃ｡rio e depois para DataFrame
            try:
                df = pd.DataFrame(df.to_dict())
            except:
                # Se falhar, retornar um DataFrame vazio
                return pd.DataFrame()
    
    # Se o objeto final nﾃ｣o for um DataFrame, retornar um DataFrame vazio
    if not isinstance(df, pd.DataFrame):
        return pd.DataFrame()
        
    # Criar uma cﾃｳpia do DataFrame para evitar modificar o original
    df_fixed = df.copy()
    
    # Percorrer todas as colunas para corrigir tipos de dados problemﾃ｡ticos
    for col in df_fixed.columns:
        # Converter colunas do tipo Int64 para int64 padrﾃ｣o (evita problemas de compatibilidade)
        if hasattr(df_fixed[col], 'dtype') and str(df_fixed[col].dtype) == 'Int64':
            df_fixed[col] = df_fixed[col].fillna(-1).astype('int64')  # Substituir valores nulos por -1 antes da conversﾃ｣o
        
        # Converter colunas do tipo objeto (strings e dados complexos) para string
        elif df_fixed[col].dtype == 'object':
            try:
                # Tentar converter diretamente para string
                df_fixed[col] = df_fixed[col].astype(str)
            except:
                # Se falhar, aplicar uma conversﾃ｣o manual, garantindo que valores None sejam tratados
                df_fixed[col] = df_fixed[col].apply(lambda x: str(x) if x is not None else "")
    
    # Retornar o DataFrame corrigido
    return df_fixed


##############################################
# -------------------------------------
# 東 Funﾃｧﾃ｣o para Configurar a Barra Lateral
# -------------------------------------

def configure_sidebar():
    """Configura a barra lateral com o logﾃｳtipo da instituiﾃｧﾃ｣o e informaﾃｧﾃｵes sobre a plataforma."""
    
    with st.sidebar:  # Define que os elementos serﾃ｣o adicionados na barra lateral
        st.image(
            "https://www.ipleiria.pt/normasgraficas/wp-content/uploads/sites/80/2017/09/estg_v-01.jpg",  # URL da imagem
            width=80,  # Define o tamanho da imagem (largura em pixels)
            caption="Logﾃｳtipo da Escola"  # Texto exibido abaixo da imagem
        )
        
        # Exibe o nome da plataforma em formato HTML para maior personalizaﾃｧﾃ｣o
        st.markdown("<p>MLCase - Plataforma de Machine Learning</p>", unsafe_allow_html=True)
        
        # Exibe o nome da autora com destaque em negrito usando HTML
        st.markdown("<p><b>Autora:</b> Bruna Sousa</p>", unsafe_allow_html=True)

# Chamada da funﾃｧﾃ｣o para configurar a barra lateral
configure_sidebar()


##############################################
import matplotlib
matplotlib.use('Agg')  # Usar backend nﾃ｣o interativo
import matplotlib.pyplot as plt
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
##############################################

# -------------------------------------
# 東 FUNﾃﾃグ DE UPLOAD DE FICHEIROS
# -------------------------------------

# Funﾃｧﾃ｣o para inicializar variﾃ｡veis de estado na aplicaﾃｧﾃ｣o
def initialize_state():
    """Inicializa variﾃ｡veis de estado utilizadas na aplicaﾃｧﾃ｣o para gerir diferentes etapas do processo."""
    st.session_state.step = 'data_preview'  # Define o estado inicial como prﾃｩ-visualizaﾃｧﾃ｣o dos dados
    st.session_state.selected_columns = []  # Lista para armazenar colunas selecionadas pelo utilizador
    st.session_state.numeric_types = {}  # Dicionﾃ｡rio para armazenar tipos numﾃｩricos das variﾃ｡veis
    st.session_state.variable_types = {}  # Dicionﾃ｡rio para armazenar os tipos das variﾃ｡veis
    st.session_state.treatment_state = {}  # Dicionﾃ｡rio para armazenar o estado do tratamento dos dados
    st.session_state.all_treated = False  # Flag para indicar se todos os dados foram tratados

# -------------------------------------
# 東 Funﾃｧﾃ｣o auxiliar para escolher o delimitador de ficheiros CSV
# -------------------------------------

def choose_delimiter():
    """Permite ao utilizador escolher um delimitador para ficheiros CSV carregados."""
    
    # Lista de delimitadores comuns, incluindo a opﾃｧﾃ｣o personalizada
    delimiters = [",", ";", "\t", "|", "Outro"]
    
    # Cria um seletor na barra lateral para escolha do delimitador
    delimiter = st.sidebar.selectbox("Escolha o delimitador para CSV", delimiters, index=0)
    
    # Se o utilizador escolher a opﾃｧﾃ｣o "Outro", permite inserir um delimitador personalizado
    if delimiter == "Outro":
        delimiter = st.sidebar.text_input("Digite o delimitador personalizado:")
    
    return delimiter

# -------------------------------------
# 東 Funﾃｧﾃ｣o para a etapa de upload do ficheiro
# -------------------------------------

def upload_file():
    """Permite ao utilizador carregar um ficheiro de dados para a plataforma."""
    
    st.title("MLCase - Plataforma de Machine Learning")  # Tﾃｭtulo principal da aplicaﾃｧﾃ｣o

    # Seleﾃｧﾃ｣o do tipo de ficheiro a ser carregado
    file_type = st.sidebar.selectbox("Selecione o tipo de arquivo", ["CSV", "Excel", "JSON"])
    delimiter = ","  # Define o delimitador padrﾃ｣o para CSV

    # Processo de upload conforme o tipo de ficheiro selecionado
    if file_type == "CSV":
        delimiter = choose_delimiter()  # Permite selecionar um delimitador para o CSV
        file = st.sidebar.file_uploader("Carregar arquivo", type=["csv"])  # Botﾃ｣o de upload
    elif file_type == "Excel":
        file = st.sidebar.file_uploader("Carregar arquivo", type=["xlsx", "xls"])  # Upload de ficheiro Excel
    elif file_type == "JSON":
        file = st.sidebar.file_uploader("Carregar arquivo", type=["json"])  # Upload de ficheiro JSON

    # Se um ficheiro for carregado, tenta processﾃ｡-lo
    if file is not None:
        try:
            # Chama a funﾃｧﾃ｣o de carregamento de dados e inicializa as variﾃ｡veis de estado
            st.session_state.data = load_data(file_type, file, delimiter)
            initialize_state()
            st.sidebar.success(f"Conjunto de dados {file_type} carregado com sucesso!")  # Mensagem de sucesso

            # Botﾃ｣o para avanﾃｧar para a prﾃｳxima etapa (prﾃｩ-visualizaﾃｧﾃ｣o dos dados)
            if st.sidebar.button("Dados Carregados"):
                st.session_state.step = 'data_preview'  # Atualiza o estado para a prﾃｩ-visualizaﾃｧﾃ｣o
                st.stop()  # Para a execuﾃｧﾃ｣o para refletir as mudanﾃｧas

        except Exception as e:
            st.sidebar.error(f"Erro ao carregar o arquivo: {e}")  # Exibe mensagem de erro caso algo corra mal

# -------------------------------------
# 東 Funﾃｧﾃ｣o para carregar dados com cache (evita recarregamento desnecessﾃ｡rio)
# -------------------------------------

@st.cache_data  # Usa cache para evitar recarregar os dados vﾃ｡rias vezes
def load_data(file_type, file, delimiter):
    """Carrega um ficheiro de dados conforme o tipo selecionado pelo utilizador."""
    
    if file_type == "CSV":
        return pd.read_csv(file, delimiter=delimiter)  # Carrega dados CSV com o delimitador escolhido
    elif file_type == "Excel":
        return pd.read_excel(file)  # Carrega ficheiro Excel
    elif file_type == "JSON":
        return pd.read_json(file)  # Carrega ficheiro JSON

##############################################
# -------------------------------------
# 東 FUNﾃﾃグ DE SELEﾃﾃグ DE COLUNAS
# -------------------------------------

# Funﾃｧﾃ｣o para prﾃｩ-visualizar os dados e permitir a seleﾃｧﾃ｣o de colunas e tipos de variﾃ｡veis
def data_preview():
    """Permite visualizar os dados carregados, selecionar colunas e definir os seus tipos."""

    # Exibir uma prﾃｩ-visualizaﾃｧﾃ｣o dos primeiros registos do dataset (com correﾃｧﾃ｣o de tipos)
    st.subheader("Prﾃｩ-visualizaﾃｧﾃ｣o dos dados")
    st.dataframe(fix_dataframe_types(st.session_state.data.head()))  # Corrige os tipos antes da exibiﾃｧﾃ｣o

    # Obter a lista de colunas do dataset
    columns = st.session_state.data.columns.tolist()

    # Criar uma caixa de seleﾃｧﾃ｣o mﾃｺltipla para escolher quais colunas utilizar
    selected_columns = st.multiselect("Colunas", columns, columns)  # Por defeito, todas as colunas sﾃ｣o selecionadas
    st.session_state.selected_columns = selected_columns  # Guardar as colunas selecionadas no estado global

    # Preservar transformaﾃｧﾃｵes no estado global
    if 'filtered_data' not in st.session_state:
        st.session_state.filtered_data = st.session_state.data.copy()  # Criar uma cﾃｳpia inicial dos dados
    else:
        # Atualizar os dados filtrados apenas com as colunas selecionadas, mantendo transformaﾃｧﾃｵes jﾃ｡ aplicadas
        st.session_state.filtered_data = st.session_state.data[selected_columns]

    # Se houver colunas selecionadas, permitir a identificaﾃｧﾃ｣o dos tipos de variﾃ｡veis
    if selected_columns:
        st.subheader("Identificar tipos de variﾃ｡veis")

        # Inicializar dicionﾃ｡rio para armazenar os tipos de variﾃ｡veis, caso ainda nﾃ｣o exista
        if 'variable_types' not in st.session_state:
            st.session_state.variable_types = {}

        variable_types = st.session_state.variable_types
        st.session_state.numeric_types = {}  # Dicionﾃ｡rio para armazenar os tipos numﾃｩricos

        # Percorrer cada coluna selecionada para definir os tipos de variﾃ｡veis
        for col in selected_columns:
            # Criar um seletor para definir se a variﾃ｡vel ﾃｩ Numﾃｩrica, Categﾃｳrica ou Data
            var_type = st.selectbox(
                f"Tipo de variﾃ｡vel para {col}",
                ["Numﾃｩrica", "Categﾃｳrica", "Data"],
                index=0 if pd.api.types.is_numeric_dtype(st.session_state.filtered_data[col]) else 1,
                key=f"var_{col}"  # Cada seletor tem uma chave ﾃｺnica para evitar conflitos
            )
            variable_types[col] = var_type  # Guardar o tipo selecionado

            # Se a variﾃ｡vel for numﾃｩrica, permitir configurar o tipo especﾃｭfico
            if var_type == "Numﾃｩrica":
                num_type = st.selectbox(
                    f"Tipo numﾃｩrico para {col}",
                    ["Int", "Float", "Complex", "Dec", "Frac", "Bool"],
                    index=0 if pd.api.types.is_integer_dtype(st.session_state.filtered_data[col]) else 1,
                    key=f"num_{col}"  # Chave ﾃｺnica para o seletor de tipo numﾃｩrico
                )
                st.session_state.numeric_types[col] = num_type  # Guardar o tipo numﾃｩrico no estado global

                # Discretizaﾃｧﾃ｣o da variﾃ｡vel (conversﾃ｣o para categorias)
                # Verifica primeiro se a coluna jﾃ｡ foi discretizada
                if col not in st.session_state.filtered_data.columns or pd.api.types.is_numeric_dtype(st.session_state.filtered_data[col]):
                    if st.checkbox(f"Discretizar {col}?", key=f"discretize_{col}"):
                        discretize_column(col)  # Aplica a funﾃｧﾃ｣o de discretizaﾃｧﾃ｣o
                else:
                    st.write(f"Coluna {col} jﾃ｡ foi discretizada.")  # Informaﾃｧﾃ｣o para o utilizador

        # Atualizar o estado global com os tipos de variﾃ｡veis definidos
        st.session_state.variable_types = variable_types

    # Criar uma cﾃｳpia dos dados filtrados para manter alteraﾃｧﾃｵes recentes
    st.session_state.filtered_data = st.session_state.filtered_data.copy()

    # -------------------------------------
    # 東 Navegaﾃｧﾃ｣o entre etapas
    # -------------------------------------

    col1, col2 = st.columns(2)  # Criar duas colunas para os botﾃｵes "Voltar" e "Prﾃｳxima etapa"

    # Botﾃ｣o para voltar ﾃ etapa anterior
    with col1:
        if st.button("Voltar"):
            # Apagar estados salvos explicitamente para evitar conflitos
            keys_to_reset = [
                'filtered_data', 'selected_columns', 'variable_types',
                'numeric_types', 'treatment_state'
            ]
            for key in keys_to_reset:
                st.session_state.pop(key, None)  # Remove do estado se existir

            # Restaurar os dados originais
            st.session_state.data = st.session_state.data.copy()

            # Voltar para a etapa de upload do ficheiro
            st.session_state.step = 'file_upload'
            st.rerun()  # Recarregar a aplicaﾃｧﾃ｣o para refletir as mudanﾃｧas

    # Botﾃ｣o para avanﾃｧar para a prﾃｳxima etapa
    with col2:
        if st.button("Prﾃｳxima etapa"):
            apply_numeric_types()  # Aplicar os tipos numﾃｩricos definidos pelo utilizador
            st.session_state.step = 'missing_values'  # Atualizar o estado para a etapa seguinte
            st.rerun()  # Recarregar a aplicaﾃｧﾃ｣o para refletir as alteraﾃｧﾃｵes


# -------------------------------------
# 東 Funﾃｧﾃ｣o para Aplicar Tipos Numﾃｩricos ﾃs Colunas Filtradas
# -------------------------------------

def apply_numeric_types():
    """Aplica os tipos numﾃｩricos definidos pelo utilizador ﾃs colunas filtradas no dataset."""
    
    # Percorre todas as colunas que tﾃｪm tipos numﾃｩricos definidos pelo utilizador
    for col, num_type in st.session_state.numeric_types.items():
        # Verifica se a coluna ainda existe no conjunto de dados filtrado
        if col in st.session_state.filtered_data.columns:
            # Converte a coluna para o tipo numﾃｩrico selecionado
            st.session_state.filtered_data[col] = convert_numeric_type(st.session_state.filtered_data[col], num_type)

# -------------------------------------
# 東 Funﾃｧﾃ｣o para Conversﾃ｣o de Tipos de Dados Numﾃｩricos
# -------------------------------------

def convert_numeric_type(series, num_type):
    """
    Converte uma sﾃｩrie de dados para o tipo numﾃｩrico especificado.
    
    Parﾃ｢metros:
    - series: pd.Series -> Coluna do DataFrame a ser convertida.
    - num_type: str -> Tipo numﾃｩrico desejado ("Int", "Float", "Complex", "Dec", "Frac", "Bool", "Date", "Duration").

    Retorna:
    - pd.Series convertida para o tipo especificado ou a mesma sﾃｩrie original caso ocorra um erro.
    """
    
    try:
        # Conversﾃ｣o para nﾃｺmero inteiro (Int64)
        if num_type == "Int":
            return pd.to_numeric(series, errors='coerce').astype('Int64')  # Mantﾃｩm valores nulos compatﾃｭveis com Pandas

        # Conversﾃ｣o para nﾃｺmero decimal (Float)
        elif num_type == "Float":
            return pd.to_numeric(series, errors='coerce').astype(float)

        # Conversﾃ｣o para nﾃｺmero complexo
        elif num_type == "Complex":
            return pd.to_numeric(series, errors='coerce').apply(lambda x: complex(x) if pd.notnull(x) else np.nan)

        # Conversﾃ｣o para Decimal (melhor precisﾃ｣o para cﾃ｡lculos financeiros)
        elif num_type == "Dec":
            return series.apply(lambda x: Decimal(x) if pd.notnull(x) else np.nan)

        # Conversﾃ｣o para Fraﾃｧﾃ｣o (representaﾃｧﾃ｣o matemﾃ｡tica exata)
        elif num_type == "Frac":
            return series.apply(lambda x: Fraction(x) if pd.notnull(x) else np.nan)

        # Conversﾃ｣o para Booleano (True/False)
        elif num_type == "Bool":
            return series.apply(lambda x: str(x).strip().lower() in ['true', '1'])

        # Conversﾃ｣o para Data/Hora
        elif num_type == "Date":
            return pd.to_datetime(series, errors='coerce')

        # Conversﾃ｣o para Duraﾃｧﾃ｣o/Intervalo de Tempo
        elif num_type == "Duration":
            return pd.to_timedelta(series, errors='coerce')

        # Se o tipo especificado nﾃ｣o estiver listado, retorna a sﾃｩrie original sem alteraﾃｧﾃｵes
        else:
            return series

    except Exception as e:
        # Exibe um erro no Streamlit caso ocorra um problema na conversﾃ｣o
        st.error(f"Erro ao converter coluna {series.name} para tipo {num_type}: {e}")


# -------------------------------------
# 東 Funﾃｧﾃ｣o para Discretizar uma Coluna Numﾃｩrica
# -------------------------------------

def discretize_column(col):
    """Permite ao utilizador discretizar uma coluna numﾃｩrica, transformando-a em categorias definidas manualmente."""

    # -------------------------------------
    # 東 Seﾃｧﾃ｣o de Ajuda - Explicaﾃｧﾃ｣o sobre Discretizaﾃｧﾃ｣o
    # -------------------------------------
    
    # Explicaﾃｧﾃ｣o interativa sobre como definir bins (intervalos) e labels (categorias)
    with st.expander("Como preencher os bins e labels?"):
        st.write("**Bins:** Intervalos numﾃｩricos para discretizaﾃｧﾃ｣o.")
        st.write("**Labels:** Nomeiam os intervalos.")
        st.write("**Exemplo:**")
        st.write("- **Bins:** -2,1,2,6,inf")
        st.write("- **Labels:** Baixo, Mﾃｩdio, Alto, Muito Alto")

    # -------------------------------------
    # 東 Diagnﾃｳstico Inicial Antes da Discretizaﾃｧﾃ｣o
    # -------------------------------------

    st.write("### Diagnﾃｳstico antes da discretizaﾃｧﾃ｣o:")
    st.write(f"- **Mﾃｭnimo:** {st.session_state.filtered_data[col].min()}")  # Valor mﾃｭnimo da coluna
    st.write(f"- **Mﾃ｡ximo:** {st.session_state.filtered_data[col].max()}")  # Valor mﾃ｡ximo da coluna
    st.write(f"- **Mﾃｩdia:** {st.session_state.filtered_data[col].mean():.2f}")  # Mﾃｩdia da coluna
    st.write(f"- **Mediana:** {st.session_state.filtered_data[col].median():.2f}")  # Mediana da coluna
    st.write(f"- **Valores ausentes antes:** {st.session_state.filtered_data[col].isna().sum()}")  # Contagem de valores nulos

    # -------------------------------------
    # 東 Entrada de Dados do Utilizador (Bins e Labels)
    # -------------------------------------

    # Caixa de texto para o utilizador inserir os bins (intervalos numﾃｩricos)
    bins_input = st.text_input(
        f"Digite os bins para {col} (separados por vﾃｭrgulas)",
        value="-2,1,2,6,inf", key=f"bins_{col}"
    )

    # Caixa de texto para o utilizador inserir os labels (nomes das categorias correspondentes aos bins)
    labels_input = st.text_input(
        f"Digite os labels para {col} (separados por vﾃｭrgulas)",
        value="Baixo,Mﾃｩdio,Alto,Muito Alto", key=f"labels_{col}"
    )

    # -------------------------------------
    # 東 Aplicaﾃｧﾃ｣o da Discretizaﾃｧﾃ｣o Apﾃｳs Confirmaﾃｧﾃ｣o
    # -------------------------------------

    # Se o utilizador clicar no botﾃ｣o, iniciar a conversﾃ｣o
    if st.button(f"Confirmar Discretizaﾃｧﾃ｣o para {col}", key=f"confirm_{col}"):

        # Verificar se o utilizador preencheu os bins e labels corretamente
        if bins_input and labels_input:
            try:
                # Converter a string de bins para uma lista de valores numﾃｩricos (float)
                bins = list(map(float, bins_input.split(',')))

                # Converter a string de labels para uma lista de nomes de categorias
                labels = labels_input.split(',')

                # -------------------------------------
                # 東 Validaﾃｧﾃ｣o de Dados Antes da Conversﾃ｣o
                # -------------------------------------

                # O nﾃｺmero de labels deve ser igual ao nﾃｺmero de bins menos um
                if len(labels) != len(bins) - 1:
                    st.error(f"O nﾃｺmero de labels deve ser igual ao nﾃｺmero de bins menos um para a coluna {col}.")

                else:
                    # Converter a coluna para tipo numﾃｩrico para evitar erros
                    st.session_state.filtered_data[col] = pd.to_numeric(
                        st.session_state.filtered_data[col], errors='coerce'
                    )

                    # Preencher valores ausentes com a mediana da coluna
                    median_value = st.session_state.filtered_data[col].median()
                    st.session_state.filtered_data[col].fillna(median_value, inplace=True)

                    # Diagnﾃｳstico apﾃｳs preenchimento de valores ausentes
                    st.write(f"Valores ausentes apﾃｳs preenchimento: {st.session_state.filtered_data[col].isna().sum()}")

                    # -------------------------------------
                    # 東 Aplicaﾃｧﾃ｣o da Discretizaﾃｧﾃ｣o
                    # -------------------------------------

                    # Criar categorias com base nos bins e labels definidos pelo utilizador
                    categorized = pd.cut(
                        st.session_state.filtered_data[col],  # Coluna de dados a ser discretizada
                        bins=bins,  # Intervalos definidos
                        labels=labels,  # Nomes das categorias correspondentes
                        include_lowest=True  # Inclui o menor valor nos intervalos
                    )

                    # Converter para tipo categﾃｳrico
                    categorized = categorized.astype('category')

                    # Adicionar uma categoria extra para valores fora do intervalo definido
                    categorized = categorized.cat.add_categories(["Fora do Intervalo"])
                    categorized = categorized.fillna("Fora do Intervalo")  # Substituir valores nﾃ｣o categorizados

                    # -------------------------------------
                    # 東 Atualizaﾃｧﾃ｣o do Estado Global e Diagnﾃｳstico Final
                    # -------------------------------------

                    # Salvar a coluna discretizada no dataset filtrado
                    st.session_state.filtered_data[col] = categorized

                    # Criar uma nova cﾃｳpia do dataset para garantir a consistﾃｪncia dos dados
                    st.session_state.filtered_data = st.session_state.filtered_data.copy()

                    # Mensagem de sucesso
                    st.success(f"Coluna {col} discretizada com sucesso!")

                    # Exibir o tipo de dados final da coluna
                    st.write(st.session_state.filtered_data[col].dtype)

                    # Exibir as categorias ﾃｺnicas geradas
                    st.write(st.session_state.filtered_data[col].unique())

                    # Exibir uma prﾃｩ-visualizaﾃｧﾃ｣o dos dados apﾃｳs a discretizaﾃｧﾃ｣o
                    st.write("Prﾃｩ-visualizaﾃｧﾃ｣o dos dados apﾃｳs discretizaﾃｧﾃ｣o:")
                    st.dataframe(fix_dataframe_types(st.session_state.filtered_data.head()))

            except ValueError as e:
                # Mensagem de erro caso a conversﾃ｣o falhe
                st.error(f"Erro ao discretizar {col}: {e}")



##############################################
# -------------------------------------
# 東 FUNﾃﾃグ DE TRATAMENTO DE VALORES OMISSOS (MISSING VALUES)
# -------------------------------------

# -------------------------------------
# 東 Funﾃｧﾃ｣o para destacar valores ausentes no DataFrame
# -------------------------------------

def highlight_missing():
    """Aplica um estilo ao DataFrame, destacando cﾃｩlulas com valores ausentes em amarelo."""

    # Funﾃｧﾃ｣o interna que aplica a cor amarela ﾃs cﾃｩlulas com valores nulos (NaN)
    def highlight_na(s):
        return ['background-color: yellow' if pd.isnull(v) else '' for v in s]

    # Aplica o estilo ao DataFrame filtrado e retorna o objeto Styler
    return st.session_state.filtered_data.style.apply(highlight_na, subset=st.session_state.filtered_data.columns)

# -------------------------------------
# 東 Funﾃｧﾃ｣o para formatar valores na tabela
# -------------------------------------

def format_table():
    """Formata os valores do DataFrame para exibiﾃｧﾃ｣o, ajustando casas decimais e representaﾃｧﾃｵes de NaN."""
    
    # Criar uma cﾃｳpia do DataFrame para evitar modificar os dados originais
    formatted_df = st.session_state.filtered_data.copy()

    # Iterar sobre todas as colunas do DataFrame
    for col in formatted_df.columns:
        # Verificar se a coluna contﾃｩm valores numﾃｩricos
        if pd.api.types.is_numeric_dtype(formatted_df[col]):
            # Formatar os valores numﾃｩricos para exibiﾃｧﾃ｣o com 2 casas decimais
            formatted_df[col] = formatted_df[col].map(lambda x: f"{x:.2f}" if pd.notnull(x) else 'NaN')

    return formatted_df  # Retorna o DataFrame formatado

# -------------------------------------
# 東 Funﾃｧﾃ｣o para exibir a prﾃｩ-visualizaﾃｧﾃ｣o dos dados com tipos de variﾃ｡veis
# -------------------------------------

def show_preview_with_types(variable_types):
    """Exibe os dados com uma prﾃｩ-visualizaﾃｧﾃ｣o dos tipos de variﾃ｡veis identificados."""

    # Tﾃｭtulo da seﾃｧﾃ｣o
    st.subheader("Prﾃｩ-visualizaﾃｧﾃ｣o dos dados com tipos de variﾃ｡veis")

    # Exibir os tipos de variﾃ｡veis definidos pelo utilizador
    st.write("Tipos de variﾃ｡veis:")
    st.write(variable_types)

    # Formatar os dados antes da exibiﾃｧﾃ｣o
    formatted_df = format_table()

    # Aplicar destaque para valores ausentes e corrigir tipos de dados antes de exibir
    st.dataframe(fix_dataframe_types(highlight_missing(formatted_df)))


# -------------------------------------
# 東 Funﾃｧﾃ｣o para Aplicar Tratamento de Valores Ausentes
# -------------------------------------

def apply_missing_value_treatment(column, method, constant_value=None):
    """Aplica um tratamento especﾃｭfico para valores ausentes numa coluna selecionada do dataset."""

    # Usa diretamente os dados filtrados armazenados no estado global
    data = st.session_state.filtered_data

    # Verifica se a coluna ﾃｩ numﾃｩrica
    if pd.api.types.is_numeric_dtype(data[column]):
        # Substituir valores ausentes pela mﾃｩdia da coluna
        if method == "Mﾃｩdia":
            data[column].fillna(data[column].mean(), inplace=True)

        # Substituir valores ausentes pela mediana da coluna
        elif method == "Mediana":
            data[column].fillna(data[column].median(), inplace=True)

        # Substituir valores ausentes pela moda (valor mais frequente) da coluna
        elif method == "Moda":
            data[column].fillna(data[column].mode().iloc[0], inplace=True)

        # Excluir linhas onde hﾃ｡ valores ausentes nesta coluna
        elif method == "Excluir":
            data.dropna(subset=[column], inplace=True)

        # Substituir por um valor constante definido pelo utilizador
        elif method == "Valor constante" and constant_value is not None:
            data[column].fillna(constant_value, inplace=True)

    # Se a coluna for categﾃｳrica (texto, categorias, etc.)
    else:
        # Substituir valores ausentes pela moda (valor mais frequente)
        if method == "Substituir por moda":
            data[column].fillna(data[column].mode().iloc[0], inplace=True)

        # Substituir valores ausentes por um valor fixo definido pelo utilizador
        elif method == "Substituir por valor constante" and constant_value is not None:
            data[column].fillna(constant_value, inplace=True)

        # Nﾃ｣o faz nada (mantﾃｩm os valores ausentes)
        elif method == "Manter valores ausentes":
            pass  

        # Excluir linhas com valores ausentes nesta coluna
        elif method == "Excluir":
            data.dropna(subset=[column], inplace=True)

    # Atualiza os dados processados no estado global
    st.session_state.filtered_data = data

# -------------------------------------
# 東 Funﾃｧﾃ｣o para Selecionar Automaticamente o Mﾃｩtodo de Tratamento de Valores Ausentes
# -------------------------------------

def auto_select_method(column_name):
    """Seleciona automaticamente o melhor mﾃｩtodo para tratar valores ausentes numa coluna."""

    # Obtﾃｩm a coluna a partir dos dados filtrados
    column = st.session_state.filtered_data[column_name]

    # Calcula a percentagem de valores ausentes na coluna
    missing_percentage = column.isnull().sum() / len(column)

    # Para colunas numﾃｩricas
    if pd.api.types.is_numeric_dtype(column):
        if missing_percentage > 0.5:
            return "Excluir"  # Se mais de 50% dos valores estﾃ｣o ausentes, sugere excluir a coluna
        else:
            return "Substituir por Mediana"  # Caso contrﾃ｡rio, sugere substituir pela mediana

    # Para colunas categﾃｳricas (texto, categorias)
    else:
        if missing_percentage > 0.5:
            return "Excluir"  # Se mais de 50% dos valores estﾃ｣o ausentes, sugere excluir a coluna
        else:
            return "Substituir por Moda"  # Caso contrﾃ｡rio, sugere substituir pela moda (valor mais frequente)

# -------------------------------------
# 東 Funﾃｧﾃ｣o para Exibir Tabela com Valores Ausentes
# -------------------------------------

def display_missing_values(dataframe):
    """Exibe uma tabela com a contagem de valores ausentes em cada coluna do dataset."""

    # Conta o nﾃｺmero de valores ausentes por coluna
    missing_data = dataframe.isnull().sum()

    # Mantﾃｩm apenas as colunas que possuem valores ausentes
    missing_data = missing_data[missing_data > 0]
    
    # Converte para DataFrame para melhor visualizaﾃｧﾃ｣o
    missing_data = missing_data.reset_index()
    missing_data.columns = ['Coluna', 'Valores Ausentes']

    # Se houver valores ausentes, exibir a tabela
    if not missing_data.empty:
        st.write("Tabela de valores ausentes:")
        st.dataframe(fix_dataframe_types(missing_data))  # Aplica correﾃｧﾃｵes de tipo antes de exibir
    else:
        st.write("Nﾃ｣o hﾃ｡ valores ausentes.")  # Mensagem caso nﾃ｣o existam valores em falta

# -------------------------------------
# 東 FUNﾃﾃグ PARA MOSTRAR E TRATAR VALORES AUSENTES
# -------------------------------------

def handle_missing_values():
    """Gerencia o tratamento de valores ausentes no dataset carregado."""

    # Exibe o tﾃｭtulo da seﾃｧﾃ｣o no Streamlit
    st.subheader("Tratamento de Valores Ausentes")

    # Obtﾃｩm os dados filtrados armazenados no estado da sessﾃ｣o
    filtered_data = st.session_state.get('filtered_data', None)

    # -------------------------------------
    # 東 Verificaﾃｧﾃ｣o Inicial dos Dados
    # -------------------------------------

    # Verifica se hﾃ｡ dados carregados e nﾃ｣o estﾃ｣o vazios
    if filtered_data is not None and not filtered_data.empty:

        # -------------------------------------
        # 東 Funﾃｧﾃ｣o Interna para Exibir Valores Ausentes
        # -------------------------------------

        def display_missing_values(df):
            """Gera uma tabela resumida com a contagem de valores ausentes por coluna."""

            # Conta a quantidade de valores ausentes em cada coluna
            missing_data = df.isnull().sum()

            # Mantﾃｩm apenas as colunas que possuem valores ausentes
            missing_data = missing_data[missing_data > 0]

            # Exibe os valores ausentes caso existam
            if not missing_data.empty:
                st.write("Resumo dos Valores Ausentes:")
                st.dataframe(fix_dataframe_types(missing_data.rename("Total de Valores Ausentes")))
            else:
                st.success("Nﾃ｣o hﾃ｡ valores ausentes nos dados.")  # Exibe uma mensagem caso nﾃ｣o haja valores ausentes

        # Exibir o resumo dos valores ausentes no dataset
        display_missing_values(filtered_data)

        # -------------------------------------
        # 東 Configuraﾃｧﾃ｣o das Opﾃｧﾃｵes de Tratamento de Valores Ausentes
        # -------------------------------------

        # Verifica se existem valores ausentes em qualquer coluna
        has_missing_values = filtered_data.isnull().any().any()

        if has_missing_values:
            # Inicializar dicionﾃ｡rio de tratamento no estado global, caso ainda nﾃ｣o exista
            if 'treatment_state' not in st.session_state:
                st.session_state.treatment_state = {
                    col: {"method": None, "constant": None}
                    for col in filtered_data.columns
                }

            # Percorre cada coluna que possui valores ausentes para exibir opﾃｧﾃｵes de tratamento
            for col in filtered_data.columns:
                if filtered_data[col].isnull().sum() > 0:
                    col_state = st.session_state.treatment_state.get(col, {"method": None, "constant": None})
                    is_numeric = pd.api.types.is_numeric_dtype(filtered_data[col])

                    # -------------------------------------
                    # 東 Tratamento de Valores Ausentes em Colunas Numﾃｩricas
                    # -------------------------------------

                    if is_numeric:
                        # Opﾃｧﾃｵes disponﾃｭveis para tratamento de valores ausentes em variﾃ｡veis numﾃｩricas
                        options = ["Substituir por Mﾃｩdia", "Substituir por Mediana", "Substituir por Moda", 
                                   "Substituir por Valor Constante", "Excluir", "Manter Valores Ausentes"]
                        
                        # Seletor para escolher o mﾃｩtodo de tratamento
                        missing_value_method = st.selectbox(
                            f"Mﾃｩtodo para tratar valores ausentes em {col}",
                            options,
                            index=options.index(col_state["method"]) if col_state["method"] in options else 0,
                            key=f"missing_value_{col}"
                        )

                        # Definir valor constante caso o utilizador escolha essa opﾃｧﾃ｣o
                        constant_value = None
                        if missing_value_method == "Substituir por Valor Constante":
                            constant_value = st.text_input(
                                f"Digite o valor constante para {col}:",
                                value=col_state["constant"] if col_state["constant"] else '',
                                key=f"constant_{col}"
                            )

                    # -------------------------------------
                    # 東 Tratamento de Valores Ausentes em Colunas Categﾃｳricas
                    # -------------------------------------

                    else:
                        # Opﾃｧﾃｵes disponﾃｭveis para colunas categﾃｳricas
                        options = ["Substituir por Moda", "Substituir por Valor Constante", "Manter Valores Ausentes", "Excluir"]
                        
                        # Seletor para escolher o mﾃｩtodo de tratamento
                        missing_value_method = st.selectbox(
                            f"Mﾃｩtodo para tratar valores ausentes em {col}",
                            options,
                            index=options.index(col_state["method"]) if col_state["method"] in options else 0,
                            key=f"cat_missing_value_{col}"
                        )

                        # Definir valor constante caso o utilizador escolha essa opﾃｧﾃ｣o
                        constant_value = None
                        if missing_value_method == "Substituir por Valor Constante":
                            constant_value = st.text_input(
                                f"Digite o valor constante para {col}:",
                                value=col_state["constant"] if col_state["constant"] else '',
                                key=f"cat_constant_{col}"
                            )

                    # Atualizar o estado global com as escolhas do utilizador para essa coluna
                    st.session_state.treatment_state[col] = {"method": missing_value_method, "constant": constant_value}

            # -------------------------------------
            # 東 Aplicaﾃｧﾃ｣o dos Tratamentos Escolhidos
            # -------------------------------------

            if st.button("Aplicar tratamentos"):
                for col, treatment in st.session_state.treatment_state.items():
                    method = treatment["method"]
                    constant_value = treatment["constant"]

                    # Aplicar o mﾃｩtodo selecionado para tratamento dos valores ausentes
                    if method == "Substituir por Mﾃｩdia":
                        filtered_data[col].fillna(filtered_data[col].mean(), inplace=True)
                    elif method == "Substituir por Mediana":
                        filtered_data[col].fillna(filtered_data[col].median(), inplace=True)
                    elif method == "Substituir por Moda":
                        filtered_data[col].fillna(filtered_data[col].mode().iloc[0], inplace=True)
                    elif method == "Substituir por Valor Constante" and constant_value is not None:
                        filtered_data[col].fillna(constant_value, inplace=True)
                    elif method == "Excluir":
                        filtered_data.dropna(subset=[col], inplace=True)

                # Atualizar os dados processados no estado global
                st.session_state.data = filtered_data.copy()

                # Mensagem de sucesso
                st.success("Tratamentos aplicados com sucesso!")

        # -------------------------------------
        # 東 Navegaﾃｧﾃ｣o entre Etapas
        # -------------------------------------

        col1, col2 = st.columns(2)

        # Botﾃ｣o para voltar ﾃ etapa anterior
        with col1:
            if st.button("Voltar"):
                st.session_state.step = 'data_preview'
                st.rerun()

        # Botﾃ｣o para avanﾃｧar para a prﾃｳxima etapa
        with col2:
            if st.button("Prﾃｳxima etapa"):
                st.session_state.step = 'outlier_detection'
                st.rerun()

    else:
        # Caso nﾃ｣o haja dados disponﾃｭveis, exibir uma mensagem de erro
        st.error("Nenhum dado disponﾃｭvel para tratamento de valores ausentes.")


##############################################
# -------------------------------------
# 東 FUNﾃﾃグ DE TRATAMENTO DE OUTLIERS (VALORES EXTREMOS)
# -------------------------------------

# -------------------------------------
# 東 Funﾃｧﾃ｣o para Detetar e Calcular Informaﾃｧﾃｵes sobre Outliers
# -------------------------------------

@st.cache_data  # Usa cache para evitar recﾃ｡lculo desnecessﾃ｡rio ao interagir com a aplicaﾃｧﾃ｣o
def calculate_outliers(columns, data):
    """
    Identifica e calcula estatﾃｭsticas sobre outliers em variﾃ｡veis numﾃｩricas.

    Parﾃ｢metros:
    - columns: lista com os nomes das colunas a serem analisadas.
    - data: DataFrame contendo os dados.

    Retorna:
    - variables_with_outliers: Lista com as variﾃ｡veis que possuem outliers.
    - outlier_summary: Lista de dicionﾃ｡rios com informaﾃｧﾃｵes detalhadas sobre os outliers identificados.
    """

    # Lista para armazenar os nomes das variﾃ｡veis que contﾃｪm outliers
    variables_with_outliers = []

    # Lista para armazenar o resumo estatﾃｭstico dos outliers encontrados
    outlier_summary = []

    # Percorre todas as colunas selecionadas para anﾃ｡lise de outliers
    for col in columns:
        # Verifica se a coluna contﾃｩm dados numﾃｩricos antes de continuar a anﾃ｡lise
        if pd.api.types.is_numeric_dtype(data[col]):

            # -------------------------------------
            # 東 Cﾃ｡lculo do Intervalo Interquartil (IQR)
            # -------------------------------------

            # Primeiro quartil (Q1) - 25% dos dados estﾃ｣o abaixo deste valor
            Q1 = data[col].quantile(0.25)

            # Terceiro quartil (Q3) - 75% dos dados estﾃ｣o abaixo deste valor
            Q3 = data[col].quantile(0.75)

            # Intervalo Interquartil (IQR) - Diferenﾃｧa entre Q3 e Q1
            IQR = Q3 - Q1

            # Definiﾃｧﾃ｣o dos limites para deteﾃｧﾃ｣o de outliers
            lower_bound = Q1 - 1.5 * IQR  # Limite inferior
            upper_bound = Q3 + 1.5 * IQR  # Limite superior

            # -------------------------------------
            # 東 Identificaﾃｧﾃ｣o de Outliers
            # -------------------------------------

            # Contagem de outliers, ou seja, valores que estﾃ｣o abaixo do limite inferior ou acima do superior
            num_outliers = len(data[(data[col] < lower_bound) | (data[col] > upper_bound)])

            # Se forem encontrados outliers na coluna, armazenar os resultados
            if num_outliers > 0:
                # Calcular a percentagem de outliers em relaﾃｧﾃ｣o ao total de dados na variﾃ｡vel
                percentage_outliers = (num_outliers / len(data[col])) * 100

                # Adicionar o nome da variﾃ｡vel ﾃ lista de variﾃ｡veis com outliers
                variables_with_outliers.append(col)

                # Criar um dicionﾃ｡rio com o resumo estatﾃｭstico dos outliers na variﾃ｡vel analisada
                outlier_summary.append({
                    "Variﾃ｡vel": col,
                    "Total de Outliers": num_outliers,
                    "Percentagem de Outliers (%)": round(percentage_outliers, 2)
                })

    # Retorna a lista de variﾃ｡veis que possuem outliers e o resumo estatﾃｭstico
    return variables_with_outliers, outlier_summary


# Interface de detecﾃｧﾃ｣o e tratamento de outliers
# -------------------------------------
# 東 FUNﾃﾃグ DE DETEﾃﾃグ E TRATAMENTO DE OUTLIERS
# -------------------------------------

def outlier_detection():
    """Realiza a deteﾃｧﾃ｣o e o tratamento de outliers (valores extremos) em variﾃ｡veis numﾃｩricas do dataset."""

    # Exibir o tﾃｭtulo da seﾃｧﾃ｣o no Streamlit
    st.subheader("Deteﾃｧﾃ｣o de Outliers")

    # -------------------------------------
    # 東 Armazenamento dos Dados Originais
    # -------------------------------------

    # Se for a primeira execuﾃｧﾃ｣o, armazenar uma cﾃｳpia dos dados originais
    if 'original_data' not in st.session_state:
        st.session_state.original_data = st.session_state.data.copy()

    # -------------------------------------
    # 東 Boxplot Inicial (Visualizaﾃｧﾃ｣o dos Dados Antes do Tratamento)
    # -------------------------------------

    st.write("### Boxplot Inicial (Dados Originais)")
    fig, ax = plt.subplots(figsize=(12, 6))
    st.session_state.original_data.boxplot(ax=ax)  # Criar boxplot para visualizar outliers
    plt.xticks(rotation=45)  # Ajustar rotaﾃｧﾃ｣o dos rﾃｳtulos do eixo X
    st.pyplot(fig)  # Exibir grﾃ｡fico no Streamlit

    # -------------------------------------
    # 東 Inicializar Estados Globais Necessﾃ｡rios
    # -------------------------------------

    # Armazena colunas que jﾃ｡ passaram por tratamento
    if 'treated_columns' not in st.session_state:
        st.session_state.treated_columns = []

    # Armazena detalhes sobre os outliers identificados
    if 'outlier_details' not in st.session_state:
        st.session_state.outlier_details = {}

    # Armazena os limites iniciais dos outliers (antes do tratamento)
    if 'initial_limits' not in st.session_state:
        st.session_state.initial_limits = {}

    # Lista de colunas que possuem outliers
    if 'columns_with_outliers' not in st.session_state:
        st.session_state.columns_with_outliers = []

    # Estado global para armazenar as decisﾃｵes do utilizador sobre tratamento de outliers
    if 'outlier_treatment_state' not in st.session_state:
        st.session_state.outlier_treatment_state = {}

    # Flag para indicar se todos os outliers foram tratados
    if 'all_outliers_treated' not in st.session_state:
        st.session_state.all_outliers_treated = False

    # -------------------------------------
    # 東 Verificaﾃｧﾃ｣o da Disponibilidade dos Dados
    # -------------------------------------

    if 'data' not in st.session_state or st.session_state.data is None:
        st.error("Os dados nﾃ｣o estﾃ｣o carregados! Volte para a etapa anterior.")
        return

    # -------------------------------------
    # 東 Identificaﾃｧﾃ｣o de Outliers
    # -------------------------------------

    # Selecionar apenas as colunas numﾃｩricas do dataset
    numeric_columns = list(st.session_state.data.select_dtypes(include=[np.number]).columns)

    # Lista para armazenar resumo dos outliers
    outlier_summary = []

    # Percorrer todas as colunas numﾃｩricas para calcular limites e identificar outliers
    for col in numeric_columns:

        # Ignorar colunas que jﾃ｡ foram tratadas
        if col in st.session_state.treated_columns:
            continue

        # Calcular o primeiro quartil (Q1) e o terceiro quartil (Q3)
        Q1 = st.session_state.data[col].quantile(0.25)
        Q3 = st.session_state.data[col].quantile(0.75)

        # Calcular o intervalo interquartil (IQR)
        IQR = Q3 - Q1

        # Definir limites inferior e superior para identificaﾃｧﾃ｣o de outliers
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Contar outliers normais (fora do intervalo IQR)
        total_outliers = len(st.session_state.data[(st.session_state.data[col] < lower_bound) | 
                                                   (st.session_state.data[col] > upper_bound)])

        # Contar outliers severos (fora do intervalo 3*IQR)
        total_severe_outliers = len(st.session_state.data[(st.session_state.data[col] < (Q1 - 3.0 * IQR)) | 
                                                           (st.session_state.data[col] > (Q3 + 3.0 * IQR))])

        # Se a variﾃ｡vel contiver outliers, armazenar detalhes
        if total_outliers > 0:
            st.session_state.initial_limits[col] = {
                "lower_bound": lower_bound,
                "upper_bound": upper_bound,
            }

            st.session_state.outlier_details[col] = {
                "total_outliers": total_outliers,
                "total_severe_outliers": total_severe_outliers,
                "skewness": st.session_state.data[col].skew()  # Assimetria da distribuiﾃｧﾃ｣o
            }

            # Adicionar ao resumo estatﾃｭstico
            outlier_summary.append({
                "Nome variﾃ｡vel": col,
                "Total de outliers": total_outliers,
                "Total de outliers severos": total_severe_outliers
            })

            # Adicionar ﾃ lista de colunas com outliers
            if col not in st.session_state.columns_with_outliers:
                st.session_state.columns_with_outliers.append(col)

    # Salvar o resumo inicial no estado global
    st.session_state.initial_outlier_summary = outlier_summary

    # -------------------------------------
    # 東 Verificar se Restam Outliers para Tratar
    # -------------------------------------

    remaining_outliers = [col for col in st.session_state.columns_with_outliers 
                          if col not in st.session_state.treated_columns]

    if not remaining_outliers:
        if not outlier_summary and not st.session_state.columns_with_outliers:
            st.success("Nenhum outlier detetado nas variﾃ｡veis numﾃｩricas!")
        else:
            st.success("Todos os outliers detetados foram tratados!")
    else:
        st.write("Resumo dos Outliers:")
        st.dataframe(fix_dataframe_types(pd.DataFrame(outlier_summary)))

    # -------------------------------------
    # 東 Exibiﾃｧﾃ｣o e Tratamento de Outliers Restantes
    # -------------------------------------

    for col in remaining_outliers:
        st.write(f"**Diagnﾃｳstico para {col}:**")
        details = st.session_state.outlier_details[col]
        st.write(f"- Total de Registos: {len(st.session_state.data)}")
        st.write(f"- Outliers: {details['total_outliers']} ({(details['total_outliers'] / len(st.session_state.data)):.2%})")
        st.write(f"- Outliers Severos: {details['total_severe_outliers']} ({(details['total_severe_outliers'] / len(st.session_state.data)):.2%})")
        st.write(f"- Assimetria (Skewness): {details['skewness']:.2f}")

        # Sugestﾃ｣o automﾃ｡tica de mﾃｩtodo de tratamento
        if col not in st.session_state.outlier_treatment_state:
            suggested_method = auto_select_outlier_treatment(
                col, st.session_state.data, st.session_state.initial_limits[col]["lower_bound"], st.session_state.initial_limits[col]["upper_bound"]
            )
            st.session_state.outlier_treatment_state[col] = suggested_method

        # Seletor de mﾃｩtodo de tratamento
        method = st.selectbox(
            f"Selecione o mﾃｩtodo para tratar outliers em {col}",
            ["Sem Aﾃｧﾃ｣o", "Remover Outliers", "Remover Outliers Severos", "Substituir por Limites", "Substituir por Mﾃｩdia", "Substituir por Mediana"],
            index=["Sem Aﾃｧﾃ｣o", "Remover Outliers", "Remover Outliers Severos", "Substituir por Limites", "Substituir por Mﾃｩdia", "Substituir por Mediana"].index(
                st.session_state.outlier_treatment_state[col]
            ),
            key=f"outlier_method_{col}_{len(st.session_state.treated_columns)}"
        )

        # Botﾃ｣o para aplicar o tratamento selecionado
        if st.button(f"Aplicar tratamento em {col}"):
            apply_outlier_treatment(col, method, st.session_state.initial_limits[col]["lower_bound"], st.session_state.initial_limits[col]["upper_bound"])
            if col not in st.session_state.treated_columns:
                st.session_state.treated_columns.append(col)
            st.rerun()

    # -------------------------------------
    # 東 Boxplot Final Apﾃｳs Tratamento
    # -------------------------------------

    st.write("### Boxplot Apﾃｳs Tratamento")
    fig, ax = plt.subplots(figsize=(12, 6))
    st.session_state.data.boxplot(ax=ax)
    plt.xticks(rotation=45)
    st.pyplot(fig)

    # -------------------------------------
    # 東 Botﾃ｣o para Avanﾃｧar para a Prﾃｳxima Etapa
    # -------------------------------------

    if st.button("Prﾃｳxima etapa"):
        st.session_state.step = 'data_summary'
        st.rerun()

# -------------------------------------
# 東 FUNﾃﾃグ DE SUGESTﾃグ AUTOMﾃゝICA PARA TRATAMENTO DE OUTLIERS
# -------------------------------------

def auto_select_outlier_treatment(col, data, lower_bound, upper_bound):
    """
    Sugere automaticamente o melhor mﾃｩtodo de tratamento de outliers com base na distribuiﾃｧﾃ｣o dos dados.

    Parﾃ｢metros:
    - col: Nome da coluna a ser analisada.
    - data: DataFrame contendo os dados.
    - lower_bound: Limite inferior dos valores considerados normais (IQR 1.5x abaixo do Q1).
    - upper_bound: Limite superior dos valores considerados normais (IQR 1.5x acima do Q3).

    Retorna:
    - Mﾃｩtodo sugerido para tratamento dos outliers.
    """

    # -------------------------------------
    # 東 Cﾃ｡lculo da Proporﾃｧﾃ｣o de Outliers
    # -------------------------------------

    total = len(data)  # Nﾃｺmero total de registos

    # Contar outliers normais (fora do intervalo de 1.5 * IQR)
    total_outliers = len(data[(data[col] < lower_bound) | (data[col] > upper_bound)])

    # Contar outliers severos (fora do intervalo de 3 * IQR)
    total_severe_outliers = len(data[(data[col] < (lower_bound - 1.5 * (upper_bound - lower_bound))) |
                                     (data[col] > (upper_bound + 1.5 * (upper_bound - lower_bound)))])

    # Calcular percentagens
    percentage = total_outliers / total  # Percentagem de outliers normais
    severe_percentage = total_severe_outliers / total  # Percentagem de outliers severos

    # -------------------------------------
    # 東 Verificaﾃｧﾃ｣o da Assimetria dos Dados (Skewness)
    # -------------------------------------

    skewness = data[col].skew()  # Medida de assimetria da distribuiﾃｧﾃ｣o dos dados

    # -------------------------------------
    # 東 Definiﾃｧﾃ｣o das Regras para Sugerir o Melhor Mﾃｩtodo
    # -------------------------------------

    if severe_percentage > 0.10:
        # Se mais de 10% dos valores forem outliers severos, recomenda-se remover apenas os extremos
        return "Remover Outliers Severos"
    elif percentage > 0.20:
        # Se mais de 20% dos valores forem outliers, recomenda-se remover todos os outliers
        return "Remover Outliers"
    elif percentage > 0.05:
        # Se entre 5% e 20% forem outliers, recomenda-se substituﾃｭ-los pelos limites aceitﾃ｡veis
        return "Substituir por Limites"
    else:
        # Se houver menos de 5% de outliers, a escolha entre mﾃｩdia e mediana ﾃｩ baseada na simetria
        if abs(skewness) > 1:
            return "Substituir por Mediana"  # Se houver alta assimetria, usa-se a mediana
        else:
            return "Substituir por Mﾃｩdia"  # Caso contrﾃ｡rio, a mﾃｩdia ﾃｩ uma escolha razoﾃ｡vel

# -------------------------------------
# 東 FUNﾃﾃグ PARA APLICAR TRATAMENTO DE OUTLIERS
# -------------------------------------

def apply_outlier_treatment(col, method, lower_bound, upper_bound):
    """
    Aplica o tratamento de outliers na coluna especificada, conforme o mﾃｩtodo escolhido.

    Parﾃ｢metros:
    - col: Nome da coluna a ser tratada.
    - method: Mﾃｩtodo de tratamento selecionado.
    - lower_bound: Limite inferior considerado aceitﾃ｡vel.
    - upper_bound: Limite superior considerado aceitﾃ｡vel.
    """

    # Obter os dados do estado global
    data = st.session_state.data

    # -------------------------------------
    # 東 Remover Todos os Outliers (Fora do Intervalo 1.5 * IQR)
    # -------------------------------------
    
    if method == "Remover Outliers":
        st.session_state.data = data[
            (data[col] >= lower_bound) & (data[col] <= upper_bound)
        ]
        st.success(f"Todos os outliers removidos na coluna '{col}'.")

    # -------------------------------------
    # 東 Remover Apenas Outliers Severos (Fora do Intervalo 3 * IQR)
    # -------------------------------------

    elif method == "Remover Outliers Severos":
        Q1 = data[col].quantile(0.25)
        Q3 = data[col].quantile(0.75)
        IQR = Q3 - Q1

        # Definir limites mais rigorosos para outliers severos (3 * IQR)
        severe_lower = Q1 - 3.0 * IQR
        severe_upper = Q3 + 3.0 * IQR

        st.session_state.data = data[
            (data[col] >= severe_lower) & (data[col] <= severe_upper)
        ]
        st.success(f"Outliers severos removidos na coluna '{col}'.")

    # -------------------------------------
    # 東 Substituir Outliers pelos Limites Aceitﾃ｡veis
    # -------------------------------------

    elif method == "Substituir por Limites":
        st.session_state.data[col] = data[col].clip(lower_bound, upper_bound)
        st.success(f"Valores substituﾃｭdos pelos limites na coluna '{col}'.")

    # -------------------------------------
    # 東 Substituir Outliers pela Mﾃｩdia da Coluna
    # -------------------------------------

    elif method == "Substituir por Mﾃｩdia":
        mean_value = data[col].mean()
        mask = (data[col] < lower_bound) | (data[col] > upper_bound)
        st.session_state.data.loc[mask, col] = mean_value
        st.success(f"Valores substituﾃｭdos pela mﾃｩdia ({mean_value:.2f}) na coluna '{col}'.")

    # -------------------------------------
    # 東 Substituir Outliers pela Mediana da Coluna
    # -------------------------------------

    elif method == "Substituir por Mediana":
        median_value = data[col].median()
        mask = (data[col] < lower_bound) | (data[col] > upper_bound)
        st.session_state.data.loc[mask, col] = median_value
        st.success(f"Valores substituﾃｭdos pela mediana ({median_value:.2f}) na coluna '{col}'.")


##########################################################
# -------------------------------------
# 東 FUNﾃﾃグ PARA GUARDAR O DATASET APﾃ鉄 O PRﾃ-PROCESSAMENTO
# -------------------------------------

def save_modified_dataset_in_memory():
    """
    Salva o dataset tratado na memﾃｳria (session_state) para uso posterior.
    """

    # Criar uma cﾃｳpia do dataset tratado e armazenﾃ｡-lo no estado da sessﾃ｣o
    st.session_state.data_tratada = st.session_state.data.copy()

    # Exibir uma mensagem de sucesso
    st.success("O dataset tratado foi salvo na memﾃｳria para uso posterior.")

# -------------------------------------
# 東 FUNﾃﾃグ PARA PERMITIR O DOWNLOAD DO DATASET TRATADO
# -------------------------------------

def download_button(df, filename="dataset_tratado.csv"):
    """
    Permite ao utilizador descarregar o dataset tratado em formato CSV.

    Parﾃ｢metros:
    - df: DataFrame tratado a ser disponibilizado para download.
    - filename: Nome do ficheiro CSV a ser descarregado (padrﾃ｣o: "dataset_tratado.csv").
    """

    # Converter o DataFrame para formato CSV (sem ﾃｭndice)
    csv = df.to_csv(index=False)

    # Criar um buffer de memﾃｳria para armazenar o conteﾃｺdo do ficheiro
    buf = io.BytesIO()

    # Escrever o conteﾃｺdo do CSV no buffer e posicionar o cursor no inﾃｭcio
    buf.write(csv.encode())  # Converter para bytes e armazenar no buffer
    buf.seek(0)  # Definir a posiﾃｧﾃ｣o do cursor para o inﾃｭcio do ficheiro

    # Criar um botﾃ｣o de download no Streamlit
    st.download_button(
        label="Baixar Dataset Tratado",  # Texto do botﾃ｣o
        data=buf,  # Ficheiro a ser descarregado
        file_name=filename,  # Nome do ficheiro ao fazer o download
        mime="text/csv"  # Tipo MIME do ficheiro
    )


##########################################################
# -------------------------------------
# 東 CLASSE PARA CRIAR O PDF COM O RESUMO APﾃ鉄 O PRﾃ-PROCESSAMENTO
# -------------------------------------

from fpdf import FPDF
import requests
import tempfile
from datetime import datetime

class CustomPDF(FPDF):
    """
    Classe personalizada para gerar um relatﾃｳrio em PDF com cabeﾃｧalho e rodapﾃｩ customizados.
    """

    def header(self):
        """
        Mﾃｩtodo para gerar o cabeﾃｧalho do PDF, incluindo o logﾃｳtipo da instituiﾃｧﾃ｣o.
        """

        # URL do logﾃｳtipo da instituiﾃｧﾃ｣o
        logo_url = 'https://www.ipleiria.pt/normasgraficas/wp-content/uploads/sites/80/2017/09/estg_v-01.jpg'

        # Fazer o download da imagem
        response = requests.get(logo_url)

        if response.status_code == 200:
            # Criar um ficheiro temporﾃ｡rio para armazenar a imagem baixada
            with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as tmpfile:
                tmpfile.write(response.content)  # Escrever o conteﾃｺdo da imagem no ficheiro temporﾃ｡rio
                tmpfile_path = tmpfile.name  # Obter o caminho do ficheiro

                # Adicionar a imagem no cabeﾃｧalho do PDF
                self.image(tmpfile_path, x=10, y=8, w=20)  # Definir posiﾃｧﾃ｣o e tamanho da imagem
        else:
            # Se a imagem nﾃ｣o for baixada corretamente, exibir mensagem no PDF
            self.set_font('Arial', 'B', 12)
            self.cell(0, 10, "Logo nﾃ｣o disponﾃｭvel", align='C')

        # Definir a fonte do cabeﾃｧalho
        self.set_font('Arial', 'B', 12)

        # Adicionar o tﾃｭtulo da plataforma no cabeﾃｧalho
        self.cell(0, 10, 'MLCase - Plataforma de Machine Learning', align='C', ln=True)

        # Criar um espaﾃｧo entre o cabeﾃｧalho e o conteﾃｺdo
        self.ln(15)

    def footer(self):
        """
        Mﾃｩtodo para gerar o rodapﾃｩ do PDF, incluindo a data e nﾃｺmero da pﾃ｡gina.
        """

        # Definir a posiﾃｧﾃ｣o do rodapﾃｩ a 1.5 cm do final da pﾃ｡gina
        self.set_y(-15)

        # Definir a fonte do rodapﾃｩ
        self.set_font('Arial', 'I', 10)

        # Obter a data atual no formato dia/mﾃｪs/ano
        current_date = datetime.now().strftime('%d/%m/%Y')

        # Adicionar rodapﾃｩ com a data e o nﾃｺmero da pﾃ｡gina
        self.cell(0, 10, f'{current_date} - Pﾃ｡gina {self.page_no()}  |  Autora da Plataforma: Bruna Sousa', align='C')

# -------------------------------------
# 東 FUNﾃﾃグ PARA GERAR O PDF COM O RESUMO DO PRﾃ-PROCESSAMENTO
# -------------------------------------

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from io import BytesIO

def generate_pdf_resumo(dataset, summary_df, missing_data, outlier_summary):
    """
    Gera um relatﾃｳrio em PDF com informaﾃｧﾃｵes estatﾃｭsticas do dataset, valores ausentes, outliers,
    matriz de correlaﾃｧﾃ｣o e boxplot.

    Parﾃ｢metros:
    - dataset: DataFrame original apﾃｳs prﾃｩ-processamento.
    - summary_df: DataFrame com estatﾃｭsticas descritivas do dataset.
    - missing_data: Sﾃｩrie contendo a contagem de valores ausentes por coluna.
    - outlier_summary: Lista contendo o resumo dos outliers identificados.

    Retorna:
    - Um buffer de memﾃｳria contendo o PDF gerado.
    """

    # -------------------------------------
    # 東 Funﾃｧﾃ｣o Auxiliar para Limpar Texto
    # -------------------------------------

    def clean_text(text):
        """Remove caracteres incompatﾃｭveis com a codificaﾃｧﾃ｣o do PDF."""
        if not isinstance(text, str):
            return text
        return text.encode('latin-1', errors='ignore').decode('latin-1')

    # -------------------------------------
    # 東 Inicializaﾃｧﾃ｣o do PDF
    # -------------------------------------

    pdf = CustomPDF(format='A4')
    pdf.set_auto_page_break(auto=True, margin=15)
    pdf.add_page()
    pdf.set_font("Arial", size=8)

    # -------------------------------------
    # 東 Tﾃｭtulo do Relatﾃｳrio
    # -------------------------------------

    pdf.set_font("Arial", style="B", size=12)
    pdf.cell(0, 10, txt=clean_text("Relatﾃｳrio Resumo dos Dados"), ln=True, align="C")
    pdf.ln(5)

    # -------------------------------------
    # 東 Estatﾃｭsticas Descritivas Simplificadas
    # -------------------------------------

    pdf.set_font("Arial", style="B", size=12)
    pdf.cell(0, 10, txt=clean_text("Estatﾃｭsticas Descritivas"), ln=True)
    pdf.set_font("Arial", size=8)

    # Criar DataFrame simplificado com estatﾃｭsticas principais
    summary_simplified = pd.DataFrame({
        'Coluna': dataset.columns,
        'Tipo de Dados': dataset.dtypes,
        'Count': dataset.count(),
        'Top': dataset.mode().iloc[0],  # Valor mais frequente (moda)
    })

    # Inicializar colunas estatﾃｭsticas apenas para colunas numﾃｩricas
    summary_simplified['std'] = None
    summary_simplified['min'] = None
    summary_simplified['max'] = None
    summary_simplified['Mﾃｩdia'] = None

    numeric_columns = dataset.select_dtypes(include=['float64', 'int64']).columns
    summary_simplified.loc[summary_simplified['Coluna'].isin(numeric_columns), 'Mﾃｩdia'] = dataset[numeric_columns].mean()
    summary_simplified.loc[summary_simplified['Coluna'].isin(numeric_columns), 'std'] = dataset[numeric_columns].std()
    summary_simplified.loc[summary_simplified['Coluna'].isin(numeric_columns), 'min'] = dataset[numeric_columns].min()
    summary_simplified.loc[summary_simplified['Coluna'].isin(numeric_columns), 'max'] = dataset[numeric_columns].max()

    # Formatar valores numﾃｩricos para 4 casas decimais
    for col in ['Mﾃｩdia', 'std', 'min', 'max']:
        summary_simplified[col] = summary_simplified[col].apply(lambda x: f"{x:.4f}" if isinstance(x, (int, float)) else x)

    # Substituir 'nan' por vazio
    summary_simplified = summary_simplified.fillna('')

    # -------------------------------------
    # 東 Adicionar Tabela das Estatﾃｭsticas ao PDF
    # -------------------------------------

    pdf.set_fill_color(144, 238, 144)  # Cor de fundo do cabeﾃｧalho
    col_widths = [pdf.get_string_width(col) for col in summary_simplified.columns]
    max_width = 180

    total_width = sum(col_widths)
    scale_factor = max_width / total_width
    col_widths = [width * scale_factor for width in col_widths]

    for i, col in enumerate(summary_simplified.columns):
        pdf.cell(col_widths[i], 10, clean_text(col), 1, 0, 'C', True)
    pdf.ln()

    for i, row in summary_simplified.iterrows():
        for j, cell in enumerate(row):
            pdf.cell(col_widths[j], 8, clean_text(str(cell)), 1, 0, 'C')
        pdf.ln()

    pdf.ln(10)

    # -------------------------------------
    # 東 Resumo de Valores Ausentes
    # -------------------------------------

    pdf.set_font("Arial", style="B", size=12)
    pdf.cell(0, 10, txt=clean_text("Resumo de Valores Ausentes"), ln=True)
    pdf.set_font("Arial", size=8)

    if not missing_data.empty:
        pdf.set_fill_color(144, 238, 144)
        pdf.cell(50, 10, clean_text("Variﾃ｡vel"), 1, 0, 'C', True)
        pdf.cell(50, 10, clean_text("Total de Ausentes"), 1, 1, 'C', True)
        for col, count in missing_data.items():
            pdf.cell(50, 10, clean_text(col), 1)
            pdf.cell(50, 10, clean_text(str(count)), 1, 1)
        pdf.ln(10)
    else:
        pdf.cell(0, 10, txt=clean_text("Nﾃ｣o hﾃ｡ valores ausentes."), ln=True)

    # -------------------------------------
    # 東 Resumo de Outliers
    # -------------------------------------

    pdf.set_font("Arial", style="B", size=12)
    pdf.cell(0, 10, txt=clean_text("Resumo de Outliers"), ln=True)
    pdf.set_font("Arial", size=8)

    if outlier_summary:
        pdf.set_fill_color(144, 238, 144)
        pdf.cell(50, 10, clean_text("Variﾃ｡vel"), 1, 0, 'C', True)
        pdf.cell(50, 10, clean_text("Total de Outliers"), 1, 1, 'C', True)
        for entry in outlier_summary:
            pdf.cell(50, 10, clean_text(entry["Variﾃ｡vel"]), 1)
            pdf.cell(50, 10, clean_text(str(entry["Total de Outliers"])), 1, 1)
        pdf.ln(10)
    else:
        pdf.cell(0, 10, txt=clean_text("Nﾃ｣o hﾃ｡ outliers."), ln=True)

    # -------------------------------------
    # 東 Matriz de Correlaﾃｧﾃ｣o (Heatmap)
    # -------------------------------------

    pdf.cell(0, 10, txt=clean_text("Matriz de Correlaﾃｧﾃ｣o das Variﾃ｡veis"), ln=True)
    numeric_data = dataset.select_dtypes(include=['float64', 'int64'])
    correlation_matrix = numeric_data.corr()

    plt.figure(figsize=(8, 6))
    sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".4f", cbar=True, square=True)
    plt.title('Matriz de Correlaﾃｧﾃ｣o das Variﾃ｡veis', fontsize=14, fontweight='bold')

    temp_filename = "correlation_heatmap.png"
    plt.savefig(temp_filename)
    plt.close()
    pdf.image(temp_filename, x=10, w=180)
    pdf.ln(95)

    # -------------------------------------
    # 東 Boxplot das Variﾃ｡veis Numﾃｩricas
    # -------------------------------------

    pdf.cell(0, 10, txt=clean_text("Boxplot das Variﾃ｡veis Numﾃｩricas"), ln=True)
    plt.figure(figsize=(10, 6))
    sns.boxplot(data=numeric_data)
    plt.title('Boxplot das Variﾃ｡veis Numﾃｩricas')

    temp_filename_boxplot = "boxplot_combined.png"
    plt.savefig(temp_filename_boxplot)
    plt.close()
    pdf.image(temp_filename_boxplot, x=10, w=180)
    pdf.ln(75)

    # -------------------------------------
    # 東 Gerar o PDF no Buffer de Memﾃｳria
    # -------------------------------------

    pdf_buffer = BytesIO()
    pdf_output = pdf.output(dest='S').encode('latin-1', errors='ignore')
    pdf_buffer.write(pdf_output)
    pdf_buffer.seek(0)

    return pdf_buffer

# -------------------------------------
# 東 FUNﾃﾃグ PARA SALVAR UMA TABELA COMO IMAGEM (PNG)
# -------------------------------------

import matplotlib.pyplot as plt

def save_table_as_image(df, filename="table_image.png"):
    """
    Converte um DataFrame Pandas numa imagem (PNG), formatando os valores para melhor visualizaﾃｧﾃ｣o.

    Parﾃ｢metros:
    - df: DataFrame contendo a tabela a ser convertida em imagem.
    - filename: Nome do ficheiro da imagem a ser salva (padrﾃ｣o: "table_image.png").
    """

    # -------------------------------------
    # 東 Tratamento de Valores no DataFrame Antes da Geraﾃｧﾃ｣o da Imagem
    # -------------------------------------

    # Substituir valores `NaN` por valores vazios para evitar exibiﾃｧﾃｵes incorretas
    df = df.fillna('')

    # Formatar valores numﾃｩricos para 4 casas decimais
    for col in df.select_dtypes(include=['float64', 'int64']).columns:
        df[col] = df[col].apply(lambda x: f"{x:.4f}" if isinstance(x, (int, float)) else x)

    # -------------------------------------
    # 東 Configuraﾃｧﾃ｣o da Figura para Geraﾃｧﾃ｣o da Tabela
    # -------------------------------------

    fig, ax = plt.subplots(figsize=(8, 4))  # Define o tamanho da imagem gerada
    ax.axis('tight')  # Ajusta os limites para caber na figura
    ax.axis('off')  # Remove os eixos para melhor visualizaﾃｧﾃ｣o

    # Criar a tabela no grﾃ｡fico
    table = ax.table(
        cellText=df.values,  # Conteﾃｺdo da tabela
        colLabels=df.columns,  # Cabeﾃｧalhos das colunas
        loc='center',  # Centralizar a tabela na imagem
        cellLoc='center',  # Centralizar o texto nas cﾃｩlulas
        colColours=['#D9EAF7'] * len(df.columns)  # Definir cor do cabeﾃｧalho da tabela
    )

    # -------------------------------------
    # 東 Ajustes de Formataﾃｧﾃ｣o da Tabela
    # -------------------------------------

    table.auto_set_font_size(False)  # Desativar ajuste automﾃ｡tico do tamanho da fonte
    table.set_fontsize(10)  # Definir tamanho da fonte manualmente
    table.auto_set_column_width(col=list(range(len(df.columns))))  # Ajustar automaticamente a largura das colunas

    # -------------------------------------
    # 東 Salvamento da Tabela Como Imagem (PNG)
    # -------------------------------------

    plt.savefig(filename, format='png', bbox_inches='tight')  # Salvar imagem no formato PNG
    plt.close()  # Fechar a figura para evitar sobrecarga de memﾃｳria

# Resumo do Prﾃｩ-processamento de dados:
# -------------------------------------
# 東 FUNﾃﾃグ PARA GERAR O RESUMO DOS DADOS
# -------------------------------------

def data_summary():
    """
    Apresenta um resumo dos dados tratados, incluindo estatﾃｭsticas descritivas, valores ausentes,
    detecﾃｧﾃ｣o de outliers, boxplots e matriz de correlaﾃｧﾃ｣o. Alﾃｩm disso, permite o download do resumo
    em PDF e do dataset tratado.
    """

    st.subheader("Resumo dos Dados")

    # -------------------------------------
    # 東 Verificar Disponibilidade do Dataset
    # -------------------------------------

    if 'data' in st.session_state and st.session_state.data is not None:
        dataset = st.session_state.data
        st.success("Usando o dataset tratado!")
    else:
        st.error("Nenhum dataset estﾃ｡ disponﾃｭvel. Por favor, execute o tratamento de dados antes.")
        return  # Encerra a funﾃｧﾃ｣o caso nﾃ｣o haja dados disponﾃｭveis

    # -------------------------------------
    # 東 Seleﾃｧﾃ｣o de Colunas para Exibiﾃｧﾃ｣o
    # -------------------------------------

    # Obter colunas selecionadas ou usar todas as colunas do dataset
    selected_columns = st.session_state.get('selected_columns', [])
    if not selected_columns:
        selected_columns = dataset.columns.tolist()

    # Permitir que o utilizador selecione as colunas para visualizaﾃｧﾃ｣o
    selected_columns_to_display = st.multiselect(
        "Selecione as variﾃ｡veis para visualizar as estatﾃｭsticas",
        options=selected_columns,
        default=selected_columns
    )

    # Exibir o nﾃｺmero de linhas e colunas do dataset filtrado
    st.write("Nﾃｺmero de linhas e colunas:", dataset[selected_columns_to_display].shape)

    # -------------------------------------
    # 東 Estatﾃｭsticas Descritivas
    # -------------------------------------

    # Identificar colunas numﾃｩricas
    numeric_columns = dataset[selected_columns_to_display].select_dtypes(include=['number']).columns

    # Criar um dicionﾃ｡rio para armazenar estatﾃｭsticas
    summary_data = {
        'Count': dataset[selected_columns_to_display].count(),
        'Mean': dataset[numeric_columns].mean(),
        'Std': dataset[numeric_columns].std(),
        'Min': dataset[numeric_columns].min(),
        '25%': dataset[numeric_columns].quantile(0.25),
        '50%': dataset[numeric_columns].median(),
        '75%': dataset[numeric_columns].quantile(0.75),
        'Max': dataset[numeric_columns].max(),
    }

    # Converter para DataFrame e adicionar os tipos de dados
    summary_df = pd.DataFrame(summary_data)
    summary_df['Tipo de Dados'] = dataset[selected_columns_to_display].dtypes

    # Arredondar valores numﾃｩricos para 4 casas decimais e preencher valores ausentes com 0
    summary_df = summary_df.round(4).fillna(0)

    # Exibir a tabela de estatﾃｭsticas descritivas
    st.write("Estatﾃｭsticas Descritivas e Tipos de Dados")
    st.dataframe(fix_dataframe_types(summary_df))

    # -------------------------------------
    # 東 Anﾃ｡lise de Valores Ausentes
    # -------------------------------------

    st.subheader("Resumo de Valores Ausentes")

    # Identificar colunas com valores ausentes
    missing_data = dataset[selected_columns_to_display].isnull().sum()
    missing_data = missing_data[missing_data > 0]

    if not missing_data.empty:
        st.write("Valores ausentes encontrados:")
        st.dataframe(fix_dataframe_types(missing_data.rename("Total de Valores Ausentes")))
    else:
        st.write("Nﾃ｣o hﾃ｡ valores ausentes nas variﾃ｡veis selecionadas.")

    # -------------------------------------
    # 東 Anﾃ｡lise de Outliers
    # -------------------------------------

    st.subheader("Resumo de Outliers")

    # Selecionar apenas colunas numﾃｩricas
    numeric_data = dataset[selected_columns_to_display].select_dtypes(include=['number'])

    # Obter colunas jﾃ｡ tratadas
    treated_columns = st.session_state.get('treated_columns', [])

    # Criar lista para armazenar o resumo dos outliers
    outlier_summary = []

    if not numeric_data.empty:
        for column in numeric_data.columns:
            if column in treated_columns:  # Ignorar colunas jﾃ｡ tratadas
                continue

            # Cﾃ｡lculo dos quartis e do intervalo interquartil (IQR)
            Q1 = numeric_data[column].quantile(0.25)
            Q3 = numeric_data[column].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR

            # Identificar outliers
            outliers = numeric_data[(numeric_data[column] < lower_bound) | (numeric_data[column] > upper_bound)]
            if len(outliers) > 0:
                outlier_summary.append({
                    "Variﾃ｡vel": column,
                    "Total de Outliers": len(outliers)
                })

        # Exibir o resumo dos outliers encontrados
        if outlier_summary:
            st.dataframe(fix_dataframe_types(pd.DataFrame(outlier_summary)))
        else:
            st.write("Nﾃ｣o hﾃ｡ outliers nas variﾃ｡veis selecionadas.")
    else:
        st.write("Nenhuma variﾃ｡vel numﾃｩrica para anﾃ｡lise de outliers.")

    # -------------------------------------
    # 東 Grﾃ｡fico Boxplot das Variﾃ｡veis Numﾃｩricas
    # -------------------------------------

    st.subheader("Boxplot das Variﾃ｡veis Numﾃｩricas")

    plt.figure(figsize=(10, 6))
    sns.boxplot(data=numeric_data)
    plt.title('Boxplot das Variﾃ｡veis Numﾃｩricas')
    st.pyplot(plt)

    # -------------------------------------
    # 東 Matriz de Correlaﾃｧﾃ｣o (Heatmap)
    # -------------------------------------

    st.subheader("Matriz de Correlaﾃｧﾃ｣o das Variﾃ｡veis")

    # Calcular a correlaﾃｧﾃ｣o entre variﾃ｡veis numﾃｩricas
    correlation_matrix = numeric_data.corr()

    # Gerar e exibir o heatmap da correlaﾃｧﾃ｣o
    plt.figure(figsize=(8, 6))
    sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".4f", cbar=True, square=True)
    plt.title('Matriz de Correlaﾃｧﾃ｣o das Variﾃ｡veis', fontsize=14, fontweight='bold', fontname='Arial')
    st.pyplot(plt)

    # -------------------------------------
    # 東 Download do Resumo em PDF
    # -------------------------------------

    pdf_buffer = generate_pdf_resumo(dataset, summary_df, missing_data, outlier_summary)
    st.download_button(
        label="Baixar PDF com o Resumo",
        data=pdf_buffer,
        file_name="resumo_dos_dados.pdf",
        mime="application/pdf"
    )

    # -------------------------------------
    # 東 Download do Dataset Tratado
    # -------------------------------------

    dataset_to_download = dataset[selected_columns_to_display]
    download_button(dataset_to_download)

    # -------------------------------------
    # 東 Navegaﾃｧﾃ｣o Entre Etapas
    # -------------------------------------

    col1, col2 = st.columns([1, 1])

    with col1:
        if st.button("Voltar"):
            st.session_state.step = 'outlier_detection'
            st.rerun()

    with col2:
        if st.button("Prﾃｳxima etapa"):
            st.session_state.step = 'model_selection'
            st.rerun()


##########################################################
# -------------------------------------
# 東 FUNﾃﾃグ PARA PLOTAR Mﾃ欝RICAS DE DESEMPENHO DOS MODELOS
# -------------------------------------

import streamlit as st
import matplotlib.pyplot as plt

def plot_metrics(metrics_df):
    """
    Gera grﾃ｡ficos para visualizar as mﾃｩtricas de desempenho dos modelos, diferenciando entre
    tarefas de classificaﾃｧﾃ｣o e regressﾃ｣o.

    Parﾃ｢metros:
    - metrics_df: DataFrame contendo as mﾃｩtricas de desempenho dos modelos.

    Retorno:
    - Exibe os grﾃ｡ficos no Streamlit.
    """

    try:
        # -------------------------------------
        # 東 Inicializar Armazenamento de Mﾃｩtricas no Estado da Sessﾃ｣o
        # -------------------------------------

        # Se a chave 'metrics' ainda nﾃ｣o estiver no session_state, inicializﾃ｡-la
        if 'metrics' not in st.session_state:
            st.session_state['metrics'] = {}

        # Verificar se o DataFrame estﾃ｡ vazio
        if metrics_df.empty:
            st.warning("Nenhum dado para exibir no grﾃ｡fico.")
            return

        # Armazenar as mﾃｩtricas no estado da sessﾃ｣o para referﾃｪncia posterior
        for _, row in metrics_df.iterrows():
            model_name = row.name  # Assumindo que o ﾃｭndice contﾃｩm o nome do modelo
            st.session_state['metrics'][model_name] = row.to_dict()

        # -------------------------------------
        # 東 Configuraﾃｧﾃ｣o do ﾃ肱dice e Identificaﾃｧﾃ｣o do Tipo de Modelo
        # -------------------------------------

        # Definir a coluna 'Modelo' como ﾃｭndice, se ainda nﾃ｣o estiver
        metrics_df.set_index('Modelo', inplace=True)

        # Listas de mﾃｩtricas tﾃｭpicas para classificaﾃｧﾃ｣o e regressﾃ｣o
        classification_columns = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
        regression_columns = ['MSE', 'MAE', 'Rﾂｲ']

        # -------------------------------------
        # 東 Plotagem de Grﾃ｡ficos de Classificaﾃｧﾃ｣o
        # -------------------------------------

        if all(col in metrics_df.columns for col in classification_columns):
            # Criar a figura do grﾃ｡fico de barras
            fig, ax = plt.subplots(figsize=(10, 6))
            
            # Plotar as mﾃｩtricas de classificaﾃｧﾃ｣o
            metrics_df[classification_columns].plot(kind='bar', ax=ax)

            # Configuraﾃｧﾃ｣o do grﾃ｡fico
            plt.title('Mﾃｩtricas de Desempenho dos Modelos (Classificaﾃｧﾃ｣o)', fontsize=16)
            plt.ylabel('Valor', fontsize=14)
            plt.xlabel('Modelos', fontsize=14)
            plt.xticks(rotation=45, ha='right', fontsize=12)
            plt.ylim(0, 1)  # As mﾃｩtricas de classificaﾃｧﾃ｣o geralmente variam entre 0 e 1
            plt.legend(loc='lower right', fontsize=12)
            plt.grid(axis='y', linestyle='--', alpha=0.7)

        # -------------------------------------
        # 東 Plotagem de Grﾃ｡ficos de Regressﾃ｣o
        # -------------------------------------

        elif all(col in metrics_df.columns for col in regression_columns):
            # Criar a figura do grﾃ｡fico de barras
            fig, ax = plt.subplots(figsize=(10, 6))

            # Plotar as mﾃｩtricas de regressﾃ｣o
            metrics_df[regression_columns].plot(kind='bar', ax=ax)

            # Configuraﾃｧﾃ｣o do grﾃ｡fico
            plt.title('Mﾃｩtricas de Desempenho dos Modelos (Regressﾃ｣o)', fontsize=16)
            plt.ylabel('Valor', fontsize=14)
            plt.xlabel('Modelos', fontsize=14)
            plt.xticks(rotation=45, ha='right', fontsize=12)
            plt.legend(loc='upper right', fontsize=12)
            plt.grid(axis='y', linestyle='--', alpha=0.7)

        else:
            st.error("O DataFrame nﾃ｣o contﾃｩm mﾃｩtricas vﾃ｡lidas para classificaﾃｧﾃ｣o ou regressﾃ｣o.")
            return  # Se nﾃ｣o hﾃ｡ mﾃｩtricas vﾃ｡lidas, encerra a funﾃｧﾃ｣o

        # -------------------------------------
        # 東 Exibir o Grﾃ｡fico no Streamlit
        # -------------------------------------

        st.pyplot(fig)

    except Exception as e:
        # Tratamento de erros genﾃｩrico para evitar falhas inesperadas
        st.error(f"Ocorreu um erro ao plotar as mﾃｩtricas: {str(e)}")

    finally:
        # Limpar a figura para evitar sobreposiﾃｧﾃ｣o de grﾃ｡ficos na interface do Streamlit
        plt.clf()

# -------------------------------------
# 東 FUNﾃﾃグ PARA DEFINIR O GRID DE HIPERPARﾃMETROS PADRﾃグ PARA CADA MODELO
# -------------------------------------

def get_default_param_grid(model_name):
    """
    Retorna um dicionﾃ｡rio contendo os hiperparﾃ｢metros padrﾃ｣o para cada modelo de Machine Learning.

    Parﾃ｢metros:
    - model_name: Nome do modelo para o qual se deseja obter o conjunto de hiperparﾃ｢metros.

    Retorno:
    - Dicionﾃ｡rio com os hiperparﾃ｢metros e os respetivos intervalos de valores para otimizaﾃｧﾃ｣o.
    """

    # -------------------------------------
    # 東 Configuraﾃｧﾃ｣o do Grid Search para Support Vector Classification (SVC)
    # -------------------------------------
    if model_name == "Support Vector Classification (SVC)":
        return {
            'C': [0.1, 1, 10],  # Define a penalizaﾃｧﾃ｣o do erro
            'kernel': ['linear', 'rbf'],  # Tipos de kernel utilizados
            'gamma': ['scale', 'auto']  # Apenas utilizado quando kernel='rbf'
        }

    # -------------------------------------
    # 東 Configuraﾃｧﾃ｣o do Grid Search para K-Nearest Neighbors (KNN)
    # -------------------------------------
    elif model_name == "K-Nearest Neighbors (KNN)":
        return {
            'n_neighbors': list(range(1, 21)),  # Testa todos os valores de 1 a 20 para o nﾃｺmero de vizinhos
            'weights': ['uniform', 'distance']  # Define a forma de ponderaﾃｧﾃ｣o das distﾃ｢ncias
        }

    # -------------------------------------
    # 東 Configuraﾃｧﾃ｣o do Grid Search para Random Forest
    # -------------------------------------
    elif model_name == "Random Forest":
        # Geraﾃｧﾃ｣o dinﾃ｢mica do parﾃ｢metro `max_depth`
        max_depth_range = [None] + list(range(5, 21, 5))  # [None, 5, 10, 15, 20]
        return {
            'max_depth': max_depth_range,  # Profundidade mﾃ｡xima da ﾃ｡rvore
            'n_estimators': [10, 50, 100]  # Nﾃｺmero de ﾃ｡rvores na floresta
        }

    # -------------------------------------
    # 東 Configuraﾃｧﾃ｣o do Grid Search para Suporte de Vetores em Regressﾃ｣o (SVR)
    # -------------------------------------
    elif model_name == "Regressﾃ｣o por Vetores de Suporte (SVR)":
        return {
            'C': [1, 10],  # Penalizaﾃｧﾃ｣o do erro
            'epsilon': [0.1, 0.2],  # Margem de tolerﾃ｢ncia para erro
            'kernel': ['linear', 'rbf']  # Tipos de kernel utilizados
        }

    # -------------------------------------
    # 東 Configuraﾃｧﾃ｣o para Regressﾃ｣o Linear Simples (RLS)
    # -------------------------------------
    elif model_name == "Regressﾃ｣o Linear Simples (RLS)":
        return {}  # A regressﾃ｣o linear geralmente nﾃ｣o requer ajuste de hiperparﾃ｢metros

    # -------------------------------------
    # 東 Retorno para modelos nﾃ｣o especificados
    # -------------------------------------
    else:
        return {}  # Se o modelo nﾃ｣o for reconhecido, retorna um dicionﾃ｡rio vazio

def configure_manual_params(model_key, param_grid, manual_params):
    """
    Configura manualmente os parﾃ｢metros para o modelo selecionado com intervalos personalizados exibidos.
    """
    st.write(f"Configuraﾃｧﾃｵes manuais para o modelo: {model_key}")

    # **Limpar parﾃ｢metros invﾃ｡lidos no estado global ANTES de criar os widgets**
    if 'manual_params' in st.session_state and 'gamma' in st.session_state['manual_params']:
        del st.session_state['manual_params']['gamma']  # Remove 'gamma' do estado global

    # Intervalos especﾃｭficos para parﾃ｢metros
    param_ranges = {
        'C': {'min': 0.1, 'max': 100.0, 'step': 0.1, 'default': 1.0},
        'epsilon': {'min': 0.01, 'max': 1.0, 'step': 0.01, 'default': 0.1},
        'gamma': {'min': 0.01, 'max': 1.0, 'step': 0.01, 'default': 0.1},
        'degree': {'min': 1, 'max': 5, 'step': 1, 'default': 3},
    }

    # Criar widgets para parﾃ｢metros
    for param in param_grid:
        # Parﾃ｢metros categﾃｳricos
        if isinstance(param_grid[param][0], str):
            manual_params[param] = st.selectbox(
                f"{param} (Opﾃｧﾃｵes: {', '.join(param_grid[param])}):",
                options=param_grid[param],
                index=0,
                key=f"{model_key}_{param}"
            )
        # Parﾃ｢metros numﾃｩricos
        elif isinstance(param_grid[param][0], (int, float)):
            param_type = float if any(isinstance(x, float) for x in param_grid[param]) else int

            # Verificar se existe intervalo personalizado
            if param in param_ranges:
                config = param_ranges[param]

                # Mostrar intervalo aceito como dica para o utilizador
                st.write(f"**{param}** (Intervalo: {config['min']} a {config['max']})")

                # Configuraﾃｧﾃ｣o interativa
                if param == 'max_depth':  # Verifica se o parﾃ｢metro ﾃｩ 'max_depth'
                    manual_params[param] = st.selectbox(
                        f"{param}:",
                        options=[None] + list(range(1, 21)),  # Inclusﾃ｣o de None
                        index=0 if config['default'] is None else list(range(1, 21)).index(config['default']),
                        key=f"{model_key}_{param}"
                    )
                else:
                    # Para outros parﾃ｢metros numﾃｩricos
                    manual_params[param] = st.number_input(
                        f"{param}:",
                        min_value=config['min'],
                        max_value=config['max'],
                        value=config['default'],
                        step=config['step'],
                        key=f"{model_key}_{param}"
                    )

    # **Configuraﾃｧﾃ｣o dinﾃ｢mica para 'gamma' com base no kernel**
    if 'kernel' in manual_params and manual_params['kernel'] == 'rbf':
        config = param_ranges['gamma']
        st.write(f"**gamma** (Intervalo: {config['min']} a {config['max']})")
        manual_params['gamma'] = st.number_input(
            "gamma:",
            min_value=config['min'],
            max_value=config['max'],
            value=config['default'],
            step=config['step'],
            key=f"{model_key}_gamma"
        )
    else:
        # **Remover 'gamma' do manual_params e do estado global se o kernel nﾃ｣o for 'rbf'**
        manual_params.pop('gamma', None)
        if 'manual_params' in st.session_state and 'gamma' in st.session_state['manual_params']:
            del st.session_state['manual_params']['gamma']  # Remove tambﾃｩm do estado global

    # Atualizar estado global com os parﾃ｢metros finais
    st.session_state['manual_params'] = manual_params
    st.session_state['best_params_str'] = json.dumps(manual_params, indent=2)

    # Diagnﾃｳstico: Exibir parﾃ｢metros salvos
    st.write("Parﾃ｢metros manuais salvos:", st.session_state['manual_params'])

    return manual_params




# Dicionﾃ｡rio que mapeia modelos aos seus parﾃ｢metros vﾃ｡lidos
VALID_PARAMS = {
    "Random Forest": ["n_estimators", "max_depth"],
    "Support Vector Classification (SVC)": ["C", "kernel", "gamma"],  # Agora inclui "gamma"
    "K-Nearest Neighbors (KNN)": ["n_neighbors", "weights"],
    "Regressﾃ｣o Linear Simples (RLS)": [],  # Sem hiperparﾃ｢metros ajustﾃ｡veis
    "Regressﾃ｣o por Vetores de Suporte (SVR)": ["C", "epsilon", "kernel"],  # Parﾃ｢metros ajustﾃ｡veis para SVR
}


# Funﾃｧﾃ｣o para configurar a validaﾃｧﾃ｣o cruzada com base na escolha do utilizador
def get_cv_strategy(cv_choice, X_train, y_train):
    if cv_choice == "K-Fold":
        return KFold(n_splits=5, shuffle=True, random_state=42)
    elif cv_choice == "Leave-One-Out":
        return LeaveOneOut()
    elif cv_choice == "Divisﾃ｣o em Treino e Teste":
        # Exemplo de divisﾃ｣o simples em treino e teste
        return train_test_split(X_train, y_train, test_size=0.3, random_state=42)
    elif cv_choice == "Holdout":
        # Pode ser uma abordagem similar ao treino-teste com outro conjunto
        return train_test_split(X_train, y_train, test_size=0.3, random_state=42)
    else:
        return KFold(n_splits=5, shuffle=True, random_state=42)  # Default ﾃｩ K-Fold

def configure_svr(model_key, manual_params):
    st.write("Configuraﾃｧﾃ｣o de parﾃ｢metros para Support Vector Regression (SVR)")
    
    # Configurar parﾃ｢metros comuns
    c = st.number_input("Parﾃ｢metro C (Regularizaﾃｧﾃ｣o)", min_value=0.1, max_value=100.0, step=0.1, value=1.0)
    epsilon = st.number_input("Parﾃ｢metro epsilon", min_value=0.0, max_value=1.0, step=0.1, value=0.1)
    kernel = st.selectbox("Kernel", options=["linear", "rbf", "poly", "sigmoid"], index=0)

    # Salvar os valores no dicionﾃ｡rio de parﾃ｢metros
    manual_params['C'] = c
    manual_params['epsilon'] = epsilon
    manual_params['kernel'] = kernel

    # Configuraﾃｧﾃ｣o adicional para kernels especﾃｭficos
    if kernel == "rbf":
        gamma = st.number_input("Parﾃ｢metro gamma", min_value=0.0, max_value=1.0, step=0.1, value=0.1)
        manual_params['gamma'] = gamma

    return manual_params

def configure_svc(model_key, manual_params):
    """Configura os parﾃ｢metros para o modelo SVC."""

    # Diagnﾃｳstico: Mostrar parﾃ｢metros antes da seleﾃｧﾃ｣o manual
    st.write("Estado inicial dos parﾃ｢metros:", st.session_state.get('manual_params', {}))

    # Seleﾃｧﾃ｣o do kernel
    kernel_value = st.selectbox(
        "Escolha o valor para 'kernel'",
        options=["linear", "rbf"],
        index=0,  # Define 'linear' como padrﾃ｣o
        key="kernel_selectbox"
    )

    # Configurar 'C' (sempre exibido)
    C_value = st.number_input(
        "Defina o valor para 'C'",
        min_value=0.01, step=0.01, value=1.0,
        key="C_input"
    )

    # Inicializa manual_params com 'C' e 'kernel'
    manual_params = {
        "C": C_value,
        "kernel": kernel_value
    }

    # **Exibir 'gamma' apenas se o kernel for 'rbf'**
    if kernel_value == "rbf":
        gamma_value = st.selectbox(
            "Escolha o valor para 'gamma'",
            options=["scale", "auto"],
            index=0,
            key="gamma_selectbox"
        )
        manual_params["gamma"] = gamma_value  # Adiciona gamma se necessﾃ｡rio
    else:
        # **Remover 'gamma' se o kernel for 'linear'**
        # Remover do manual_params local
        manual_params.pop('gamma', None)
        # Remover do estado global
        if 'manual_params' in st.session_state and 'gamma' in st.session_state['manual_params']:
            del st.session_state['manual_params']['gamma']  # Remove globalmente
        if 'best_params_str' in st.session_state:  # Remove dos parﾃ｢metros salvos
            st.session_state['best_params_str'] = json.dumps(manual_params, indent=2)

    # Diagnﾃｳstico: Mostrar parﾃ｢metros apﾃｳs a seleﾃｧﾃ｣o manual
    st.write("Parﾃ｢metros atualizados:", manual_params)

    # Salvar no estado global apenas parﾃ｢metros vﾃ｡lidos
    st.session_state['manual_params'] = manual_params
    st.session_state['best_params_str'] = json.dumps(manual_params, indent=2)

    # Exibir os parﾃ｢metros salvos para depuraﾃｧﾃ｣o
    st.write("Parﾃ｢metros manuais salvos:", st.session_state['manual_params'])

    return manual_params

import pickle
import os

def save_best_params(params):
    """Salva os melhores parﾃ｢metros encontrados em um arquivo."""
    with open('best_params.pkl', 'wb') as f:
        pickle.dump(params, f)

def load_best_params():
    """Carrega os melhores parﾃ｢metros salvos, se existirem."""
    if os.path.exists('best_params.pkl'):
        with open('best_params.pkl', 'rb') as f:
            return pickle.load(f)
    return None

# Treina um modelo de Regressﾃ｣o por Vetores de Suporte (SVR) com GridSearch opcional
def train_svr_with_gridsearch(X_train, y_train, X_test, y_test, use_grid_search=True, manual_params=None):
    """
    Train Support Vector Regression (SVR) model with optional GridSearchCV
    
    Parameters:
    -----------
    X_train : array-like
        Training feature matrix
    y_train : array-like
        Training target vector
    X_test : array-like
        Testing feature matrix
    y_test : array-like
        Testing target vector
    use_grid_search : bool, optional (default=True)
        Whether to use GridSearchCV for hyperparameter tuning
    manual_params : dict, optional
        Manually specified parameters to override GridSearch
    
    Returns:
    --------
    dict
        Dictionary containing model performance metrics and details
    """
    try:
        # Standardize the features
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # Base SVR model
        svr = SVR()
        
        # Default parameter grid for SVR
        param_grid = {
            'C': [0.1, 1, 10, 100],
            'epsilon': [0.01, 0.1, 0.2],
            'kernel': ['linear', 'rbf'],
            'gamma': ['scale', 'auto']
        }
        
        # If manual parameters are provided, update the param_grid
        if manual_params:
            for param, value in manual_params.items():
                # Ensure the value is a list for GridSearchCV
                param_grid[param] = [value] if not isinstance(value, list) else value
        
        # Cross-validation strategy
        cv_strategy = KFold(n_splits=5, shuffle=True, random_state=42)
        
        if use_grid_search:
            # Perform GridSearchCV
            grid_search = GridSearchCV(
                estimator=svr, 
                param_grid=param_grid, 
                cv=cv_strategy, 
                scoring='neg_mean_squared_error', 
                n_jobs=-1
            )
            grid_search.fit(X_train_scaled, y_train)
            
            # Best model from GridSearch
            best_model = grid_search.best_estimator_
            best_params = grid_search.best_params_
        else:
            # Use manual or default parameters
            if manual_params:
                svr.set_params(**manual_params)
            
            best_model = svr.fit(X_train_scaled, y_train)
            best_params = manual_params or {}
        
        # Make predictions
        y_pred = best_model.predict(X_test_scaled)
        
        # Calculate metrics
        mse = mean_squared_error(y_test, y_pred)
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        
        # Prepare metrics dictionary
        metrics = {
            "Modelo": "Support Vector Regression (SVR)",
            "Rﾂｲ": r2,
            "MAE": mae,
            "MSE": mse,
            "Best Parameters": best_params
        }
        
        return metrics
    
    except Exception as e:
        st.error(f"Erro ao treinar o modelo SVR: {str(e)}")
        return None

def train_model_with_gridsearch(model, param_grid, X_train, y_train, use_grid_search, manual_params=None, cv_choice="K-Fold"):
    try:
        # Inicializar parﾃ｢metros manuais como vazio, se nﾃ｣o fornecido
        if manual_params is None:
            manual_params = {}

        # Obter o nome do modelo
        model_name = type(model).__name__

        # Logs para diagnﾃｳstico - Parﾃ｢metros no estado global antes do treino
        st.write("Parﾃ｢metros no estado global antes do treino:")
        st.write("best_params:", st.session_state.get('best_params', {}))
        st.write("manual_params:", st.session_state.get('manual_params', {}))

        # Carregar parﾃ｢metros salvos do estado global
        saved_params = st.session_state.get('best_params', None)

        # Aplicar parﾃ｢metros salvos, se existirem e nﾃ｣o usar GridSearch
        if saved_params and not use_grid_search:
            st.info(f"Aplicando parﾃ｢metros salvos ao modelo: {saved_params}")
            model.set_params(**saved_params)

        # Remover 'gamma' se o kernel for 'linear'
        if manual_params.get("kernel") == "linear" and "gamma" in manual_params:
            del manual_params["gamma"]
            if 'gamma' in st.session_state.get('manual_params', {}):
                del st.session_state['manual_params']['gamma']

        # Se usar GridSearch
        if use_grid_search:
            # Atualizar grid com parﾃ｢metros manuais fornecidos
            if manual_params:
                for param, value in manual_params.items():
                    if not isinstance(value, list):
                        manual_params[param] = [value]
                param_grid.update(manual_params)

            # Configurar validaﾃｧﾃ｣o cruzada
            cv_strategy = get_cv_strategy(cv_choice, X_train, y_train)
            scoring = 'r2' if model_name == "SVR" else 'accuracy'

            # Treinar com GridSearch
            grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv_strategy, scoring=scoring, n_jobs=-1)
            grid_search.fit(X_train, y_train)

            # Melhor modelo e parﾃ｢metros
            best_model = grid_search.best_estimator_
            best_params = grid_search.best_params_
            st.session_state['best_params'] = best_params
            st.success(f"Melhores parﾃ｢metros encontrados: {best_params}")

            return best_model, best_params

        else:
            # Se nﾃ｣o usar GridSearch, aplicar manualmente os parﾃ｢metros
            valid_params = model.get_params().keys()
            manual_params = {k: v for k, v in manual_params.items() if k in valid_params}
            model.set_params(**manual_params)

            # Treinar diretamente
            model.fit(X_train, y_train)

            # Salvar parﾃ｢metros manuais no estado global
            st.session_state['manual_params'] = manual_params
            st.success(f"Parﾃ｢metros manuais salvos: {manual_params}")

            return model, manual_params

    except Exception as e:
        st.error(f"Ocorreu um erro ao treinar o modelo: {str(e)}")
        return None, None

# Funﾃｧﾃ｣o para calcular o Gap Statistic para o Clustering Hierﾃ｡rquico
def calculate_gap_statistic_hierarchical(X, n_clusters_range, n_ref=10):
    """
    Calcula o Gap Statistic para o AgglomerativeClustering.
    
    Parﾃ｢metros:
        X (ndarray): Dados de entrada (n_samples x n_features).
        n_clusters_range (tuple): Intervalo de nﾃｺmeros de clusters para avaliar.
        n_ref (int): Nﾃｺmero de amostras de referﾃｪncia aleatﾃｳrias a serem geradas.
    
    Retorna:
        gap_scores (list): Gap statistics para cada nﾃｺmero de clusters.
    """
    # Normalizar os dados
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # Armazenar os Gap Statistic Scores
    gap_scores = []
    
    for n_clusters in range(n_clusters_range[0], n_clusters_range[1] + 1):
        # Ajustar o modelo AgglomerativeClustering aos dados reais
        model = AgglomerativeClustering(n_clusters=n_clusters)
        model.fit(X_scaled)
        labels = model.labels_
        
        # Calcular a soma das distﾃ｢ncias dos pontos aos seus respectivos clusters
        intra_cluster_dist = sum([np.sum(np.linalg.norm(X_scaled[labels == i] - X_scaled[labels == i].mean(axis=0), axis=1)) for i in range(n_clusters)])
        
        # Gerar amostras de referﾃｪncia aleatﾃｳrias e calcular as distﾃ｢ncias dentro dos clusters aleatﾃｳrios
        ref_inertias = []
        for _ in range(n_ref):
            random_data = np.random.random_sample(size=X_scaled.shape)
            random_model = AgglomerativeClustering(n_clusters=n_clusters)
            random_model.fit(random_data)
            ref_labels = random_model.labels_
            ref_inertia = sum([np.sum(np.linalg.norm(random_data[ref_labels == i] - random_data[ref_labels == i].mean(axis=0), axis=1)) for i in range(n_clusters)])
            ref_inertias.append(ref_inertia)
        
        # Calcular a mﾃｩdia e o desvio padrﾃ｣o das inﾃｩrcias nos dados aleatﾃｳrios
        ref_inertia_mean = np.mean(ref_inertias)
        ref_inertia_std = np.std(ref_inertias)
        
        # Gap Statistic: diferenﾃｧa entre a inﾃｩrcia real e a mﾃｩdia das inﾃｩrcias aleatﾃｳrias
        gap = np.log(ref_inertia_mean) - np.log(intra_cluster_dist)
        gap_scores.append(gap)
    
    return gap_scores



# Funﾃｧﾃ｣o para a seleﾃｧﾃ｣o e treino de modelos
def model_selection():
    st.subheader("Seleﾃｧﾃ｣o e treino de Modelos")

    # Verificar se os dados estﾃ｣o disponﾃｭveis
    if 'data' not in st.session_state or st.session_state.data is None:
        st.error("Dados nﾃ｣o encontrados. Por favor, carregue os dados primeiro.")
        return

    # Usa diretamente st.session_state.data
    data = st.session_state.data
    columns = data.columns.tolist()
    
    # Inicializar variﾃ｡veis de estado se nﾃ｣o estiverem presentes
    if 'target_column' not in st.session_state:
        st.session_state.target_column = None
    if 'target_column_confirmed' not in st.session_state:
        st.session_state.target_column_confirmed = False
    if 'validation_method' not in st.session_state:
        st.session_state.validation_method = None
    if 'validation_confirmed' not in st.session_state:
        st.session_state.validation_confirmed = False
    if 'model_type' not in st.session_state:
        st.session_state.model_type = None
    if 'model_type_confirmed' not in st.session_state:
        st.session_state.model_type_confirmed = False
    if 'X_train' not in st.session_state:
        st.session_state.X_train = None
    if 'X_test' not in st.session_state:
        st.session_state.X_test = None
    if 'y_train' not in st.session_state:
        st.session_state.y_train = None
    if 'y_test' not in st.session_state:
        st.session_state.y_test = None
    if 'feature_selection_done' not in st.session_state:
        st.session_state.feature_selection_done = False

    # Configuraﾃｧﾃｵes
    st.write("### Configuraﾃｧﾃｵes")


    # 1. Escolha do Tipo de Modelo
    if not st.session_state.model_type_confirmed:
        st.write("Escolha o Tipo de Modelo")
        model_types = ["Classificaﾃｧﾃ｣o", "Regressﾃ｣o", "Clustering"]
        st.session_state.model_type = st.selectbox("Selecione o tipo de modelo", model_types)

        if st.button("Confirmar Tipo de Modelo"):
            st.session_state.model_type_confirmed = True
            st.success("Tipo de modelo confirmado!")

    # 2. Escolha do Modelo Especﾃｭfico
    if st.session_state.model_type_confirmed and not st.session_state.selected_model_name:
        st.write("Selecione o(s) Modelo(s)")

        # Modelos disponﾃｭveis com base no tipo selecionado
        if st.session_state.model_type == "Classificaﾃｧﾃ｣o":
            models = {
                "Support Vector Classification (SVC)": SVC(),
                "K-Nearest Neighbors (KNN)": KNeighborsClassifier(),
                "Random Forest": RandomForestClassifier()
            }
        elif st.session_state.model_type == "Regressﾃ｣o":
            models = {
                "Regressﾃ｣o Linear Simples (RLS)": LinearRegression(),
                "Regressﾃ｣o por Vetores de Suporte (SVR)": SVR(),
            }
        elif st.session_state.model_type == "Clustering":
            models = {
                "KMeans": KMeans(),
                "Clustering Hierﾃ｡rquico": AgglomerativeClustering(linkage='ward'),
            }

        # Armazena os modelos no session_state para uso posterior
        st.session_state.models = models
        

        # Condicional para exibir ou nﾃ｣o a opﾃｧﾃ｣o "Treinar todos os modelos"
        if st.session_state.model_type != "Clustering":
            model_options = list(models.keys()) 
        else:
            model_options = list(models.keys())  # Apenas os modelos de clustering

        default_model_name = st.session_state["model_name"]
        if default_model_name not in model_options:
            default_model_name = model_options[0]  # Corrigir para um valor vﾃ｡lido

        # Configurar o selectbox
        model_name = st.selectbox(
            "Selecione o modelo", 
            options=model_options, 
            key="model_name_selectbox", 
            index=model_options.index(default_model_name)
        )

        # Atualizar o estado do modelo selecionado
        st.session_state["model_name"] = model_name
        st.session_state.model_name = model_name

        # Botﾃ｣o para confirmar o modelo
        if st.button("Confirmar Modelo"):
            if model_name:  # Verifica se um modelo foi selecionado
                st.session_state.selected_model_name = model_name
                st.success(f"Modelo selecionado: {st.session_state.selected_model_name}")
            else:
                st.warning("Selecione um modelo antes de continuar.")


    # Funﾃｧﾃ｣o para a configuraﾃｧﾃ｣o de Clustering
    import pandas as pd
    from sklearn.decomposition import PCA
    import numpy as np

    # Inicializar a variﾃ｡vel `best_n_clusters_retrain` com um valor padrﾃ｣o
    best_n_clusters_retrain = None

    # Inicializar estados se nﾃ｣o existirem
    if 'pca_configured' not in st.session_state:
        st.session_state.pca_configured = False
    if 'ready_for_clustering' not in st.session_state:
        st.session_state.ready_for_clustering = False

    # Funﾃｧﾃ｣o para a configuraﾃｧﾃ｣o de Clustering
    if st.session_state.model_type == "Clustering" and st.session_state.selected_model_name:
        st.write("### Configuraﾃｧﾃ｣o para Clustering")

        # Dados categﾃｳricos codificados
        X = pd.get_dummies(st.session_state.data)

        # Padronizar os dados
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # ETAPA 1: Configuraﾃｧﾃ｣o do PCA para Clustering Hierﾃ｡rquico
        if st.session_state.selected_model_name == "Clustering Hierﾃ｡rquico" and not st.session_state.pca_configured:
            st.write("### Reduﾃｧﾃ｣o de Dimensionalidade com PCA para Clustering Hierﾃ｡rquico")
            
            # Verificar se o dataset ﾃｩ grande o suficiente para um aviso
            if X.shape[0] > 1000 or X.shape[1] > 10:
                st.warning(f"Atenﾃｧﾃ｣o: Seu dataset tem {X.shape[0]} registros e {X.shape[1]} dimensﾃｵes. A aplicaﾃｧﾃ｣o de PCA ﾃｩ necessﾃ｡ria para Clustering Hierﾃ｡rquico.")
            
            # Permitir ao utilizador escolher o nﾃｺmero de componentes ou usar valor automﾃ｡tico
            use_auto_components = st.checkbox("Determinar automaticamente o nﾃｺmero de componentes", value=True, key="auto_comp_hierarch")
            
            if use_auto_components:
                # Calcular o PCA para determinar a variﾃ｢ncia explicada
                pca_full = PCA().fit(X_scaled)
                explained_variance_ratio = pca_full.explained_variance_ratio_
                cumulative_variance = np.cumsum(explained_variance_ratio)
                
                # Encontrar o nﾃｺmero de componentes que explicam pelo menos 90% da variﾃ｢ncia
                n_components = np.argmax(cumulative_variance >= 0.9) + 1
                n_components = min(n_components, 10)  # Limitar a no mﾃ｡ximo 10 componentes
                
                st.write(f"Nﾃｺmero de componentes selecionados automaticamente: {n_components} (explica aproximadamente {cumulative_variance[n_components-1]*100:.1f}% da variﾃ｢ncia)")
                
                # Mostrar grﾃ｡fico de variﾃ｢ncia explicada
                fig, ax = plt.subplots(figsize=(8, 4))
                ax.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='-')
                ax.axhline(y=0.9, color='r', linestyle='--', label='90% Variﾃ｢ncia Explicada')
                ax.axvline(x=n_components, color='g', linestyle='--', label=f'{n_components} Componentes')
                ax.set_xlabel('Nﾃｺmero de Componentes')
                ax.set_ylabel('Variﾃ｢ncia Explicada Acumulada')
                ax.set_title('Variﾃ｢ncia Explicada por Componentes do PCA')
                ax.legend()
                st.pyplot(fig)
                plt.clf()
            else:
                # Permitir que o utilizador escolha o nﾃｺmero de componentes
                max_components = min(X.shape[1], 20)  # Limitar ao nﾃｺmero de features ou 20, o que for menor
                n_components = st.slider("Nﾃｺmero de componentes PCA para Hierﾃ｡rquico", 2, max_components, value=min(3, max_components), key="n_comp_hierarch")
            
            # Botﾃ｣o para confirmar a configuraﾃｧﾃ｣o do PCA
            if st.button("Confirmar Configuraﾃｧﾃ｣o do PCA para Clustering Hierﾃ｡rquico"):
                # Aplicar PCA com o nﾃｺmero de componentes escolhido
                pca = PCA(n_components=n_components)
                X_pca = pca.fit_transform(X_scaled)
                
                # Salvar no estado da sessﾃ｣o
                st.session_state.X_pca = X_pca
                st.session_state.pca_n_components = n_components
                st.session_state.pca_configured = True
                st.session_state.pca_model = pca
                st.session_state.explained_variance = pca.explained_variance_ratio_
                
                st.success(f"PCA configurado com sucesso! Dimensionalidade reduzida de {X_scaled.shape[1]} para {X_pca.shape[1]} componentes.")
                
                # Visualizaﾃｧﾃ｣o 2D dos dados com PCA se tivermos pelo menos 2 componentes
                if n_components >= 2:
                    st.write("### Visualizaﾃｧﾃ｣o dos Dados Apﾃｳs PCA")
                    
                    # Permitir que o utilizador escolha quais componentes visualizar
                    available_components = min(n_components, 10)  # Limitar a 10 para evitar sobrecarga
                    
                    component_x = st.selectbox(
                        "Escolha o componente para o eixo X:",
                        options=list(range(available_components)),
                        format_func=lambda x: f"Componente {x+1}",
                        index=0,
                        key="comp_x_hierarch"
                    )
                    
                    component_y = st.selectbox(
                        "Escolha o componente para o eixo Y:",
                        options=list(range(available_components)),
                        format_func=lambda x: f"Componente {x+1}",
                        index=1 if available_components > 1 else 0,
                        key="comp_y_hierarch"
                    )
                    
                    # Criar a visualizaﾃｧﾃ｣o 2D baseada nos componentes escolhidos
                    fig, ax = plt.subplots(figsize=(10, 6))
                    scatter = ax.scatter(X_pca[:, component_x], X_pca[:, component_y], alpha=0.7)
                    ax.set_xlabel(f'Componente Principal {component_x+1}', fontsize=12)
                    ax.set_ylabel(f'Componente Principal {component_y+1}', fontsize=12)
                    ax.set_title(f'Visualizaﾃｧﾃ｣o 2D dos Componentes PCA {component_x+1} e {component_y+1}', fontsize=14, fontweight='bold')
                    ax.grid(True, linestyle='--', alpha=0.7)
                    
                    # Mostrar a variﾃ｢ncia explicada por estes componentes
                    if hasattr(pca, 'explained_variance_ratio_'):
                        var_x = pca.explained_variance_ratio_[component_x] * 100
                        var_y = pca.explained_variance_ratio_[component_y] * 100
                        ax.set_xlabel(f'Componente Principal {component_x+1} ({var_x:.1f}% variﾃ｢ncia)', fontsize=12)
                        ax.set_ylabel(f'Componente Principal {component_y+1} ({var_y:.1f}% variﾃ｢ncia)', fontsize=12)
                    
                    plt.tight_layout()
                    st.pyplot(fig)
                    plt.clf()

            # Botﾃ｣o para avanﾃｧar para a configuraﾃｧﾃ｣o do clustering (fora do if anterior)
                if st.button("Prosseguir para Clustering"):
                    st.session_state.ready_for_clustering = True
                    st.rerun()
            
        # ETAPA 1: Configuraﾃｧﾃ｣o do PCA para KMeans
        if st.session_state.selected_model_name == "KMeans" and not st.session_state.pca_configured:
            st.write("### Reduﾃｧﾃ｣o de Dimensionalidade com PCA")
            
            # Verificar se o dataset ﾃｩ grande o suficiente para um aviso
            if X.shape[0] > 1000 or X.shape[1] > 10:
                st.warning(f"Atenﾃｧﾃ｣o: Seu dataset tem {X.shape[0]} registros e {X.shape[1]} dimensﾃｵes. A aplicaﾃｧﾃ｣o de PCA ﾃｩ altamente recomendada.")
            
            # Permitir ao utilizador escolher o nﾃｺmero de componentes ou usar valor automﾃ｡tico
            use_auto_components = st.checkbox("Determinar automaticamente o nﾃｺmero de componentes", value=True)
            
            if use_auto_components:
                # Calcular o PCA para determinar a variﾃ｢ncia explicada
                pca_full = PCA().fit(X_scaled)
                explained_variance_ratio = pca_full.explained_variance_ratio_
                cumulative_variance = np.cumsum(explained_variance_ratio)
                
                # Encontrar o nﾃｺmero de componentes que explicam pelo menos 90% da variﾃ｢ncia
                n_components = np.argmax(cumulative_variance >= 0.9) + 1
                n_components = min(n_components, 10)  # Limitar a no mﾃ｡ximo 10 componentes
                
                st.write(f"Nﾃｺmero de componentes selecionados automaticamente: {n_components} (explica aproximadamente {cumulative_variance[n_components-1]*100:.1f}% da variﾃ｢ncia)")
                
                # Mostrar grﾃ｡fico de variﾃ｢ncia explicada
                fig, ax = plt.subplots(figsize=(8, 4))
                ax.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='-')
                ax.axhline(y=0.9, color='r', linestyle='--', label='90% Variﾃ｢ncia Explicada')
                ax.axvline(x=n_components, color='g', linestyle='--', label=f'{n_components} Componentes')
                ax.set_xlabel('Nﾃｺmero de Componentes')
                ax.set_ylabel('Variﾃ｢ncia Explicada Acumulada')
                ax.set_title('Variﾃ｢ncia Explicada por Componentes do PCA')
                ax.legend()
                st.pyplot(fig)
                plt.clf()
            else:
                # Permitir que o utilizador escolha o nﾃｺmero de componentes
                max_components = min(X.shape[1], 20)  # Limitar ao nﾃｺmero de features ou 20, o que for menor
                n_components = st.slider("Nﾃｺmero de componentes PCA", 2, max_components, value=min(3, max_components))
            
            # Botﾃ｣o para confirmar a configuraﾃｧﾃ｣o do PCA
            if st.button("Confirmar Configuraﾃｧﾃ｣o do PCA"):
                # Aplicar PCA com o nﾃｺmero de componentes escolhido
                pca = PCA(n_components=n_components)
                X_pca = pca.fit_transform(X_scaled)
                
                # Salvar no estado da sessﾃ｣o
                st.session_state.X_pca = X_pca
                st.session_state.pca_n_components = n_components
                st.session_state.pca_configured = True
                st.session_state.pca_model = pca
                st.session_state.explained_variance = pca.explained_variance_ratio_
                
                st.success(f"PCA configurado com sucesso! Dimensionalidade reduzida de {X_scaled.shape[1]} para {X_pca.shape[1]} componentes.")
                
                # Visualizaﾃｧﾃ｣o 2D e 3D simultﾃ｢nea dos dados com PCA se tivermos pelo menos 2 componentes
                if n_components >= 2:
                    st.write("### Visualizaﾃｧﾃ｣o dos Dados Apﾃｳs PCA")
                    
                    # Permitir que o utilizador escolha quais componentes visualizar
                    available_components = min(n_components, 10)  # Limitar a 10 para evitar sobrecarga
                    
                    component_x = st.selectbox(
                        "Escolha o componente para o eixo X:",
                        options=list(range(available_components)),
                        format_func=lambda x: f"Componente {x+1}",
                        index=0
                    )
                    
                    component_y = st.selectbox(
                        "Escolha o componente para o eixo Y:",
                        options=list(range(available_components)),
                        format_func=lambda x: f"Componente {x+1}",
                        index=1 if available_components > 1 else 0
                    )
                    
                    # Criar a visualizaﾃｧﾃ｣o 2D baseada nos componentes escolhidos
                    fig, ax = plt.subplots(figsize=(10, 6))
                    scatter = ax.scatter(X_pca[:, component_x], X_pca[:, component_y], alpha=0.7)
                    ax.set_xlabel(f'Componente Principal {component_x+1}', fontsize=12)
                    ax.set_ylabel(f'Componente Principal {component_y+1}', fontsize=12)
                    ax.set_title(f'Visualizaﾃｧﾃ｣o 2D dos Componentes PCA {component_x+1} e {component_y+1}', fontsize=14, fontweight='bold')
                    ax.grid(True, linestyle='--', alpha=0.7)
                    
                    # Mostrar a variﾃ｢ncia explicada por estes componentes (se disponﾃｭvel)
                    if hasattr(pca, 'explained_variance_ratio_'):
                        var_x = pca.explained_variance_ratio_[component_x] * 100
                        var_y = pca.explained_variance_ratio_[component_y] * 100
                        ax.set_xlabel(f'Componente Principal {component_x+1} ({var_x:.1f}% variﾃ｢ncia)', fontsize=12)
                        ax.set_ylabel(f'Componente Principal {component_y+1} ({var_y:.1f}% variﾃ｢ncia)', fontsize=12)
                    
                    plt.tight_layout()
                    st.pyplot(fig)
                    plt.clf()

                # Botﾃ｣o para avanﾃｧar para a configuraﾃｧﾃ｣o do clustering
                if st.button("Prosseguir para Clustering"):
                    st.session_state.ready_for_clustering = True
                    st.rerun()
        
        # ETAPA 2: Configuraﾃｧﾃ｣o do Clustering (apﾃｳs o PCA para Hierarchical ou diretamente para K-means)
        elif st.session_state.selected_model_name == "KMeans" or (st.session_state.selected_model_name == "Clustering Hierﾃ｡rquico" and st.session_state.pca_configured):
            # Escolher o intervalo de clusters (reduzido de 2-20 para 2-10 por padrﾃ｣o para ser menos pesado)
            num_clusters_range = st.slider("Intervalo de clusters para explorar (para anﾃ｡lise)", 2, 10, (2, 6))
            
            # Preparar dados para anﾃ｡lise
            if st.session_state.selected_model_name == "Clustering Hierﾃ｡rquico":
                # Para clustering hierﾃ｡rquico, usar dados com PCA
                training_data = st.session_state.X_pca
            else:
                # Para K-means, usar dados originais
                training_data = X_scaled
            
            # Opﾃｧﾃ｣o para usar amostragem para anﾃ｡lise mais rﾃ｡pida
            use_sampling = st.checkbox("Usar amostragem dos dados para anﾃ｡lise mais rﾃ｡pida", value=True)
            if use_sampling:
                sample_size = st.slider("Tamanho da amostra", 
                                    min_value=min(100, training_data.shape[0]),
                                    max_value=min(2000, training_data.shape[0]),
                                    value=min(1000, training_data.shape[0]))
                # Realizar amostragem
                np.random.seed(42)  # Para reprodutibilidade
                sample_indices = np.random.choice(training_data.shape[0], sample_size, replace=False)
                analysis_data = training_data[sample_indices]
                st.info(f"Usando {sample_size} pontos ({sample_size/training_data.shape[0]:.1%} dos dados) para anﾃ｡lise.")
            else:
                analysis_data = training_data
            
            # Anﾃ｡lise de clusters
            st.write("### Anﾃ｡lise para Determinaﾃｧﾃ｣o do Nﾃｺmero de Clusters")
            silhouette_scores = []
            davies_bouldin_scores = []
            calinski_harabasz_scores = []

            # Adicionar barra de progresso
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # Calcular mﾃｩtricas para cada nﾃｺmero de clusters
            total_iterations = num_clusters_range[1] - num_clusters_range[0] + 1

            # Condicional para KMeans e Clustering Hierﾃ｡rquico
            for i, n_clusters in enumerate(range(num_clusters_range[0], num_clusters_range[1] + 1)):
                # Atualizar barra de progresso
                progress = (i + 1) / total_iterations
                progress_bar.progress(progress)
                status_text.text(f"Analisando com {n_clusters} clusters... ({i+1}/{total_iterations})")
                
                try:
                    if st.session_state.selected_model_name == "KMeans":
                        # Otimizaﾃｧﾃ｣o: Reduzir n_init e max_iter para KMeans
                        temp_model = KMeans(n_clusters=n_clusters, random_state=42, n_init=5, max_iter=100)
                    else:  # Clustering Hierﾃ｡rquico
                        temp_model = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')
                    
                    # Treinar modelo com dados amostrados
                    temp_model.fit(analysis_data)
                    labels = temp_model.labels_
                    
                    # Calcular as mﾃｩtricas
                    silhouette_scores.append(silhouette_score(analysis_data, labels))
                    davies_bouldin_scores.append(davies_bouldin_score(analysis_data, labels))
                    calinski_harabasz_scores.append(calinski_harabasz_score(analysis_data, labels))
                    
                except Exception as e:
                    st.error(f"Erro ao processar {n_clusters} clusters: {str(e)}")
                    # Adicionar valores neutros para manter o array no tamanho correto
                    silhouette_scores.append(0)
                    davies_bouldin_scores.append(float('inf'))
                    calinski_harabasz_scores.append(0)
            
            # Limpar barra de progresso e status
            status_text.empty()
            progress_bar.empty()

            # Criar DataFrame com os resultados
            metrics_df = pd.DataFrame({
                "Nﾃｺmero de Clusters": range(num_clusters_range[0], num_clusters_range[1] + 1),
                "Silhouette Score": silhouette_scores,
                "Davies-Bouldin Index": davies_bouldin_scores,
                "Calinski-Harabasz Score": calinski_harabasz_scores,
            })
            
            # Exibir a tabela no Streamlit
            st.write("#### Tabela de Mﾃｩtricas por Nﾃｺmero de Clusters")
            st.dataframe(fix_dataframe_types(metrics_df.style.format({
                "Silhouette Score": "{:.2f}",
                "Davies-Bouldin Index": "{:.2f}",
                "Calinski-Harabasz Score": "{:.2f}",
            })))

            # Exibir grﾃ｡ficos para as mﾃｩtricas
            st.write("#### Grﾃ｡ficos das Mﾃｩtricas por Nﾃｺmero de Clusters")
            col1, col2, col3 = st.columns(3)

            with col1:
                plt.figure(figsize=(6, 4))
                plt.plot(metrics_df["Nﾃｺmero de Clusters"], metrics_df["Silhouette Score"], marker='o')
                plt.title("Silhouette Score por Nﾃｺmero de Clusters")
                plt.xlabel("Nﾃｺmero de Clusters")
                plt.ylabel("Silhouette Score")
                st.pyplot(plt.gcf())
                plt.clf()

            with col2:
                plt.figure(figsize=(6, 4))
                plt.plot(metrics_df["Nﾃｺmero de Clusters"], metrics_df["Davies-Bouldin Index"], marker='o')
                plt.title("Davies-Bouldin Index por Nﾃｺmero de Clusters")
                plt.xlabel("Nﾃｺmero de Clusters")
                plt.ylabel("Davies-Bouldin Index")
                st.pyplot(plt.gcf())
                plt.clf()

            with col3:
                plt.figure(figsize=(6, 4))
                plt.plot(metrics_df["Nﾃｺmero de Clusters"], metrics_df["Calinski-Harabasz Score"], marker='o')
                plt.title("Calinski-Harabasz Score por Nﾃｺmero de Clusters")
                plt.xlabel("Nﾃｺmero de Clusters")
                plt.ylabel("Calinski-Harabasz Score")
                st.pyplot(plt.gcf())
                plt.clf()

            # Escolher o melhor nﾃｺmero de clusters com base no Silhouette Score
            if silhouette_scores and any(score > 0 for score in silhouette_scores):
                best_n_clusters = metrics_df.loc[metrics_df["Silhouette Score"].idxmax(), "Nﾃｺmero de Clusters"]
                st.write(f"**Melhor Nﾃｺmero de Clusters** (com base no Silhouette Score): {best_n_clusters}")
                best_n_clusters_retrain = best_n_clusters

            # Escolher abordagem para nﾃｺmero de clusters
            st.write("### Escolha a Abordagem para Determinar o Nﾃｺmero de Clusters")
            method = st.radio("Selecione a abordagem:", ["Automﾃ｡tico", "Manual"], key="initial_training_method")

            if method == "Automﾃ｡tico":
                # Escolher o melhor nﾃｺmero de clusters com base no Silhouette Score
                if silhouette_scores and any(score > 0 for score in silhouette_scores):
                    best_n_clusters = range(num_clusters_range[0], num_clusters_range[1] + 1)[np.argmax(silhouette_scores)]
                    best_n_clusters_retrain = best_n_clusters  # Atualizar o valor para re-treino
                else:
                    st.error("Nﾃ｣o foi possﾃｭvel determinar automaticamente o nﾃｺmero de clusters. Por favor, selecione manualmente.")
                    best_n_clusters_retrain = 3  # Valor padrﾃ｣o

            elif method == "Manual":
                best_n_clusters = st.slider("Escolha o nﾃｺmero de clusters", num_clusters_range[0], num_clusters_range[1], value=3)
                best_n_clusters_retrain = best_n_clusters  # Atualizar o valor para re-treino

            # Garantir que `best_n_clusters_retrain` tenha um valor vﾃ｡lido antes de usar
            if best_n_clusters_retrain is None:
                st.warning("Por favor, selecione uma abordagem para determinar o nﾃｺmero de clusters.")
            else:
                # Treinar modelo inicial
                if st.button(f"Treinar Modelo Inicial"):
                    # Configurar e treinar o modelo (usando todos os dados para treino final)
                    if st.session_state.selected_model_name == "Clustering Hierﾃ｡rquico":
                        model = st.session_state.models["Clustering Hierﾃ｡rquico"]
                        model.set_params(n_clusters=best_n_clusters_retrain, linkage='ward')
                    else:  # KMeans
                        model = st.session_state.models["KMeans"]
                        # Otimizar KMeans para maior velocidade no treino final
                        model.set_params(n_clusters=best_n_clusters_retrain, n_init=5, max_iter=300)
                    
                    # Barra de progresso para o treino
                    with st.spinner(f"Treinando o modelo com {best_n_clusters_retrain} clusters..."):
                        model.fit(training_data)
                        st.session_state.clustering_labels = model.labels_
                    
                    # Calcular mﾃｩtricas
                    st.session_state.initial_metrics = {
                        "Nﾃｺmero de Clusters": best_n_clusters_retrain,
                        "Silhouette Score": silhouette_score(training_data, st.session_state.clustering_labels),
                        "Davies-Bouldin Index": davies_bouldin_score(training_data, st.session_state.clustering_labels),
                        "Calinski-Harabasz Score": calinski_harabasz_score(training_data, st.session_state.clustering_labels)
                    }
                    
                    # Salvar informaﾃｧﾃｵes importantes no estado da sessﾃ｣o
                    st.session_state.training_data = training_data
                    st.session_state.training_completed = True
                    st.session_state.trained_model = model  # Salvar o modelo treinado
                    
                    # Mostrar mensagem de sucesso
                    if st.session_state.selected_model_name == "Clustering Hierﾃ｡rquico":
                        st.success(f"Modelo hierﾃ｡rquico treinado com sucesso usando {best_n_clusters_retrain} clusters e {st.session_state.pca_n_components} componentes PCA!")
                    else:
                        st.success(f"Modelo K-means treinado com sucesso usando {best_n_clusters_retrain} clusters!")

            # Exibir mﾃｩtricas e prﾃｳxima aﾃｧﾃ｣o apenas apﾃｳs o treino
            if st.session_state.get("training_completed", False):
                st.write("### Mﾃｩtricas do Treino Inicial")
                st.table(fix_dataframe_types(pd.DataFrame([st.session_state.initial_metrics])))

                # Visualizaﾃｧﾃ｣o dos clusters
                if 'clustering_labels' in st.session_state:
                    st.write("### Visualizaﾃｧﾃ｣o dos Clusters")
                                        
                    # Para KMeans podemos mostrar os centroides
                    if st.session_state.selected_model_name == "KMeans":
                        if "trained_model" in st.session_state and hasattr(st.session_state.trained_model, 'cluster_centers_'):
                            st.write("#### Centroides dos Clusters")
                            centroids = st.session_state.trained_model.cluster_centers_
                            if centroids.shape[1] > 10:
                                st.write(f"(Mostrando apenas as primeiras 10 dimensﾃｵes de {centroids.shape[1]})")
                                centroids_df = pd.DataFrame(centroids[:, :10])
                            else:
                                centroids_df = pd.DataFrame(centroids)
                            
                            st.dataframe(fix_dataframe_types(centroids_df))
                    
                    # Preparar dados para visualizaﾃｧﾃ｣o
                    if st.session_state.selected_model_name == "Clustering Hierﾃ｡rquico":
                        # Para hierﾃ｡rquico, jﾃ｡ temos os dados PCA
                        plot_data = st.session_state.X_pca
                    else:
                        # Para K-means, podemos reduzir os dados para visualizaﾃｧﾃ｣o se necessﾃ｡rio
                        if X_scaled.shape[1] > 3:
                            pca_viz = PCA(n_components=3)
                            plot_data = pca_viz.fit_transform(X_scaled)
                            st.write("(Dados reduzidos via PCA para visualizaﾃｧﾃ｣o)")
                        else:
                            plot_data = X_scaled

                    # Obter nﾃｺmero total de componentes
                    total_components = plot_data.shape[1]

                    # Permitir escolha de componentes para x e y
                    st.write("### Escolha os Componentes para Visualizaﾃｧﾃ｣o")
                    col1, col2 = st.columns(2)

                    with col1:
                        x_component = st.selectbox(
                            "Componente para o Eixo X", 
                            list(range(total_components)), 
                            index=0,
                            format_func=lambda x: f"Componente {x+1}",
                            key="initial_x_component"  # Chave ﾃｺnica adicionada
                        )

                    with col2:
                        y_component = st.selectbox(
                            "Componente para o Eixo Y", 
                            list(range(total_components)), 
                            index=1 if total_components > 1 else 0,
                            format_func=lambda x: f"Componente {x+1}",
                            key="initial_y_component"  # Chave ﾃｺnica adicionada
                        )

                    # Verificar se componentes sﾃ｣o diferentes
                    if x_component == y_component:
                        st.warning("Por favor, selecione componentes diferentes para X e Y.")
                    else:
                        # Visualizaﾃｧﾃ｣o 2D com componentes selecionados
                        fig, ax = plt.subplots(figsize=(10, 6))
                        scatter = ax.scatter(
                            plot_data[:, x_component], 
                            plot_data[:, y_component], 
                            c=st.session_state.clustering_labels, 
                            cmap='viridis', 
                            alpha=0.7
                        )
                        ax.set_title(f'Visualizaﾃｧﾃ｣o 2D dos Clusters ({best_n_clusters_retrain} clusters)')
                        ax.set_xlabel(f'Componente {x_component+1}')
                        ax.set_ylabel(f'Componente {y_component+1}')
                        legend = ax.legend(*scatter.legend_elements(), title="Clusters")
                        ax.add_artist(legend)
                        st.pyplot(fig)
                        plt.clf()

                # Escolher aﾃｧﾃ｣o seguinte
                next_action = st.selectbox(
                    "Selecione a prﾃｳxima aﾃｧﾃ｣o:",
                    ["Re-Treinar o Modelo", "Finalizar"]
                )

                # Botﾃ｣o de confirmaﾃｧﾃ｣o da escolha
                if st.button("Confirmar Escolha"):
                    if next_action == "Finalizar":
                        st.session_state.step = 'clustering_final_page'
                        st.rerun()
                    elif next_action == "Re-Treinar o Modelo":
                        st.session_state.retrain_mode = True

            # Re-Treinar o Modelo (sﾃｳ aparece se a escolha foi confirmada)
            if st.session_state.get("retrain_mode", False):
                st.write("### Re-Treino do Modelo")
                
                # Escolha do mﾃｩtodo para determinar o nﾃｺmero de clusters
                retrain_method = st.radio(
                    "Escolha a Abordagem para Determinar o Nﾃｺmero de Clusters no novo treino:",
                    ["Automﾃ｡tico", "Manual"]
                )

                if retrain_method == "Manual":
                    st.session_state.num_clusters = st.slider(
                        "Selecione o nﾃｺmero de clusters para o re-treino",
                        min_value=2,
                        max_value=20,
                        value=st.session_state.num_clusters if "num_clusters" in st.session_state else 3
                    )
                    best_n_clusters_retrain = st.session_state.num_clusters

                elif retrain_method == "Automﾃ｡tico":
                    # Determinar o melhor nﾃｺmero de clusters com base no Silhouette Score
                    if silhouette_scores and any(score > 0 for score in silhouette_scores):
                        best_n_clusters_retrain = range(num_clusters_range[0], num_clusters_range[1] + 1)[np.argmax(silhouette_scores)]
                    else:
                        st.error("Nﾃ｣o foi possﾃｭvel determinar automaticamente o nﾃｺmero de clusters. Por favor, selecione manualmente.")
                        best_n_clusters_retrain = 3  # Valor padrﾃ｣o
                        
                # Botﾃ｣o para executar o re-treino
                if st.button("Treinar Novamente"):
                    model = st.session_state.models[st.session_state.selected_model_name]
                    
                    # Preparar modelo
                    if st.session_state.selected_model_name == "Clustering Hierﾃ｡rquico":
                        model.set_params(n_clusters=best_n_clusters_retrain, linkage='ward')
                    else:
                        model.set_params(n_clusters=best_n_clusters_retrain, n_init=5, max_iter=300)
                    
                    # Treinar o modelo com uma barra de progresso
                    with st.spinner(f"Realizando re-treino com {best_n_clusters_retrain} clusters..."):
                        model.fit(st.session_state.training_data)
                    
                    # Calcular mﾃｩtricas
                    st.session_state.retrain_metrics = {
                        "Nﾃｺmero de Clusters": best_n_clusters_retrain,
                        "Silhouette Score": silhouette_score(st.session_state.training_data, model.labels_),
                        "Davies-Bouldin Index": davies_bouldin_score(st.session_state.training_data, model.labels_),
                        "Calinski-Harabasz Score": calinski_harabasz_score(st.session_state.training_data, model.labels_)
                    }
                    
                    # Atualizar rﾃｳtulos dos clusters
                    st.session_state.retrain_labels = model.labels_
                    st.session_state.retrain_completed = True
                    
                    # Mensagem de sucesso
                    if st.session_state.selected_model_name == "Clustering Hierﾃ｡rquico":
                        st.success(f"Re-treino concluﾃｭdo com sucesso com {best_n_clusters_retrain} clusters e {st.session_state.pca_n_components} componentes PCA!")
                    else:
                        st.success(f"Re-treino concluﾃｭdo com sucesso com {best_n_clusters_retrain} clusters!")
                    
                # Exibir mﾃｩtricas do re-treino apﾃｳs a execuﾃｧﾃ｣o
                if st.session_state.get("retrain_completed", False):
                    st.write("### Mﾃｩtricas do Re-Treino")
                    st.table(fix_dataframe_types(pd.DataFrame([st.session_state.retrain_metrics])))
                    
                    # Recuperar o modelo do estado da sessﾃ｣o
                    current_model = st.session_state.models[st.session_state.selected_model_name]

                    # Verificar centroides para KMeans
                    if st.session_state.selected_model_name == "KMeans":
                        if hasattr(current_model, 'cluster_centers_'):
                            st.write("#### Centroides dos Clusters")
                            centroids = current_model.cluster_centers_
                            if centroids.shape[1] > 10:
                                st.write(f"(Mostrando apenas as primeiras 10 dimensﾃｵes de {centroids.shape[1]})")
                                centroids_df = pd.DataFrame(centroids[:, :10])
                            else:
                                centroids_df = pd.DataFrame(centroids)
                            
                            st.dataframe(fix_dataframe_types(centroids_df))
    
                    # Visualizaﾃｧﾃ｣o dos clusters do re-treino
                    if 'retrain_labels' in st.session_state:
                        st.write("### Visualizaﾃｧﾃ｣o dos Clusters do Re-Treino")
                        
                        # Preparar dados para visualizaﾃｧﾃ｣o
                        if st.session_state.selected_model_name == "Clustering Hierﾃ｡rquico":
                            # Para hierﾃ｡rquico, jﾃ｡ temos os dados PCA
                            plot_data = st.session_state.X_pca
                        else:
                            # Para K-means, aplicamos um novo PCA para visualizaﾃｧﾃ｣o
                            # Use os dados originais ou X_scaled
                            X_for_viz = X_scaled  # ou outro conjunto de dados apropriado
                            if X_for_viz.shape[1] > 3:
                                pca_viz = PCA(n_components=3)
                                plot_data = pca_viz.fit_transform(X_for_viz)
                                st.write("(Dados reduzidos via PCA para visualizaﾃｧﾃ｣o)")
                            else:
                                plot_data = X_for_viz
                        
                        # Obter nﾃｺmero total de componentes
                        total_components = plot_data.shape[1]
                        
                        # Permitir escolha de componentes para x e y
                        st.write("### Escolha os Componentes para Visualizaﾃｧﾃ｣o")
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            x_component = st.selectbox(
                                "Componente para o Eixo X", 
                                list(range(total_components)), 
                                index=0,
                                format_func=lambda x: f"Componente {x+1}",
                                key="retrain_x_component"  # Chave ﾃｺnica adicionada
                            )
                        
                        with col2:
                            y_component = st.selectbox(
                                "Componente para o Eixo Y", 
                                list(range(total_components)), 
                                index=1 if total_components > 1 else 0,
                                format_func=lambda x: f"Componente {x+1}",
                                key="retrain_y_component"  # Chave ﾃｺnica adicionada
                            )
                        
                        # Verificar se componentes sﾃ｣o diferentes
                        if x_component == y_component:
                            st.warning("Por favor, selecione componentes diferentes para X e Y.")
                        else:
                            # Visualizaﾃｧﾃ｣o 2D com componentes selecionados
                            fig, ax = plt.subplots(figsize=(10, 6))
                            scatter = ax.scatter(
                                plot_data[:, x_component], 
                                plot_data[:, y_component], 
                                c=st.session_state.retrain_labels, 
                                cmap='viridis', 
                                alpha=0.7
                            )
                            ax.set_title(f'Visualizaﾃｧﾃ｣o 2D dos Clusters do Re-Treino ({best_n_clusters_retrain} clusters)')
                            ax.set_xlabel(f'Componente {x_component+1}')
                            ax.set_ylabel(f'Componente {y_component+1}')
                            legend = ax.legend(*scatter.legend_elements(), title="Clusters")
                            ax.add_artist(legend)
                            st.pyplot(fig)
                            plt.clf()
                            
                # Finalizar apﾃｳs o re-treino
                if st.session_state.get("retrain_completed", False):
                    st.write("## Concluir o Processo de Clustering")
                    if st.button("Seguir para o Relatﾃｳrio"):
                        st.session_state.step = 'clustering_final_page'
                        st.rerun()
                    
    # 3. Seleﾃｧﾃ｣o da Coluna Alvo
    from sklearn.preprocessing import LabelEncoder
    import pandas as pd

    # Inicializar variﾃ｡veis de estado
    if 'bins_confirmed' not in st.session_state:
        st.session_state['bins_confirmed'] = False  # Confirmaﾃｧﾃ｣o dos bins
    if 'bins_value' not in st.session_state:
        st.session_state['bins_value'] = 3  # Valor padrﾃ｣o dos bins

    # Filtrar colunas disponﾃｭveis com base no tipo de modelo
    if st.session_state.model_type == "Classificaﾃｧﾃ｣o":
        valid_columns = [col for col in columns if data[col].dtype in ['object', 'int64'] or data[col].nunique() <= 10]
    else:
        valid_columns = [col for col in columns if data[col].dtype in ['float64', 'int64'] and data[col].nunique() > 10]

    # Seleﾃｧﾃ｣o da Coluna Alvo
    if st.session_state.model_type != "Clustering" and st.session_state.selected_model_name and not st.session_state.target_column_confirmed:
        st.write("Escolha a Coluna Alvo")
        target_column = st.selectbox(
            "Selecione a coluna alvo",
            options=valid_columns,
            key='target_column_selectbox'
        )

        if st.button("Confirmar Coluna Alvo"):
            if target_column in columns:
                st.session_state.target_column = target_column
                st.session_state.target_column_confirmed = True
                st.session_state.validation_method = None
                st.session_state.validation_confirmed = False

                # Processar a coluna alvo
                y = data[st.session_state.target_column]

                # Verificar o tipo de modelo
                model_type = st.session_state.model_type

                # **Se o modelo for de Classificaﾃｧﾃ｣o**
                if model_type == "Classificaﾃｧﾃ｣o":
                    le = LabelEncoder()
                    y_encoded = le.fit_transform(y)
                    st.session_state['target_column_encoded'] = y_encoded
                    st.success("Coluna categﾃｳrica detectada e codificada com LabelEncoder.")

                elif model_type == "Regressﾃ｣o":
                    if y.dtype in ['float64', 'int64']:
                        st.session_state['target_column_encoded'] = y
                        st.success("Coluna contﾃｭnua detectada e pronta para regressﾃ｣o.")
                    else:
                        st.error("Modelos de regressﾃ｣o requerem uma coluna contﾃｭnua como alvo.")
                        st.stop()



    # Exibir a Coluna Alvo Confirmada
    if st.session_state.model_type != "Clustering" and st.session_state.target_column_confirmed:
        st.write(f"Coluna Alvo Confirmada: {st.session_state.target_column}")
        st.write(f"Tipo: {st.session_state.get('target_column_type', 'Nﾃ｣o definido')}")


        # 4. GridSearch
        # Modelos sem hiperparﾃ｢metros ajustﾃ｡veis
        def limpar_parametros_invalidos():
            """Remove parﾃ｢metros invﾃ｡lidos do session_state."""
            if 'manual_params' in st.session_state:
                if 'gamma' in st.session_state['manual_params']:
                    del st.session_state['manual_params']['gamma']  # Remove 'gamma' se presente

        # Inicializa modelos sem hiperparﾃ｢metros ajustﾃ｡veis
        NO_HYPERPARAM_MODELS = ["Regressﾃ｣o Linear Simples (RLS)"]

        # Verifica se o modelo foi selecionado
        if st.session_state.selected_model_name and not st.session_state.grid_search_confirmed:

            # Verificar se o modelo nﾃ｣o possui hiperparﾃ｢metros ajustﾃ｡veis
            if st.session_state.selected_model_name in NO_HYPERPARAM_MODELS:
                st.write(f"O modelo {st.session_state.selected_model_name} nﾃ｣o possui hiperparﾃ｢metros ajustﾃ｡veis.")
                st.session_state.use_grid_search = "Nﾃ｣o"
                param_grid = {}  # Nenhum parﾃ｢metro para ajustar
                st.session_state.grid_search_confirmed = True
            else:
                # Perguntar ao utilizador se quer usar GridSearch
                use_grid_search = st.radio(
                    "Usar GridSearch?", 
                    ["Sim", "Nﾃ｣o"], 
                    key='grid_search_radio', 
                    index=0 if st.session_state.get('use_grid_search', "Sim") == "Sim" else 1
                )
                st.session_state.use_grid_search = use_grid_search

                # Inicializar param_grid como vazio
                param_grid = {}  # Evita erros de variﾃ｡vel nﾃ｣o definida

                if use_grid_search == "Sim":
                    # Perguntar como os parﾃ｢metros devem ser escolhidos
                    param_choice = st.radio(
                        "Escolher os parﾃ｢metros de GridSearch?",
                        ["Utilizar os melhores parﾃ｢metros", "Escolher manualmente os parﾃ｢metros de GridSearch"],
                        key='param_choice_radio',
                        index=0 if st.session_state.get('param_choice', "Utilizar os melhores parﾃ｢metros") == "Utilizar os melhores parﾃ｢metros" else 1
                    )
                    st.session_state.param_choice = param_choice

                    # Inicializar parﾃ｢metros manuais
                    if 'manual_params' not in st.session_state:
                        st.session_state.manual_params = {}

                    manual_params = st.session_state.manual_params

                    # Configuraﾃｧﾃ｣o manual dos parﾃ｢metros
                    if param_choice == "Escolher manualmente os parﾃ｢metros de GridSearch":
                        # Recuperar o modelo selecionado
                        model_key = st.session_state.selected_model_name
                    
                        # Inicializar os parﾃ｢metros padrﾃ｣o do modelo selecionado
                        param_grid = get_default_param_grid(model_key)
                    
                        # Se nﾃ｣o houver parﾃ｢metros padrﾃ｣o, informar o utilizador
                        if not param_grid:
                            st.warning(f"Parﾃ｢metros padrﾃ｣o nﾃ｣o definidos para o modelo {model_key}.")
                            param_grid = {}
                    
                        # Exibir os parﾃ｢metros para o utilizador ajustar manualmente
                        manual_params = {}
                        for param, values in param_grid.items():
                            # **Lﾃｳgica Especial para o Kernel**
                            if param == "kernel":
                                # Selecionar o kernel
                                manual_params[param] = st.selectbox(
                                    f"Escolha o valor para '{param}':",
                                    values,  # Lista de valores permitidos
                                    index=0,  # Primeiro valor como padrﾃ｣o
                                    key=f"{model_key}_{param}"
                                )
                    
                            # **Mostrar 'gamma' apenas se o kernel for 'rbf'**
                            elif param == "gamma":
                                if "kernel" in manual_params and manual_params["kernel"] == "rbf":
                                    # Mostrar gamma apenas para 'rbf'
                                    manual_params[param] = st.selectbox(
                                        f"Escolha o valor para '{param}':",
                                        values,  # Lista de valores permitidos
                                        index=0,  # Primeiro valor como padrﾃ｣o
                                        key=f"{model_key}_{param}"
                                    )
                                else:
                                    # Remover 'gamma' do estado global e local
                                    manual_params.pop(param, None)
                                    if 'manual_params' in st.session_state and param in st.session_state['manual_params']:
                                        del st.session_state['manual_params'][param]
                    
                            # **Tratar parﾃ｢metros numﾃｩricos**
                            elif isinstance(values[0], (int, float)):
                                # Mostrar os valores disponﾃｭveis para o parﾃ｢metro
                                st.write(f"Parﾃ｢metro: **{param}** | Intervalo disponﾃｭvel: [{min(values)}, {max(values)}]")
                            
                                # Verificar o tipo de dado (float ou int) para parametrizaﾃｧﾃ｣o
                                param_type = float if any(isinstance(v, float) for v in values) else int
                            
                                # Criar o nﾃｺmero interativo
                                manual_params[param] = st.number_input(
                                    f"Escolha o valor para '{param}':",
                                    min_value=float(min(values)) if param_type == float else int(min(values)),
                                    max_value=float(max(values)) if param_type == float else int(max(values)),
                                    value=float(values[0]) if param_type == float else int(values[0]),
                                    step=0.1 if param_type == float else 1,  # Ajuste o step dinamicamente
                                    key=f"{model_key}_{param}"
                                )
                            
                            # **Tratar `max_depth` separadamente como um selectbox**
                            elif param == "max_depth":
                                st.write(f"Parﾃ｢metro: **{param}** | Valores disponﾃｭveis: {values}")
                                manual_params[param] = st.selectbox(
                                    f"Escolha o valor para '{param}':",
                                    values,
                                    index=0,  # Primeiro valor como padrﾃ｣o
                                    key=f"{model_key}_{param}"
                                )
                    
                            # **Tratar parﾃ｢metros categﾃｳricos (ex.: 'weights')**
                            elif isinstance(values[0], str):
                                # Mostrar os valores disponﾃｭveis para o parﾃ｢metro
                                st.write(f"Parﾃ｢metro: **{param}** | Valores disponﾃｭveis: {values}")
                            
                                # Criar o selectbox interativo
                                manual_params[param] = st.selectbox(
                                    f"Escolha o valor para '{param}':",
                                    values,  # Lista de valores permitidos
                                    index=0,  # Primeiro valor como padrﾃ｣o
                                    key=f"{model_key}_{param}"
                                )
                    
                        # Salvar os parﾃ｢metros manuais no estado global
                        st.session_state['manual_params'] = manual_params
                        st.write("Parﾃ｢metros manuais salvos:", manual_params)



                # Confirmar configuraﾃｧﾃｵes do GridSearch
                if st.button("Confirmar GridSearch"):
                    st.session_state.grid_search_confirmed = True
                    st.success("Configuraﾃｧﾃ｣o do GridSearch confirmada!")

                    # Parﾃ｢metros padrﾃ｣o atﾃｩ o treino
                    if st.session_state.use_grid_search == "Sim" and st.session_state.param_choice == "Utilizar os melhores parﾃ｢metros":
                        st.session_state['manual_params'] = {}
                        st.session_state['best_params_str'] = "{}"
                        st.session_state['best_params'] = param_grid
                        st.session_state['best_params_selected'] = param_grid
                        

        # 5. Escolha do Mﾃｩtodo de Validaﾃｧﾃ｣o
        # O mﾃｩtodo de validaﾃｧﾃ｣o agora aparece somente apﾃｳs confirmaﾃｧﾃ｣o do GridSearch
        if st.session_state.grid_search_confirmed and st.session_state.selected_model_name and not st.session_state.validation_method:
            st.write("Escolha o Mﾃｩtodo de Validaﾃｧﾃ｣o")
            validation_methods = ["Divisﾃ｣o em Treino e Teste", "Holdout"]
            validation_method = st.radio(
                "Escolha o mﾃｩtodo de validaﾃｧﾃ｣o",
                validation_methods,
                key='validation_method_radio'
            )

            # Configuraﾃｧﾃｵes especﾃｭficas para cada mﾃｩtodo de validaﾃｧﾃ｣o
            if validation_method == "Divisﾃ｣o em Treino e Teste":
                test_size = st.slider(
                    "Proporﾃｧﾃ｣o do conjunto de teste",
                    min_value=0.1, max_value=0.9, value=0.3, step=0.1
                )
                st.session_state.test_size = test_size

            elif validation_method == "Holdout":
                train_size = st.slider(
                    "Proporﾃｧﾃ｣o do conjunto de treino",
                    min_value=0.1, max_value=0.9, value=0.7, step=0.1
                )
                st.session_state.train_size = train_size

            # Botﾃ｣o de confirmaﾃｧﾃ｣o para o mﾃｩtodo de validaﾃｧﾃ｣o
            if st.button("Confirmar Validaﾃｧﾃ｣o"):
                st.session_state.validation_method = validation_method  # Armazena o mﾃｩtodo de validaﾃｧﾃ｣o escolhido

                # Preparaﾃｧﾃ｣o de dados para validaﾃｧﾃ｣o
                X = data.drop(columns=[st.session_state.target_column])
                y = data[st.session_state.target_column]

                # Conversﾃ｣o de variﾃ｡veis categﾃｳricas para numﾃｩricas
                X = pd.get_dummies(X)

                try:
                    # Tratamento de diferentes mﾃｩtodos de validaﾃｧﾃ｣o
                    if st.session_state.validation_method == "Divisﾃ｣o em Treino e Teste":
                        # Divisﾃ｣o simples em treino e teste
                        st.session_state.X_train, st.session_state.X_test, st.session_state.y_train, st.session_state.y_test = train_test_split(
                            X, y, test_size=st.session_state.test_size, random_state=42
                        )
                        st.success("Divisﾃ｣o dos dados realizada com sucesso!")

                    elif st.session_state.validation_method == "Holdout":
                        # Holdout: outra forma de divisﾃ｣o de treino e teste
                        st.session_state.X_train, st.session_state.X_test, st.session_state.y_train, st.session_state.y_test = train_test_split(
                            X, y, train_size=st.session_state.train_size, random_state=42
                        )
                        st.success("Divisﾃ｣o dos dados realizada com sucesso!")

                    # Confirma a validaﾃｧﾃ｣o
                    st.session_state.validation_confirmed = True

                except Exception as e:
                    st.error(f"Erro na divisﾃ｣o dos dados: {e}")

                # Exibir mﾃｩtodo de validaﾃｧﾃ｣o confirmado
                if st.session_state.validation_confirmed:
                    st.write(f"Mﾃｩtodo de Validaﾃｧﾃ｣o Confirmado: {st.session_state.validation_method}")

        # Exibir o botﾃ｣o para treinar o modelo **apenas apﾃｳs a validaﾃｧﾃ｣o ser confirmada**
        # 6. Treino do Modelo
        if st.session_state.validation_confirmed:
            if st.button("Treinar o Modelo"):
                st.session_state.validation_confirmed = False  # Resetando apﾃｳs o treino
                st.success("Treino iniciado com sucesso!")

                # Recuperar o modelo selecionado
                model_name = st.session_state.selected_model_name
                model = st.session_state.models.get(st.session_state.selected_model_name)

                # Verificar se o modelo foi encontrado
                if model is None:
                    st.error(f"Modelo {st.session_state.selected_model_name} nﾃ｣o encontrado.")
                    return  # Interrompe o fluxo caso o modelo nﾃ｣o seja encontrado

                # Inicializar 'treinos_realizados' se necessﾃ｡rio
                if 'treinos_realizados' not in st.session_state:
                    st.session_state['treinos_realizados'] = []

                # Coletar as informaﾃｧﾃｵes armazenadas no session_state
                target_column = st.session_state.target_column
                validation_method = st.session_state.validation_method
                use_grid_search = st.session_state.use_grid_search
                manual_params = st.session_state.manual_params
                X_train = st.session_state.X_train
                y_train = st.session_state.y_train
                X_test = st.session_state.X_test
                y_test = st.session_state.y_test

                # **Remover parﾃ｢metros invﾃ｡lidos antes do treino**
                if 'manual_params' in st.session_state:
                    if manual_params.get('kernel') == 'linear' and 'gamma' in manual_params:
                        del manual_params['gamma']  # Remove o parﾃ｢metro local
                    if 'gamma' in st.session_state['manual_params']:
                        del st.session_state['manual_params']['gamma']  # Remove do estado global

                # **Adicionar tratamento de valores ausentes**
                from sklearn.impute import SimpleImputer

                imputer = SimpleImputer(strategy="mean")  # Ou "median" conforme necessﾃ｡rio
                X_train = imputer.fit_transform(X_train)  # Tratamento no conjunto de treino
                X_test = imputer.transform(X_test)        # Tratamento no conjunto de teste

                # Exibir resumo das escolhas feitas antes do treino
                st.write("### Resumo das Escolhas Feitas:")
                st.write(f"**Modelo Selecionado**: {model_name}")
                st.write(f"**Coluna Alvo**: {target_column}")
                st.write(f"**Mﾃｩtodo de Validaﾃｧﾃ｣o**: {validation_method}")
                st.write(f"GridSearch Ativado? {use_grid_search}")  # Debug para verificar a escolha do utilizador

                # Treino de um ﾃｺnico modelo
                param_grid = get_default_param_grid(model_name) if use_grid_search == "Sim" else {}
                resultado = train_and_evaluate(
                    model, param_grid, X_train, y_train, X_test, y_test, use_grid_search, manual_params
                )

                # **Salvar apenas os parﾃ｢metros vﾃ｡lidos no estado global apﾃｳs o treino**
                if 'Best Parameters' in resultado:
                    st.session_state['best_params'] = resultado['Best Parameters']  # Para treino inicial
                    st.session_state['best_params_selected'] = resultado['Best Parameters']  # Para seleﾃｧﾃ｣o de features
                    st.session_state['best_params_str'] = json.dumps(st.session_state['best_params'], indent=2)
                    st.write("Parﾃ｢metros salvos no estado global:", st.session_state['best_params'])
                else:
                    st.warning("Nenhum parﾃ｢metro encontrado para salvar.")

                # Apﾃｳs o primeiro treino
                # Apﾃｳs o primeiro treino
                if resultado:
                    # Armazena os resultados iniciais para comparaﾃｧﾃ｣o futura
                    st.session_state['resultado_sem_selecao'] = resultado  # Salva os resultados sem seleﾃｧﾃ｣o
                    st.session_state['treinos_realizados'].append(resultado)
                    
                    # Criar o DataFrame com as mﾃｩtricas
                    df_resultado = pd.DataFrame([resultado])
                
                    # Corrigir os tipos antes de formatar
                    df_corrigido = fix_dataframe_types(df_resultado)
                    
                    # Aplicar formataﾃｧﾃ｣o depois de corrigir os tipos
                    st.write("Mﾃｩtricas do modelo treinado:")
                    formatted_display = df_corrigido.style.format(
                        {col: "{:.4f}" for col in df_corrigido.select_dtypes(include=['float', 'float64']).columns}
                    )
                    st.dataframe(formatted_display)
                
                    # Grﾃ｡fico das mﾃｩtricas
                    plot_metrics(df_corrigido)
                
                    # Marcar o treino como concluﾃｭdo
                    st.session_state['treino_concluido'] = True
                else:
                    st.error("O treino do modelo falhou.")

        # Avanﾃｧar para Seleﾃｧﾃ｣o de Features SOMENTE apﾃｳs o grﾃ｡fico de mﾃｩtricas ser mostrado
        if st.session_state.get('treino_concluido', False):
            st.write("### Avanﾃｧar para Seleﾃｧﾃ｣o de Features")

            # Garantir que hﾃ｡ treinos realizados
            if 'treinos_realizados' in st.session_state and st.session_state['treinos_realizados']:
                # Depuraﾃｧﾃ｣o: Exibir treinos realizados
                #st.write("Treinos realizados:", st.session_state['treinos_realizados'])

                # Identificar o tipo de problema para usar a mﾃｩtrica apropriada
                if st.session_state.model_type == "Classificaﾃｧﾃ｣o":
                    melhores_metricas = sorted(
                        st.session_state['treinos_realizados'], 
                        key=lambda x: x.get('Accuracy', 0),  # Usar Accuracy para classificaﾃｧﾃ｣o
                        reverse=True
                    )[0]  # Escolher o melhor modelo
                elif st.session_state.model_type == "Regressﾃ｣o":
                    melhores_metricas = sorted(
                        st.session_state['treinos_realizados'], 
                        key=lambda x: x.get('Rﾂｲ', 0),  # Usar Rﾂｲ para regressﾃ｣o
                        reverse=True
                    )[0]  # Escolher o melhor modelo

                # Seleﾃｧﾃ｣o de modelo manual ou manter o melhor automaticamente
                model_options = [resultado['Modelo'] for resultado in st.session_state['treinos_realizados']]
                default_index = model_options.index(melhores_metricas['Modelo']) if melhores_metricas['Modelo'] in model_options else 0

                selected_model_temp = st.selectbox(
                    "Escolha um modelo para avanﾃｧar para a Seleﾃｧﾃ｣o de Features:",
                    options=model_options,
                    index=default_index
                )

                # Botﾃ｣o para avanﾃｧar
                if st.button("Avanﾃｧar para Seleﾃｧﾃ｣o de Features"):
                    # Atualizar o modelo selecionado no session_state apenas ao clicar no botﾃ｣o
                    st.session_state.selected_model_name = selected_model_temp
                    st.session_state.step = 'feature_selection'
                    st.session_state['treino_concluido'] = False
                    st.rerun()
            else:
                st.error("Nenhum modelo foi treinado. Execute o treino primeiro.")


# Funﾃｧﾃ｣o para treinar e avaliar os modelos de clustering
def train_clustering_model(model, X_data, model_name):
    try:
        # Padronizar os dados
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X_data)
        
        if model_name == "KMeans":
            model.set_params(n_clusters=st.session_state.kmeans_clusters)
            model.fit(X_scaled)
            st.session_state['labels'] = model.labels_
        
        elif model_name == "Clustering Hierﾃ｡rquico":
            # Configurar explicitamente todos os parﾃ｢metros necessﾃ｡rios
            model.set_params(n_clusters=st.session_state.kmeans_clusters, linkage="ward")
            model.fit(X_scaled)
            st.session_state['labels'] = model.labels_
        
        st.write(f"Clusterizaﾃｧﾃ｣o realizada com {model_name}")
        
    except Exception as e:
        st.error(f"Erro ao treinar o modelo {model_name}: {str(e)}")
# Visualizaﾃｧﾃ｣o dos Clusters usando PCA
def visualize_clusters(X_data):
    if 'labels' in st.session_state:
        pca = PCA(n_components=2)
        X_pca = pca.fit_transform(X_data)

        plt.figure(figsize=(8, 6))
        plt.scatter(X_pca[:, 0], X_pca[:, 1], c=st.session_state['labels'], cmap='viridis')
        plt.xlabel('Componente Principal 1')
        plt.ylabel('Componente Principal 2')
        plt.title('Visualizaﾃｧﾃ｣o dos Clusters em 2D')
        st.pyplot(plt.gcf())

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

def evaluate_regression_model(y_true, y_pred):
    r2 = r2_score(y_true, y_pred)
    mae = mean_absolute_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    return {"Rﾂｲ": r2, "MAE": mae,"MSE": mse }

def train_and_evaluate(model, param_grid, X_train, y_train, X_test, y_test, use_grid_search, manual_params=None):
    try:
        # Verificaﾃｧﾃｵes para tipos de modelos
        is_svr = isinstance(model, SVR)
        is_svc = isinstance(model, SVC)  # Adicionar verificaﾃｧﾃ｣o para SVC
        is_regression = is_svr or isinstance(model, LinearRegression)

        # Escalonamento para SVR
        if is_svr:
            scaler = StandardScaler()
            X_train = scaler.fit_transform(X_train)
            X_test = scaler.transform(X_test)

        # GridSearch otimizado
        if use_grid_search:
            cv = KFold(n_splits=5, shuffle=True, random_state=42)
            scoring = 'r2' if is_regression else 'accuracy'
            
            # Tratamento especial para SVC (muito mais rﾃ｡pido)
            if is_svc:
                # Grid reduzido para SVC
                simplified_grid = {
                    'C': [1],            # Apenas um valor para C
                    'kernel': ['rbf'],   # Apenas um tipo de kernel
                    'gamma': ['scale']   # Apenas uma configuraﾃｧﾃ｣o de gamma
                }
                
                # Aplicar parﾃ｢metros manuais, se fornecidos
                if manual_params:
                    for param, value in manual_params.items():
                        simplified_grid[param] = [value]
                        
                # Usar o grid simplificado
                actual_grid = simplified_grid
                
                # Reduzir nﾃｺmero de folds para SVC
                cv = KFold(n_splits=3, shuffle=True, random_state=42)
            else:
                # Para outros modelos, usar o grid original
                actual_grid = param_grid
                
                # Incorporar parﾃ｢metros manuais
                if manual_params:
                    actual_grid.update({k: [v] for k, v in manual_params.items()})
            
            # Executar GridSearch com os parﾃ｢metros apropriados
            grid_search = GridSearchCV(
                model, 
                actual_grid,
                cv=cv,
                scoring=scoring,
                n_jobs=-1  # Utilizar todos os cores disponﾃｭveis
            )
            grid_search.fit(X_train, y_train)
            
            best_model = grid_search.best_estimator_
            best_params = grid_search.best_params_
        else:
            # Treinar sem GridSearch
            if manual_params:
                model.set_params(**manual_params)
            
            model.fit(X_train, y_train)
            best_model = model
            best_params = manual_params or {}

        # Prediﾃｧﾃｵes
        y_pred = best_model.predict(X_test)

        # Mﾃｩtricas baseadas no tipo de modelo
        metrics = {
            "Modelo": model.__class__.__name__,
            **(
                {
                    "Rﾂｲ": r2_score(y_test, y_pred),
                    "MAE": mean_absolute_error(y_test, y_pred),
                    "MSE": mean_squared_error(y_test, y_pred)
                } if is_regression else 
                {
                    "Accuracy": accuracy_score(y_test, y_pred),
                    "Precision": precision_score(y_test, y_pred, average='weighted'),
                    "Recall": recall_score(y_test, y_pred, average='weighted'),
                    "F1-Score": f1_score(y_test, y_pred, average='weighted')
                }
            ),
            "Best Parameters": best_params
        }

        return metrics

    except Exception as e:
        st.error(f"Erro ao treinar o modelo: {str(e)}")
        return None

# Funﾃｧﾃ｣o para selecionar o scoring
def select_scoring():
    # Verifica se 'selected_scoring' jﾃ｡ existe, caso contrﾃ｡rio, inicializa com 'f1' como padrﾃ｣o
    if 'selected_scoring' not in st.session_state:
        st.session_state.selected_scoring = 'F1-Score'  # Definir 'f1' como valor padrﾃ｣o

    # Agora o selectbox usa o valor jﾃ｡ armazenado em 'selected_scoring'
    st.session_state.selected_scoring = st.selectbox(
        "Escolha o scoring para a seleﾃｧﾃ｣o de features:",
        ['Accuracy', 'Precision', 'Recall', 'F1-Score'],
        index=['Accuracy', 'Precision', 'Recall', 'F1-Score'].index(st.session_state.selected_scoring)
    )

    # Exibir a escolha armazenada
    st.write("Scoring selecionado:", st.session_state.selected_scoring)

    # Salvar em um arquivo ou variﾃ｡vel para persistﾃｪncia adicional
    if st.button("Salvar escolha"):
        with open("scoring_choice.txt", "w") as file:
            file.write(st.session_state.selected_scoring)
        st.success("Escolha salva com sucesso!")


# Funﾃｧﾃ｣o para remover features correlacionadas
def remove_highly_correlated_features(df, threshold=0.9):
    """
    Remove features altamente correlacionadas.
    
    Parﾃ｢metros:
    - df: DataFrame de entrada
    - threshold: Limiar de correlaﾃｧﾃ｣o (padrﾃ｣o 0.9)
    
    Retorna:
    - DataFrame com features nﾃ｣o correlacionadas
    """
    # Calcular matriz de correlaﾃｧﾃ｣o absoluta
    corr_matrix = df.corr().abs()
    
    # Obter a matriz triangular superior
    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    
    # Identificar colunas a serem removidas
    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]
    
    # Informar quais features serﾃ｣o removidas (opcional)
    if to_drop:
        st.info(f"Features removidas por alta correlaﾃｧﾃ｣o: {to_drop}")
    
    # Retornar DataFrame sem as features correlacionadas
    return df.drop(columns=to_drop)


# Funﾃｧﾃ｣o para selecionar features importantes com RandomForest
def select_important_features(X, y, threshold=0.01, model_type=None):
    """
    Seleciona features importantes usando RandomForest.
    
    Parﾃ｢metros:
    - X: Matriz de features
    - y: Vetor de rﾃｳtulos
    - threshold: Limiar de importﾃ｢ncia (padrﾃ｣o 0.01)
    - model_type: Tipo de modelo (Classificaﾃｧﾃ｣o ou Regressﾃ｣o)
    
    Retorna:
    - DataFrame com features importantes
    """
    # Definir o modelo baseado no tipo de problema
    if model_type == "Classificaﾃｧﾃ｣o":
        model = RandomForestClassifier(n_estimators=100, random_state=42)
    elif model_type == "Regressﾃ｣o":
        model = RandomForestRegressor(n_estimators=100, random_state=42)
    else:
        raise ValueError("Tipo de modelo deve ser 'Classificaﾃｧﾃ｣o' ou 'Regressﾃ｣o'")
    
    # Usar SimpleImputer para lidar com valores ausentes
    imputer = SimpleImputer(strategy='mean')
    X_imputed = imputer.fit_transform(X)
    
    # Treinar o modelo
    model.fit(X_imputed, y)
    
    # Calcular importﾃ｢ncia das features
    importances = model.feature_importances_
    
    # Criar DataFrame de importﾃ｢ncia das features
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': importances
    }).sort_values('importance', ascending=False)
    
    # Selecionar features com importﾃ｢ncia acima do threshold
    important_features = feature_importance[feature_importance['importance'] > threshold]['feature']
    
    # Informar quais features foram selecionadas
    st.info(f"Features selecionadas: {list(important_features)}")
    
    return X[important_features]


# Funﾃｧﾃ｣o principal de seleﾃｧﾃ｣o de features
def feature_selection():
    st.header("Seleﾃｧﾃ｣o de Features")
    
    if 'feature_selection_done' not in st.session_state:
        st.session_state.feature_selection_done = False
    
    model_type = st.session_state.get('model_type', 'Classificaﾃｧﾃ｣o')
    scoring_options = {"Classificaﾃｧﾃ｣o": ['Accuracy', 'Precision', 'Recall', 'F1-Score'], "Regressﾃ｣o": ['Rﾂｲ', 'MAE', 'MSE']}
    
    selected_scoring = st.selectbox("Escolha a mﾃｩtrica de scoring:", scoring_options.get(model_type, []))
    
    if st.button("Confirmar Scoring"):
        st.session_state.selected_scoring = selected_scoring
        st.session_state.scoring_confirmed = True
        st.success(f"Mﾃｩtrica de scoring {selected_scoring} confirmada!")
    
    if st.session_state.scoring_confirmed:
        method_selection = st.radio("Escolha o mﾃｩtodo de seleﾃｧﾃ｣o de features:", ["Automﾃ｡tico", "Manual"])
        
        if st.button("Confirmar Mﾃｩtodo"):
            st.session_state.method_selection = method_selection
            st.success(f"Mﾃｩtodo {method_selection} confirmado!")

        X_train, X_test, y_train, y_test = st.session_state.X_train, st.session_state.X_test, st.session_state.y_train, st.session_state.y_test
        
        if method_selection == "Automﾃ｡tico":
            feature_selector = RandomForestClassifier(n_estimators=100, random_state=42) if model_type == "Classificaﾃｧﾃ｣o" else RandomForestRegressor(n_estimators=100, random_state=42)
            feature_selector.fit(X_train, y_train)
            
            feature_importances = pd.DataFrame({'feature': X_train.columns, 'importance': feature_selector.feature_importances_}).sort_values('importance', ascending=False)
            st.dataframe(feature_importances)
            
            selected_features = feature_importances[feature_importances['importance'] > 0.01]['feature'].tolist()
        else:
            feature_selector = RandomForestClassifier(n_estimators=100, random_state=42) if model_type == "Classificaﾃｧﾃ｣o" else RandomForestRegressor(n_estimators=100, random_state=42)
            feature_selector.fit(X_train, y_train)
            
            feature_importances = pd.DataFrame({'feature': X_train.columns, 'importance': feature_selector.feature_importances_}).sort_values('importance', ascending=False)
            st.dataframe(feature_importances)
            
            num_features = st.slider("Nﾃｺmero de Features a Selecionar", 1, X_train.shape[1], min(5, X_train.shape[1]))
            selected_features = feature_importances['feature'].head(num_features).tolist()

        st.session_state.X_train_selected = X_train[selected_features]
        st.session_state.X_test_selected = X_test[selected_features]
        st.session_state.selected_features = selected_features
        st.session_state.feature_selection_done = True

        if st.button("Treinar Modelo com Features Selecionadas"):
            st.session_state.step = 'train_with_selected_features'
            st.rerun()

def train_with_selected_features_page():
    st.title("Treino do Modelo com Features Selecionadas")
    
    # Mapeamento de modelos bidirecional
    model_name_map = {
        "SVC": "Support Vector Classification (SVC)",
        "KNeighborsClassifier": "K-Nearest Neighbors (KNN)",
        "RandomForestClassifier": "Random Forest",
        "LinearRegression": "Regressﾃ｣o Linear Simples (RLS)",
        "SVR": "Regressﾃ｣o por Vetores de Suporte (SVR)",
        "Support Vector Classification (SVC)": "SVC",
        "K-Nearest Neighbors (KNN)": "KNeighborsClassifier", 
        "Random Forest": "RandomForestClassifier",
        "Regressﾃ｣o Linear Simples (RLS)": "LinearRegression",
        "Regressﾃ｣o por Vetores de Suporte (SVR)": "SVR"
    }
    
    if 'models' not in st.session_state or not st.session_state.models:
        st.error("Erro: Nenhum modelo foi treinado ou selecionado.")
        return

    if 'selected_model_name' not in st.session_state or not st.session_state.selected_model_name:
        st.error("Nenhum modelo foi selecionado. Por favor, selecione um modelo antes de continuar.")
        return

    selected_model_name = st.session_state.selected_model_name.strip()
    model_class_name = model_name_map.get(selected_model_name, selected_model_name)

    if model_class_name not in st.session_state.models:
        st.error(f"O modelo '{selected_model_name}' nﾃ｣o foi encontrado na sessﾃ｣o.")
        st.write("Modelos disponﾃｭveis:", list(st.session_state.models.keys()))
        return

    model = st.session_state.models[model_class_name]
    
    X_train_selected, X_test_selected = st.session_state.X_train_selected, st.session_state.X_test_selected
    y_train, y_test = st.session_state.y_train, st.session_state.y_test
    
    st.write(f"Treinando o modelo {selected_model_name} com {len(st.session_state.selected_features)} features selecionadas...")
    
    selected_metrics = train_and_store_metrics(model, X_train_selected, y_train, X_test_selected, y_test, "Com Seleﾃｧﾃ｣o", False)
    
    if selected_metrics:
        st.session_state['resultado_com_selecao'] = selected_metrics
        st.success("Treinamento concluﾃｭdo!")
        
        st.subheader("Mﾃｩtricas do Modelo com Features Selecionadas")
        metrics_df = pd.DataFrame([selected_metrics])
        metrics_df.insert(0, "Modelo", "Com Seleﾃｧﾃ｣o de Features")
        st.table(metrics_df)
    
    if st.button("Comparar Modelos"):
        st.session_state.step = 'evaluate_and_compare_models'
        st.rerun()

#Funﾃｧﾃ｣o para Treinar e Armazenar as metricas

def train_and_store_metrics(model, X_train, y_train, X_test, y_test, metric_type, use_grid_search=False, manual_params=None):
    try:
        # Imports necessﾃ｡rios
        from sklearn.impute import SimpleImputer
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
        from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
        from sklearn.model_selection import GridSearchCV, KFold

        # Imputar valores ausentes
        imputer = SimpleImputer(strategy="mean")
        X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)
        X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)

        # Garantir que y_train e y_test sejam vﾃ｡lidos
        if y_train.dtype == 'object':
            from sklearn.preprocessing import LabelEncoder
            le = LabelEncoder()
            y_train = le.fit_transform(y_train)
            y_test = le.transform(y_test)
        else:
            y_train = y_train.fillna(y_train.mean())
            y_test = y_test.fillna(y_test.mean())

        # **RECUPERAR PARﾃMETROS SALVOS**
        if metric_type == "Com Seleﾃｧﾃ｣o":
            saved_params = st.session_state.get('best_params_selected', None) or st.session_state.get('best_params', None)
        else:
            saved_params = st.session_state.get('best_params', None)

        # **APLICAR PARﾃMETROS SALVOS APENAS SE COMPATﾃ昂EIS COM O MODELO**
        if saved_params and hasattr(model, 'get_params') and all(param in model.get_params() for param in saved_params):
            st.info(f"Aplicando parﾃ｢metros salvos ao modelo: {saved_params}")
            model.set_params(**saved_params)


        # **TREINO COM GRIDSEARCH OU DIRETO**
        if use_grid_search and metric_type == "Sem Seleﾃｧﾃ｣o":
            param_grid = st.session_state.get('param_grid', {
                'n_neighbors': [3, 5, 7, 9],
                'weights': ['uniform', 'distance']
            })

            # Definir estratﾃｩgia de validaﾃｧﾃ｣o cruzada
            cv_strategy = KFold(n_splits=5, shuffle=True, random_state=42)
            if st.session_state.model_type == "Classificaﾃｧﾃ｣o":
                scoring = 'accuracy'
            else:
                scoring = 'r2'

            # Aplicar GridSearch
            grid_search = GridSearchCV(model, param_grid, scoring=scoring, cv=cv_strategy, n_jobs=-1)
            grid_search.fit(X_train, y_train)

            best_model = grid_search.best_estimator_
            best_params = grid_search.best_params_

            # **SALVAR PARﾃMETROS NO ESTADO GLOBAL**
            st.session_state['best_params'] = best_params
            st.session_state['best_params_selected'] = best_params

        else:
            model.fit(X_train, y_train)
            best_model = model
            best_params = saved_params if saved_params else {}

        # **SALVAR MODELO TREINADO NO ESTADO GLOBAL**
        st.session_state['trained_model'] = best_model
        st.session_state['trained_model_name'] = best_model.__class__.__name__
        

        # **AVALIAR O MODELO**
        y_pred = best_model.predict(X_test)

        if st.session_state.model_type == "Classificaﾃｧﾃ｣o":
            metrics = {
                'F1-Score': f1_score(y_test, y_pred, average='weighted'),
                'Precision': precision_score(y_test, y_pred, average='weighted'),
                'Recall': recall_score(y_test, y_pred, average='weighted'),
                'Accuracy': accuracy_score(y_test, y_pred),
                'Best Parameters': best_params
            }
        else:
            metrics = {
                'Rﾂｲ': r2_score(y_test, y_pred),
                'MSE': mean_squared_error(y_test, y_pred),
                'MAE': mean_absolute_error(y_test, y_pred),
                'Best Parameters': best_params
            }

        # **SALVAR Mﾃ欝RICAS NO ESTADO GLOBAL**
        if 'metrics' not in st.session_state:
            st.session_state['metrics'] = {}
        st.session_state['metrics'][metric_type] = metrics

        return metrics

    except Exception as e:
        st.error(f"Erro ao treinar o modelo: {str(e)}")
        return None

def evaluate_and_compare_models():
    st.title("Comparaﾃｧﾃ｣o dos Resultados do Treino dos Modelos")

    # Mapeamento de modelos bidirecional
    model_name_map = {
        "SVC": "Support Vector Classification (SVC)",
        "KNeighborsClassifier": "K-Nearest Neighbors (KNN)",
        "RandomForestClassifier": "Random Forest",
        "LinearRegression": "Regressﾃ｣o Linear Simples (RLS)",
        "SVR": "Regressﾃ｣o por Vetores de Suporte (SVR)",
        "Support Vector Classification (SVC)": "SVC",
        "K-Nearest Neighbors (KNN)": "KNeighborsClassifier", 
        "Random Forest": "RandomForestClassifier",
        "Regressﾃ｣o Linear Simples (RLS)": "LinearRegression",
        "Regressﾃ｣o por Vetores de Suporte (SVR)": "SVR"
    }

    # Verificaﾃｧﾃｵes preliminares
    if 'selected_features' not in st.session_state:
        st.error("Nenhuma feature foi selecionada. Por favor, volte ﾃ etapa de seleﾃｧﾃ｣o de features.")
        return

    # Verificar se os modelos estﾃ｣o definidos  
    if 'models' not in st.session_state or not st.session_state.models:
        st.error("Configuraﾃｧﾃ｣o de modelos nﾃ｣o encontrada. Por favor, reinicie o processo de seleﾃｧﾃ｣o de modelos.")
        return

    # Recuperar o tipo de modelo
    model_type = st.session_state.get('model_type', 'Indefinido')

    # Recuperar a mﾃｩtrica escolhida pelo utilizador para seleﾃｧﾃ｣o de features
    scoring_metric = st.session_state.get("selected_scoring", None)
    if not scoring_metric:
        st.error("Nenhuma mﾃｩtrica de avaliaﾃｧﾃ｣o foi escolhida. Por favor, volte ﾃ etapa de seleﾃｧﾃ｣o de mﾃｩtricas.")
        return

    # Recuperar o nome do modelo selecionado
    model_name = st.session_state.get('selected_model_name')
    if not model_name:
        st.error("Nenhum modelo foi selecionado. Por favor, volte ﾃ etapa de seleﾃｧﾃ｣o de modelos.")
        return

    # Encontrar o nome correto do modelo a partir do mapeamento
    model_class_name = model_name_map.get(model_name)
    if model_class_name is None:
        st.error(f"O modelo {model_name} nﾃ｣o foi encontrado na lista de modelos disponﾃｭveis.")
        st.write("Modelos disponﾃｭveis:", list(model_name_map.keys()))
        return

    # Recuperar o modelo da sessﾃ｣o com base no nome correto da classe
    model = st.session_state.models.get(model_class_name)
    if model is None:
        st.error(f"O modelo {model_class_name} nﾃ｣o foi encontrado na sessﾃ｣o.")
        st.write("Modelos disponﾃｭveis:", list(st.session_state.models.keys()))
        return

    # Recuperar mﾃｩtricas originais e com seleﾃｧﾃ｣o de features
    original_metrics = st.session_state.get('resultado_sem_selecao', {}) 
    selected_metrics = st.session_state.get('resultado_com_selecao', {})

    # Verificar se as mﾃｩtricas existem
    if not original_metrics:
        st.error("Nﾃ｣o foi possﾃｭvel encontrar as mﾃｩtricas originais. Por favor, refaﾃｧa o treinamento.")
        return
        
    if not selected_metrics:
        st.error("Nﾃ｣o foi possﾃｭvel encontrar as mﾃｩtricas com seleﾃｧﾃ｣o de features. Por favor, execute o treino com features selecionadas.")
        return

    # Criar DataFrame de comparaﾃｧﾃ｣o
    if model_type == "Classificaﾃｧﾃ｣o":
        comparison_df = pd.DataFrame({
            'Modelo': ['Sem Seleﾃｧﾃ｣o de Features', 'Com Seleﾃｧﾃ｣o de Features'],
            'Accuracy': [original_metrics.get('Accuracy', 0), selected_metrics.get('Accuracy', 0)],
            'Precision': [original_metrics.get('Precision', 0), selected_metrics.get('Precision', 0)],
            'Recall': [original_metrics.get('Recall', 0), selected_metrics.get('Recall', 0)],
            'F1-Score': [original_metrics.get('F1-Score', 0), selected_metrics.get('F1-Score', 0)],
            'Best Parameters': [original_metrics.get('Best Parameters', 'N/A'), selected_metrics.get('Best Parameters', 'N/A')]
        })
    elif model_type == "Regressﾃ｣o":
        comparison_df = pd.DataFrame({
            'Modelo': ['Sem Seleﾃｧﾃ｣o de Features', 'Com Seleﾃｧﾃ｣o de Features'],
            'Rﾂｲ': [original_metrics.get('Rﾂｲ', 0), selected_metrics.get('Rﾂｲ', 0)],
            'MAE': [original_metrics.get('MAE', 0), selected_metrics.get('MAE', 0)],
            'MSE': [original_metrics.get('MSE', 0), selected_metrics.get('MSE', 0)],
            'Best Parameters': [original_metrics.get('Best Parameters', 'N/A'), selected_metrics.get('Best Parameters', 'N/A')]
        })
    else:
        st.error(f"Tipo de modelo nﾃ｣o reconhecido: {model_type}")
        return

    # Exibir tabela de comparaﾃｧﾃ｣o
    st.subheader("嶋 Comparaﾃｧﾃ｣o dos Resultados:")
    
    # Formatar todas as colunas numﾃｩricas
    format_dict = {}
    for col in comparison_df.columns:
        if col != 'Modelo' and col != 'Best Parameters':
            format_dict[col] = '{:.4f}'
    
    st.dataframe(
    comparison_df.style.format(format_dict).set_table_styles(
        [{'selector': 'th', 'props': [('font-size', '18px')]}, 
         {'selector': 'td', 'props': [('font-size', '12px')]},  
         {'selector': 'table', 'props': [('width', '100%')]},    
        ]
    )
)
    
    # Determinar as mﾃｩtricas disponﾃｭveis com base no tipo de modelo
    if model_type == "Classificaﾃｧﾃ｣o":
        metric_columns = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
    elif model_type == "Regressﾃ｣o":
        metric_columns = ['Rﾂｲ', 'MAE', 'MSE']
    else:
        metric_columns = []
    
    # Garantir que a mﾃｩtrica escolhida existe nas colunas disponﾃｭveis
    if scoring_metric not in metric_columns:
        st.warning(f"A mﾃｩtrica selecionada '{scoring_metric}' nﾃ｣o estﾃ｡ disponﾃｭvel. Usando a primeira mﾃｩtrica disponﾃｭvel.")
        scoring_metric = metric_columns[0] if metric_columns else None
    
    if scoring_metric:
        # Grﾃ｡fico de comparaﾃｧﾃ｣o usando a mﾃｩtrica escolhida pelo utilizador

        x = comparison_df['Modelo']
        y1 = comparison_df[scoring_metric].iloc[0]  # Sem Seleﾃｧﾃ｣o de Features (ﾃｭndice 0)
        y2 = comparison_df[scoring_metric].iloc[1]  # Com Seleﾃｧﾃ｣o de Features (ﾃｭndice 1)

        # Grﾃ｡fico de comparaﾃｧﾃ｣o com melhorias no layout e visibilidade dos rﾃｳtulos
        fig, ax = plt.subplots(figsize=(10, 6))

        # Posiﾃｧﾃｵes das barras
        x_pos = [0, 1]  # Definindo a posiﾃｧﾃ｣o das barras para garantir que fiquem ao lado
        width = 0.4  # Largura das barras

        # Ajustar as barras para uma boa visibilidade
        bars1 = ax.bar(x_pos[0], y1, width=width, label="Sem Seleﾃｧﾃ｣o de Features", color='#90EE90', align='center')
        bars2 = ax.bar(x_pos[1], y2, width=width, label="Com Seleﾃｧﾃ｣o de Features", color='#006400', align='center')

        # Adicionar rﾃｳtulos de valor nas barras com melhorias
        for bar in bars1:
            height = bar.get_height()
            ax.annotate(f'{height:.4f}',
                        xy=(bar.get_x() + bar.get_width() / 2, height),
                        xytext=(0, 3),
                        textcoords="offset points",
                        ha='center', va='bottom',
                        fontsize=12, color='black')

        for bar in bars2:
            height = bar.get_height()
            ax.annotate(f'{height:.4f}',
                        xy=(bar.get_x() + bar.get_width() / 2, height),
                        xytext=(0, 3),
                        textcoords="offset points",
                        ha='center', va='bottom',
                        fontsize=12, color='black')  # Altere a cor para garantir contraste

        # Melhorando o tﾃｭtulo e as labels
        ax.set_title(f'Comparaﾃｧﾃ｣o de {scoring_metric}', fontsize=16, fontweight='bold')
        ax.set_ylabel(scoring_metric, fontsize=14)
        ax.set_xlabel("Modelos", fontsize=14)

        # Ajuste nos rﾃｳtulos do eixo X e Y
        plt.xticks(x_pos, ['Sem Seleﾃｧﾃ｣o de Features', 'Com Seleﾃｧﾃ｣o de Features'], fontsize=12)
        plt.yticks(fontsize=12)

        # Legenda
        ax.legend()

        # Ajuste do layout para garantir que tudo fique visﾃｭvel
        plt.tight_layout()

        # Exibir o grﾃ｡fico
        st.pyplot(fig)


    # Determinar o melhor modelo baseado na mﾃｩtrica escolhida
    if scoring_metric:
        score_without = comparison_df[scoring_metric].iloc[0]
        score_with = comparison_df[scoring_metric].iloc[1]
        
        better_model = "Com Seleﾃｧﾃ｣o de Features" if score_with > score_without else "Sem Seleﾃｧﾃ｣o de Features"
        better_score = max(score_with, score_without)
        
        st.success(f"醇 **Melhor modelo:** {better_model} com {scoring_metric} = {better_score:.4f}")
    
    # Botﾃ｣o para prﾃｳxima etapa
    if st.button("Seguir para Resumo Final", key="btn_resumo_final"):
        st.session_state.step = 'final_page'
        st.rerun()

# Funﾃｧﾃ｣o para gerar interpretaﾃｧﾃ｣o personalizada das mﾃｩtricas
def generate_metrics_interpretation(metrics):
    """Funﾃｧﾃ｣o para gerar interpretaﾃｧﾃ｣o personalizada das mﾃｩtricas"""
    interpretacao = []

    # Verificar se as mﾃｩtricas estﾃ｣o no formato esperado
    if not isinstance(metrics, dict):
        return "Formato de mﾃｩtricas invﾃ｡lido."

    # Accuracy
    if 'Accuracy' in metrics:
        try:
            accuracy = float(metrics['Accuracy'])
            if accuracy > 0.9:
                interpretacao.append(f"- Acurﾃ｡cia: {accuracy:.4f} - Excelente! O modelo tem uma taxa de acerto global muito elevada.")
            elif accuracy > 0.75:
                interpretacao.append(f"- Acurﾃ｡cia: {accuracy:.4f} - Boa. O modelo estﾃ｡ a funcionar bem, mas ainda hﾃ｡ margem para otimizaﾃｧﾃ｣o.")
            elif accuracy > 0.5:
                interpretacao.append(f"- Acurﾃ｡cia: {accuracy:.4f} - Moderada. Os erros ainda sﾃ｣o significativos e devem ser corrigidos.")
            else:
                interpretacao.append(f"- Acurﾃ｡cia: {accuracy:.4f} - Fraca. O modelo estﾃ｡ a falhar em muitas previsﾃｵes e precisa de ser revisto.")
        except (ValueError, TypeError):
            interpretacao.append("- Acurﾃ｡cia: Nﾃ｣o disponﾃｭvel ou invﾃ｡lida.")

    # Precision
    if 'Precision' in metrics:
        try:
            precision = float(metrics['Precision'])
            if precision > 0.9:
                interpretacao.append(f"- Precisﾃ｣o: {precision:.4f} - Excelente! O modelo estﾃ｡ a evitar a maioria dos falsos positivos.")
            elif precision > 0.75:
                interpretacao.append(f"- Precisﾃ｣o: {precision:.4f} - Bom. O modelo evita falsos positivos, mas pode ser mais rigoroso.")
            elif precision > 0.5:
                interpretacao.append(f"- Precisﾃ｣o: {precision:.4f} - Moderada. Hﾃ｡ um nﾃｺmero considerﾃ｡vel de falsos positivos a corrigir.")
            else:
                interpretacao.append(f"- Precisﾃ｣o: {precision:.4f} - Fraca. Muitos falsos positivos estﾃ｣o a prejudicar a confianﾃｧa nas previsﾃｵes.")
        except (ValueError, TypeError):
            interpretacao.append("- Precisﾃ｣o: Nﾃ｣o disponﾃｭvel ou invﾃ｡lida.")

    # Recall
    if 'Recall' in metrics:
        try:
            recall = float(metrics['Recall'])
            if recall > 0.9:
                interpretacao.append(f"- Recall: {recall:.4f} - Excelente! O modelo estﾃ｡ a identificar quase todos os positivos verdadeiros.")
            elif recall > 0.75:
                interpretacao.append(f"- Recall: {recall:.4f} - Bom. A maioria dos positivos verdadeiros ﾃｩ identificada, mas hﾃ｡ espaﾃｧo para melhorias.")
            elif recall > 0.5:
                interpretacao.append(f"- Recall: {recall:.4f} - Moderado. O modelo estﾃ｡ a perder demasiados positivos verdadeiros.")
            else:
                interpretacao.append(f"- Recall: {recall:.4f} - Fraco. O modelo falha em identificar a maioria dos positivos verdadeiros. Pode ser necessﾃ｡rio ajustar os pesos ou thresholds.")
        except (ValueError, TypeError):
            interpretacao.append("- Recall: Nﾃ｣o disponﾃｭvel ou invﾃ｡lido.")
    
    # F1-Score
    if 'F1-Score' in metrics:
        try:
            f1_score = float(metrics['F1-Score'])
            if f1_score > 0.9:
                interpretacao.append(f"- F1-Score: {f1_score:.4f} - Excelente equilﾃｭbrio entre precisﾃ｣o e sensibilidade. O modelo estﾃ｡ altamente otimizado.")
            elif f1_score > 0.75:
                interpretacao.append(f"- F1-Score: {f1_score:.4f} - Bom desempenho. Contudo, hﾃ｡ espaﾃｧo para melhorias nos falsos positivos ou negativos.")
            elif f1_score > 0.5:
                interpretacao.append(f"- F1-Score: {f1_score:.4f} - Desempenho moderado. Ajustes no treino ou balanceamento dos dados podem ajudar.")
            else:
                interpretacao.append(f"- F1-Score: {f1_score:.4f} - Desempenho fraco. Recomenda-se rever os dados, ajustar hiperparﾃ｢metros ou otimizar o modelo.")
        except (ValueError, TypeError):
            interpretacao.append("- F1-Score: Nﾃ｣o disponﾃｭvel ou invﾃ｡lido.")

    # Se nenhuma mﾃｩtrica conhecida foi encontrada
    if not interpretacao:
        interpretacao.append("Nenhuma mﾃｩtrica de classificaﾃｧﾃ｣o reconhecida encontrada nos dados.")

    # Conclusﾃ｣o Geral
    if all(key in metrics for key in ['F1-Score', 'Precision', 'Recall']):
        try:
            f1_score = float(metrics['F1-Score'])
            precision = float(metrics['Precision'])
            recall = float(metrics['Recall'])
            
            if f1_score > 0.9 and precision > 0.9 and recall > 0.9:
                interpretacao.append("\nConclusﾃ｣o Geral: 脂 O modelo apresenta um desempenho excecional em todas as mﾃｩtricas. Estﾃ｡ pronto para produﾃｧﾃ｣o!")
            elif f1_score > 0.75 and precision > 0.75 and recall > 0.75:
                interpretacao.append("\nConclusﾃ｣o Geral: 総 O modelo tem um bom desempenho geral, mas pode ser ligeiramente melhorado com ajustes finos.")
            elif f1_score > 0.5 or precision > 0.5 or recall > 0.5:
                interpretacao.append("\nConclusﾃ｣o Geral:笞ｸ O modelo tem um desempenho moderado. Recomenda-se ajustar os hiperparﾃ｢metros ou melhorar os dados de treino.")
            else:
                interpretacao.append("\nConclusﾃ｣o Geral: 笶 O modelo apresenta um desempenho fraco. Serﾃ｡ necessﾃ｡rio rever o processo de treino, os dados e os parﾃ｢metros.")
        except (ValueError, TypeError):
            pass

    return "\n".join(interpretacao)

def generate_regression_interpretation(metrics):
    """Funﾃｧﾃ｣o para gerar interpretaﾃｧﾃ｣o personalizada das mﾃｩtricas de regressﾃ｣o"""
    interpretation = []

    # Verificar se as mﾃｩtricas estﾃ｣o no formato esperado
    if not isinstance(metrics, dict):
        return "Formato de mﾃｩtricas invﾃ｡lido."

    # Rﾂｲ (Coeficiente de Determinaﾃｧﾃ｣o)
    if 'Rﾂｲ' in metrics:
        try:
            r2 = float(metrics['Rﾂｲ'])
            if r2 > 0.9:
                interpretation.append(f"- Rﾂｲ: {r2:.4f} - Excelente! O modelo explica quase toda a variabilidade dos dados. Isso indica um forte ajuste entre as previsﾃｵes e os valores reais.")
            elif r2 > 0.75:
                interpretation.append(f"- Rﾂｲ: {r2:.4f} - Muito bom! O modelo explica a maior parte da variabilidade dos dados, mas ainda pode ser melhorado.")
            elif r2 > 0.5:
                interpretation.append(f"- Rﾂｲ: {r2:.4f} - Moderado. O modelo consegue explicar uma parte significativa da variabilidade, mas hﾃ｡ limitaﾃｧﾃｵes importantes no ajuste.")
            else:
                interpretation.append(f"- Rﾂｲ: {r2:.4f} - Fraco. O modelo explica pouca variabilidade dos dados. Considere revisar as features ou usar um modelo mais adequado.")
        except (ValueError, TypeError):
            interpretation.append("- Rﾂｲ: Nﾃ｣o disponﾃｭvel ou invﾃ｡lido.")

    # MAE (Erro Absoluto Mﾃｩdio)
    if 'MAE' in metrics:
        try:
            mae = float(metrics['MAE'])
            if mae < 0.1:
                interpretation.append(f"- MAE: {mae:.4f} - Excelente! O erro absoluto mﾃｩdio ﾃｩ muito pequeno, sugerindo que as previsﾃｵes sﾃ｣o altamente precisas.")
            elif mae < 1:
                interpretation.append(f"- MAE: {mae:.4f} - Bom. O erro absoluto mﾃｩdio ﾃｩ aceitﾃ｡vel, mas ainda pode ser otimizado.")
            else:
                interpretation.append(f"- MAE: {mae:.4f} - Alto. As previsﾃｵes estﾃ｣o frequentemente desviando dos valores reais. Considere ajustar o modelo ou as features.")
        except (ValueError, TypeError):
            interpretation.append("- MAE: Nﾃ｣o disponﾃｭvel ou invﾃ｡lido.")

    # MSE (Erro Quadrﾃ｡tico Mﾃｩdio)
    if 'MSE' in metrics:
        try:
            mse = float(metrics['MSE'])
            if mse < 0.1:
                interpretation.append(f"- MSE: {mse:.4f} - Excelente! O erro quadrﾃ｡tico mﾃｩdio ﾃｩ muito baixo, indicando que as previsﾃｵes estﾃ｣o prﾃｳximas dos valores reais.")
            elif mse < 1:
                interpretation.append(f"- MSE: {mse:.4f} - Bom. O erro ﾃｩ relativamente baixo, mas ainda hﾃ｡ espaﾃｧo para reduzir as discrepﾃ｢ncias.")
            else:
                interpretation.append(f"- MSE: {mse:.4f} - Alto. O erro ﾃｩ significativo. Isso pode indicar que o modelo nﾃ｣o estﾃ｡ capturando bem os padrﾃｵes nos dados.")
        except (ValueError, TypeError):
            interpretation.append("- MSE: Nﾃ｣o disponﾃｭvel ou invﾃ｡lido.")

    # Se nenhuma mﾃｩtrica conhecida foi encontrada
    if not interpretation:
        interpretation.append("Nenhuma mﾃｩtrica de regressﾃ｣o reconhecida encontrada nos dados.")

    # Conclusﾃ｣o geral com base nas mﾃｩtricas
    if all(key in metrics for key in ['Rﾂｲ', 'MAE', 'MSE']):
        try:
            r2 = float(metrics['Rﾂｲ'])
            mse = float(metrics['MSE'])
            mae = float(metrics['MAE'])
            
            if r2 > 0.9 and mse < 0.1 and mae < 0.1:
                interpretation.append("\nConclusﾃ｣o Geral: 脂 O modelo apresenta um desempenho excepcional! Estﾃ｡ pronto para produﾃｧﾃ｣o.")
            elif r2 > 0.75 and mse < 1 and mae < 1:
                interpretation.append("\nConclusﾃ｣o Geral: 総 O modelo tem um bom desempenho geral. Com ajustes menores, pode se tornar ainda melhor.")
            elif r2 > 0.5 or mse < 1 or mae < 1:
                interpretation.append("\nConclusﾃ｣o Geral: 笞ｸ O modelo estﾃ｡ funcional, mas ainda apresenta limitaﾃｧﾃｵes. Ajustes adicionais sﾃ｣o recomendados.")
            else:
                interpretation.append("\nConclusﾃ｣o Geral: 笶 O modelo apresenta desempenho insatisfatﾃｳrio. Considere reavaliar as features, ajustar hiperparﾃ｢metros ou explorar modelos alternativos.")
        except (ValueError, TypeError):
            pass

    return "\n".join(interpretation)

# Funﾃｧﾃ｣o para salvar o modelo treinado com nome dinﾃ｢mico
def save_best_model(model, with_feature_selection=True):
    try:
        # Determinar o nome do arquivo com base na seleﾃｧﾃ｣o de features
        if with_feature_selection:
            model_filename = "best_model_com_selecao_features.pkl"
        else:
            model_filename = "best_model_sem_selecao_features.pkl"

        # Salvar o modelo usando joblib
        joblib.dump(model, model_filename)
        st.success(f"Modelo salvo com sucesso como {model_filename}")
        return model_filename
    except Exception as e:
        st.error(f"Erro ao salvar o modelo: {str(e)}")
        return None


def execute_training():
    if st.session_state.step == 'train_and_store_metrics':
        model = st.session_state.models[st.session_state.selected_model_name]

        metrics = train_and_store_metrics(
            model,
            st.session_state.X_train,
            st.session_state.y_train,
            st.session_state.X_test,
            st.session_state.y_test,
            metric_type="sem_selecao_features"
        )

        # Depuraﾃｧﾃ｣o
        st.write("Conteﾃｺdo de metrics apﾃｳs treino:", st.session_state.get('metrics', {}))

        # Avanﾃｧar para a pﾃ｡gina final
        st.session_state.step = 'final_page'
        st.rerun()


## Relatﾃｳrio Final para Classificaﾃｧﾃ｣o/Regressao ##

# Funﾃｧﾃ｣o para gerar o relatﾃｳrio em PDF
from fpdf import FPDF
import requests
import tempfile
from datetime import datetime
from io import BytesIO

class CustomPDF(FPDF):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # Baixar o logo no inﾃｭcio para reutilizﾃ｡-lo
        self.logo_path = None
        logo_url = 'https://www.ipleiria.pt/normasgraficas/wp-content/uploads/sites/80/2017/09/estg_v-01.jpg'
        try:
            response = requests.get(logo_url)
            if response.status_code == 200:
                with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as tmpfile:
                    tmpfile.write(response.content)
                    self.logo_path = tmpfile.name
        except Exception as e:
            print(f"Erro ao baixar o logo: {e}")

    def header(self):
        # Posicionar o cabeﾃｧalho no topo da pﾃ｡gina
        self.set_y(10)
        
        # Adicionar a imagem no cabeﾃｧalho se o logo foi baixado com sucesso
        if self.logo_path:
            self.image(self.logo_path, 10, 10, 25)
        
        # Configurar fonte para o tﾃｭtulo
        self.set_font('Arial', 'B', 12)
        
        # Adicionar o tﾃｭtulo centralizado
        # Deixar espaﾃｧo para o logo
        self.cell(25)  # Espaﾃｧo para o logo
        self.cell(0, 10, 'MLCase - Plataforma de Machine Learning', 0, 0, 'C')
        
        # Adicionar uma linha horizontal apﾃｳs o cabeﾃｧalho
        self.ln(15)
        self.ln(5)  # Espaﾃｧo apﾃｳs o cabeﾃｧalho

    def footer(self):
        # Ir para 1.5 cm da parte inferior
        self.set_y(-20)
        
        # Adicionar uma linha horizontal antes do rodapﾃｩ
        self.line(10, self.get_y(), 200, self.get_y())
        self.ln(3)
        
        # Definir fonte para o rodapﾃｩ
        self.set_font('Arial', 'I', 8)
        
        # Data atual
        current_date = datetime.now().strftime('%d/%m/%Y')
        
        # Adicionar rodapﾃｩ com a data e nﾃｺmero da pﾃ｡gina
        self.cell(0, 10, f'{current_date} - Pﾃ｡gina {self.page_no()}  |  Autora da Plataforma: Bruna Sousa', 0, 0, 'C')
class MLCaseModelReportGenerator:
    def __init__(self, output_path='model_performance_report.pdf', logo_url=None):
        """
        Initialize the report generator
        
        :param output_path: Path to save the PDF
        :param logo_url: Optional URL for organization logo
        """
        self.output_path = output_path
        self.logo_url = logo_url or 'https://www.ipleiria.pt/normasgraficas/wp-content/uploads/sites/80/2017/09/estg_v-01.jpg'
        
        # Fetch logo
        self.logo_path = self._fetch_logo()
        
        # Prepare styles
        self.styles = getSampleStyleSheet()
        self._create_custom_styles()
    
    def _fetch_logo(self):
        """Fetch and save logo image"""
        try:
            response = requests.get(self.logo_url)
            if response.status_code == 200:
                with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as tmpfile:
                    tmpfile.write(response.content)
                    return tmpfile.name
            return None
        except Exception:
            return None
    
    def _create_custom_styles(self):
        """Create custom paragraph styles"""
        # Title style
        self.styles.add(ParagraphStyle(
            name='MLCaseTitle',
            parent=self.styles['Title'],
            fontSize=18,
            textColor=colors.HexColor('#2C3E50'),
            alignment=1,  # Center alignment
            spaceAfter=12
        ))
        
        # Subtitle style
        self.styles.add(ParagraphStyle(
            name='MLCaseSubtitle',
            parent=self.styles['Heading2'],
            fontSize=14,
            textColor=colors.HexColor('#34495E'),
            spaceAfter=6
        ))
        
        # Normal text style
        self.styles.add(ParagraphStyle(
            name='MLCaseNormal',
            parent=self.styles['Normal'],
            fontSize=10,
            textColor=colors.HexColor('#2C3E50'),
            leading=14
        ))
    
    def create_bar_chart(self, data, labels, title):
        """Create a bar chart using matplotlib and return as an image buffer"""
        plt.figure(figsize=(6, 4), dpi=100)
        plt.bar(labels, data, color=['#3498DB', '#2980B9'])
        plt.title(title, fontsize=12, color='#2C3E50')
        plt.ylabel('Value', color='#2C3E50')
        plt.xticks(rotation=45, ha='right', color='#2C3E50')
        plt.tight_layout()
        
        # Save to a bytes buffer
        buf = io.BytesIO()
        plt.savefig(buf, format='png', bbox_inches='tight')
        buf.seek(0)
        plt.close()
        
        return buf
    
def gerar_relatorio_pdf(comparison_df, best_model, session_state):
    """
    Gera um relatﾃｳrio PDF com os resultados da comparaﾃｧﾃ｣o de modelos.
    
    Args:
        comparison_df: DataFrame com as mﾃｩtricas comparativas
        best_model: String com o nome do melhor modelo
        session_state: Estado da sessﾃ｣o do Streamlit
        
    Returns:
        BytesIO: Buffer contendo o PDF gerado
    """

    # Inicializaﾃｧﾃ｣o do PDF com cabeﾃｧalho e rodapﾃｩ
    pdf = CustomPDF(format='A4')
    pdf.set_margins(10, 30, 10)  # left, top, right
    pdf.set_auto_page_break(auto=True, margin=30)  # Margem inferior para o rodapﾃｩ
    pdf.add_page()
    
    # Funﾃｧﾃ｣o para limpar texto para compatibilidade com codificaﾃｧﾃ｣o Latin-1
    def clean_text(text):
        if not isinstance(text, str):
            return str(text)
        return text.encode('latin-1', errors='ignore').decode('latin-1')
    

    # Tﾃｭtulo do Relatﾃｳrio
    pdf.set_font("Arial", style="B", size=16)
    pdf.cell(0, 10, txt=clean_text("Relatﾃｳrio Final do Modelo Treinado"), ln=True, align="C")
    pdf.ln(10)
    
    # Tipo de Modelo
    model_type = session_state.get('model_type', 'Indefinido')
    pdf.set_font("Arial", style="B", size=12)
    pdf.cell(60, 10, txt=clean_text("Tipo de Modelo:"), ln=False)
    pdf.set_font("Arial", size=12)
    pdf.cell(0, 10, txt=clean_text(model_type), ln=True)
    
    # Modelo Selecionado
    selected_model_name = session_state.get('selected_model_name', 'Nﾃ｣o Selecionado')
    pdf.set_font("Arial", style="B", size=12)
    pdf.cell(60, 10, txt=clean_text("Modelo Selecionado:"), ln=False)
    pdf.set_font("Arial", size=12)
    pdf.cell(0, 10, txt=clean_text(selected_model_name), ln=True)
    
    # Melhor Modelo
    pdf.set_font("Arial", style="B", size=12)
    pdf.cell(60, 10, txt=clean_text("Melhor Modelo:"), ln=False)
    pdf.set_font("Arial", size=12)
    pdf.cell(0, 10, txt=clean_text(best_model), ln=True)
    pdf.ln(10)
    
    # Informaﾃｧﾃｵes do Conjunto de Dados
    if 'X_train' in session_state and 'X_test' in session_state:
        X_train = session_state.X_train
        X_test = session_state.X_test
        
        # Calcular percentuais e tamanhos
        total_samples = X_train.shape[0] + X_test.shape[0]
        train_percent = (X_train.shape[0] / total_samples) * 100
        test_percent = (X_test.shape[0] / total_samples) * 100
        
        pdf.set_font("Arial", style="B", size=14)
        pdf.cell(0, 10, txt=clean_text("Informaﾃｧﾃｵes dos Conjuntos de Dados"), ln=True)
        pdf.ln(5)
        
        # Tabela de informaﾃｧﾃｵes do conjunto de dados
        data_info = [
            ["Amostras de Treino", f"{X_train.shape[0]} ({train_percent:.1f}%)"],
            ["Amostras de Teste", f"{X_test.shape[0]} ({test_percent:.1f}%)"],
            ["Features Originais", f"{X_train.shape[1]}"]
        ]
        
        # Adicionar features apﾃｳs seleﾃｧﾃ｣o se estiverem disponﾃｭveis
        if 'X_train_selected' in session_state:
            data_info.append(["Features Apﾃｳs Seleﾃｧﾃ｣o", f"{session_state.X_train_selected.shape[1]}"])
        
        # Formatar a tabela de informaﾃｧﾃｵes
        pdf.set_font("Arial", size=10)
        pdf.set_fill_color(144, 238, 144) # Cor de fundo do cabeﾃｧalho
        
        for i, (label, value) in enumerate(data_info):
            if i % 2 == 0:  # Linhas alternadas
                pdf.set_fill_color(240, 240, 240)
            else:
                pdf.set_fill_color(255, 255, 255)
            
            pdf.cell(70, 8, txt=clean_text(label), border=1, ln=0, fill=True)
            pdf.cell(0, 8, txt=clean_text(value), border=1, ln=1, fill=True)
        
        pdf.ln(10)
    
    # Features Selecionadas
    if 'selected_features' in session_state:
        pdf.set_font("Arial", style="B", size=14)
        pdf.cell(0, 10, txt=clean_text("Features Selecionadas"), ln=True)
        
        # Listar as features
        features = session_state.selected_features
        pdf.set_font("Arial", size=10)
        for i, feature in enumerate(features):
            pdf.cell(0, 6, txt=clean_text(f"窶｢ {feature}"), ln=True)
        
        pdf.ln(10)
    
    # Tabela de Mﾃｩtricas
    pdf.set_font("Arial", style="B", size=14)
    pdf.cell(0, 10, txt=clean_text("Comparaﾃｧﾃ｣o de Mﾃｩtricas"), ln=True)
    
    # Verificar o tipo de modelo para determinar quais mﾃｩtricas exibir
    is_regression = model_type == "Regressﾃ｣o"
    metric_columns = ['Rﾂｲ', 'MAE', 'MSE'] if is_regression else ['Accuracy', 'Precision', 'Recall', 'F1-Score']
    
    # Criar tabela de mﾃｩtricas
    pdf.set_font("Arial", style="B", size=10)
    pdf.set_fill_color(144, 238, 144) # Cor de fundo do cabeﾃｧalho
    
    # Definir a largura das colunas
    column_width = 30
    first_column_width = 60
    
    # Cabeﾃｧalho da tabela
    pdf.cell(first_column_width, 10, "Modelo", 1, 0, 'C', True)
    for col in metric_columns:
        pdf.cell(column_width, 10, clean_text(col), 1, 0, 'C', True)
    pdf.ln()
    
    # Linhas da tabela
    pdf.set_font("Arial", size=10)
    for _, row in comparison_df.iterrows():
        model_name = row['Modelo']
        pdf.cell(first_column_width, 10, clean_text(model_name), 1, 0, 'L')
        
        for col in metric_columns:
            if col in row:
                # Formatar o valor numﾃｩrico com 4 casas decimais
                if isinstance(row[col], (int, float)):
                    value = f"{row[col]:.4f}"
                else:
                    value = str(row[col])
                pdf.cell(column_width, 10, clean_text(value), 1, 0, 'C')
        
        pdf.ln()
    
    pdf.ln(10)
    
    # Grﾃ｡ficos de Mﾃｩtricas
    for metric in metric_columns:
        if metric in comparison_df.columns:
            # Criar o grﾃ｡fico com tamanho ajustado
            plt.figure(figsize=(10, 6))
            
            # Dados para o grﾃ｡fico
            models = comparison_df['Modelo'].tolist()
            values = comparison_df[metric].tolist()
            
            # Criar barras com espaﾃｧamento adequado
            plt.bar(models, values, color=['#90EE90', '#006400'], width=0.4)
            
            # Adicionar valores sobre as barras
            for i, v in enumerate(values):
                if isinstance(v, (int, float)):
                    plt.text(i, v + 0.01, f"{v:.4f}", ha='center', fontsize=10)
            
            # MUDANﾃ② PRINCIPAL: Configuraﾃｧﾃ｣o do eixo X sem rotaﾃｧﾃ｣o
            plt.xticks(rotation=0, ha='center', fontsize=8)  # Mudar rotation=45 para rotation=0
            
            # Estilizaﾃｧﾃ｣o com mais espaﾃｧo
            plt.title(f"Comparaﾃｧﾃ｣o de {metric}", fontsize=14, pad=15)  # Aumentar pad para dar mais espaﾃｧo
            plt.ylabel(metric, fontsize=12)
            
            # Garantir espaﾃｧo para o conteﾃｺdo
            plt.subplots_adjust(bottom=0.2, left=0.15)  # Aumentar margem inferior
            
            # Ajustar a altura do grﾃ｡fico para evitar corte
            plt.ylim(0, max(values) * 1.2)  # Aumenta o limite superior em 20%
            
            plt.tight_layout()  # Ajusta automaticamente o layout
            
            # Salvar o grﾃ｡fico em um arquivo temporﾃ｡rio com DPI maior
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.png')
            plt.savefig(temp_file.name, bbox_inches='tight', dpi=150)  # Aumentar DPI e garantir que nada seja cortado
            plt.close()
        
            # Adicionar o grﾃ｡fico ao PDF - AJUSTADO
            pdf.add_page()
            pdf.set_font("Arial", style="B", size=14)
            pdf.cell(0, 10, txt=clean_text(f"Grﾃ｡fico de Comparaﾃｧﾃ｣o - {metric}"), ln=True, align="C")
            
            # Posicionar o grﾃ｡fico mais para baixo para evitar sobreposiﾃｧﾃ｣o com o cabeﾃｧalho
            pdf.image(temp_file.name, x=10, y=45, w=180)  # Posiﾃｧﾃ｣o Y aumentada
            
            # Fechar e remover o arquivo temporﾃ｡rio
            temp_file.close()
            try:
                os.remove(temp_file.name)
            except:
                pass  # Ignorar erros ao remover arquivos temporﾃ｡rios
        
    # Interpretaﾃｧﾃ｣o das Mﾃｩtricas
    pdf.add_page()
    pdf.set_font("Arial", style="B", size=14)
    pdf.cell(0, 10, txt=clean_text("Interpretaﾃｧﾃ｣o das Mﾃｩtricas"), ln=True, align="C")
    
    # Funﾃｧﾃ｣o para gerar interpretaﾃｧﾃ｣o de mﾃｩtricas
    def generate_metrics_interpretation(metrics, model_type):
        interpretacao = []
        
        if model_type == "Classificaﾃｧﾃ｣o":
            # Accuracy
            accuracy = float(metrics.get('Accuracy', 0))
            if accuracy > 0.9:
                interpretacao.append(f"Acurﾃ｡cia: {accuracy:.4f} - Excelente! O modelo tem uma taxa de acerto global muito elevada.")
            elif accuracy > 0.75:
                interpretacao.append(f"Acurﾃ｡cia: {accuracy:.4f} - Boa. O modelo estﾃ｡ a funcionar bem, mas ainda hﾃ｡ margem para otimizaﾃｧﾃ｣o.")
            elif accuracy > 0.5:
                interpretacao.append(f"Acurﾃ｡cia: {accuracy:.4f} - Moderada. Os erros ainda sﾃ｣o significativos e devem ser corrigidos.")
            else:
                interpretacao.append(f"Acurﾃ｡cia: {accuracy:.4f} - Fraca. O modelo estﾃ｡ a falhar em muitas previsﾃｵes e precisa de ser revisto.")
        
            # Precision
            precision = float(metrics.get('Precision', 0))
            if precision > 0.9:
                interpretacao.append(f"Precisﾃ｣o: {precision:.4f} - Excelente! O modelo estﾃ｡ a evitar a maioria dos falsos positivos.")
            elif precision > 0.75:
                interpretacao.append(f"Precisﾃ｣o: {precision:.4f} - Bom. O modelo evita falsos positivos, mas pode ser mais rigoroso.")
            elif precision > 0.5:
                interpretacao.append(f"Precisﾃ｣o: {precision:.4f} - Moderada. Hﾃ｡ um nﾃｺmero considerﾃ｡vel de falsos positivos a corrigir.")
            else:
                interpretacao.append(f"Precisﾃ｣o: {precision:.4f} - Fraca. Muitos falsos positivos estﾃ｣o a prejudicar a confianﾃｧa nas previsﾃｵes.")
        
            # Recall
            recall = float(metrics.get('Recall', 0))
            if recall > 0.9:
                interpretacao.append(f"Recall: {recall:.4f} - Excelente! O modelo estﾃ｡ a identificar quase todos os positivos verdadeiros.")
            elif recall > 0.75:
                interpretacao.append(f"Recall: {recall:.4f} - Bom. A maioria dos positivos verdadeiros ﾃｩ identificada, mas hﾃ｡ espaﾃｧo para melhorias.")
            elif recall > 0.5:
                interpretacao.append(f"Recall: {recall:.4f} - Moderado. O modelo estﾃ｡ a perder demasiados positivos verdadeiros.")
            else:
                interpretacao.append(f"Recall: {recall:.4f} - Fraco. O modelo falha em identificar a maioria dos positivos verdadeiros.")
            
            # F1-Score
            f1_score = float(metrics.get('F1-Score', 0))
            if f1_score > 0.9:
                interpretacao.append(f"F1-Score: {f1_score:.4f} - Excelente equilﾃｭbrio entre precisﾃ｣o e sensibilidade.")
            elif f1_score > 0.75:
                interpretacao.append(f"F1-Score: {f1_score:.4f} - Bom desempenho. Contudo, hﾃ｡ espaﾃｧo para melhorias.")
            elif f1_score > 0.5:
                interpretacao.append(f"F1-Score: {f1_score:.4f} - Desempenho moderado.")
            else:
                interpretacao.append(f"F1-Score: {f1_score:.4f} - Desempenho fraco.")
        
        elif model_type == "Regressﾃ｣o":
            # Rﾂｲ (Coeficiente de Determinaﾃｧﾃ｣o)
            r2 = float(metrics.get('Rﾂｲ', 0))
            if r2 > 0.9:
                interpretacao.append(f"Rﾂｲ: {r2:.4f} - Excelente! O modelo explica quase toda a variabilidade dos dados.")
            elif r2 > 0.75:
                interpretacao.append(f"Rﾂｲ: {r2:.4f} - Muito bom! O modelo explica a maior parte da variabilidade dos dados.")
            elif r2 > 0.5:
                interpretacao.append(f"Rﾂｲ: {r2:.4f} - Moderado. O modelo consegue explicar uma parte significativa da variabilidade.")
            else:
                interpretacao.append(f"Rﾂｲ: {r2:.4f} - Fraco. O modelo explica pouca variabilidade dos dados.")
        
            # MAE (Erro Absoluto Mﾃｩdio)
            mae = float(metrics.get('MAE', 0))
            if mae < 0.1:
                interpretacao.append(f"MAE: {mae:.4f} - Excelente! O erro absoluto mﾃｩdio ﾃｩ muito pequeno.")
            elif mae < 1:
                interpretacao.append(f"MAE: {mae:.4f} - Bom. O erro absoluto mﾃｩdio ﾃｩ aceitﾃ｡vel.")
            else:
                interpretacao.append(f"MAE: {mae:.4f} - Alto. As previsﾃｵes estﾃ｣o frequentemente desviando dos valores reais.")
        
            # MSE (Erro Quadrﾃ｡tico Mﾃｩdio)
            mse = float(metrics.get('MSE', 0))
            if mse < 0.1:
                interpretacao.append(f"MSE: {mse:.4f} - Excelente! O erro quadrﾃ｡tico mﾃｩdio ﾃｩ muito baixo.")
            elif mse < 1:
                interpretacao.append(f"MSE: {mse:.4f} - Bom. O erro ﾃｩ relativamente baixo.")
            else:
                interpretacao.append(f"MSE: {mse:.4f} - Alto. O erro ﾃｩ significativo.")
        
        return interpretacao
    
    # Obter dados das mﾃｩtricas originais e selecionadas
    original_metrics = {}
    selected_metrics = {}
    
    # Separar as mﾃｩtricas por tipo de modelo
    for _, row in comparison_df.iterrows():
        model_name = row['Modelo']
        
        if "Sem Seleﾃｧﾃ｣o" in model_name:
            # Extrair mﾃｩtricas do modelo sem seleﾃｧﾃ｣o de features
            for col in metric_columns:
                if col in row:
                    original_metrics[col] = row[col]
        
        if "Com Seleﾃｧﾃ｣o" in model_name:
            # Extrair mﾃｩtricas do modelo com seleﾃｧﾃ｣o de features
            for col in metric_columns:
                if col in row:
                    selected_metrics[col] = row[col]
    
    # Interpretaﾃｧﾃｵes para modelos sem e com seleﾃｧﾃ｣o de features
    pdf.set_font("Arial", style="B", size=12)
    pdf.cell(0, 10, txt=clean_text("Modelo Sem Seleﾃｧﾃ｣o de Features"), ln=True)
    pdf.set_font("Arial", size=10)
    
    # Adicionar interpretaﾃｧﾃ｣o do modelo sem seleﾃｧﾃ｣o
    for line in generate_metrics_interpretation(original_metrics, model_type):
        pdf.multi_cell(0, 8, txt=clean_text(f"窶｢ {line}"))
    
    pdf.ln(5)
    pdf.set_font("Arial", style="B", size=12)
    pdf.cell(0, 10, txt=clean_text("Modelo Com Seleﾃｧﾃ｣o de Features"), ln=True)
    pdf.set_font("Arial", size=10)
    
    # Adicionar interpretaﾃｧﾃ｣o do modelo com seleﾃｧﾃ｣o
    for line in generate_metrics_interpretation(selected_metrics, model_type):
        pdf.multi_cell(0, 8, txt=clean_text(f"窶｢ {line}"))
    
    # Conclusﾃ｣o
    pdf.ln(10)
    pdf.set_font("Arial", style="B", size=14)
    pdf.cell(0, 10, txt=clean_text("Conclusﾃ｣o"), ln=True)
    
    # Determinar a melhor mﾃｩtrica com base na escolha do utilizador
    scoring_metric = session_state.get("selected_scoring", None)

    # Fallback para mﾃｩtricas padrﾃ｣o se a mﾃｩtrica selecionada nﾃ｣o estiver disponﾃｭvel
    if not scoring_metric or scoring_metric not in metric_columns:
        main_metric = 'Rﾂｲ' if is_regression else 'F1-Score'
    else:
        main_metric = scoring_metric

    # Obter os valores da mﾃｩtrica escolhida
    original_value = original_metrics.get(main_metric, 0)
    selected_value = selected_metrics.get(main_metric, 0)

    # Texto da conclusﾃ｣o
    pdf.set_font("Arial", size=10)
    conclusion_text = f"Com base na mﾃｩtrica principal ({main_metric}), o modelo {best_model} apresentou o melhor desempenho."
    pdf.multi_cell(0, 8, txt=clean_text(conclusion_text))
    
    if original_value > selected_value:
        recommendation_text = "Recomenda-se utilizar o modelo sem seleﾃｧﾃ｣o de features, pois apresentou melhor desempenho geral."
    else:
        feature_reduction = session_state.X_train.shape[1] - session_state.X_train_selected.shape[1]
        recommendation_text = f"Recomenda-se utilizar o modelo com seleﾃｧﾃ｣o de features, pois alﾃｩm de melhorar o desempenho, reduziu a dimensionalidade em {feature_reduction} features."
    
    pdf.multi_cell(0, 8, txt=clean_text(recommendation_text))
    
    # Salvar o PDF em um buffer
    pdf_buffer = BytesIO()
    pdf_output = pdf.output(dest='S').encode('latin1', errors='ignore')
    pdf_buffer.write(pdf_output)
    pdf_buffer.seek(0)
    return pdf_buffer

# Funﾃｧﾃ｣o para exibir a pﾃ｡gina final com o relatﾃｳrio

# Mapeamento de nomes de mﾃｩtricas para as colunas do DataFrame
# Atualizar o dicionﾃ｡rio METRIC_MAPPING para garantir que MAE seja reconhecido
METRIC_MAPPING = {
    "accuracy": "Accuracy",
    "precision": "Precision", 
    "recall": "Recall",
    "f1-score": "F1-Score",
    "r2": "Rﾂｲ",
    "Rﾂｲ": "Rﾂｲ",  # Adicionar mapeamento direto para Rﾂｲ
    "r-squared": "Rﾂｲ",
    "coefficient_of_determination": "Rﾂｲ",
    "mean_squared_error": "MSE",
    "mse": "MSE",  # Adicionar versﾃ｣o minﾃｺscula de MSE
    "mean_absolute_error": "MAE",
    "mae": "MAE"  # Adicionar versﾃ｣o minﾃｺscula de MAE
}

def get_metric_mapping(metric):
    """
    Funﾃｧﾃ｣o para obter o nome da mﾃｩtrica de forma mais flexﾃｭvel
    
    Args:
        metric (str): Nome da mﾃｩtrica a ser mapeada
    
    Returns:
        str: Nome da mﾃｩtrica mapeado ou None se nﾃ｣o encontrado
    """
    # Garantir que seja uma string
    if not isinstance(metric, str):
        st.write(f"Metric nﾃ｣o ﾃｩ uma string: {metric}, tipo: {type(metric)}")
        return None
    
    # Converter para minﾃｺsculas, remover espaﾃｧos, acentos
    import unidecode # Normaliza caracteres acentuados, ﾃｺtil para lidar com strings em diferentes idiomas.
    metric_clean = unidecode.unidecode(metric.lower().replace(' ', '').replace('-', '').replace('_', ''))
    
    # Verificar se a mﾃｩtrica jﾃ｡ estﾃ｡ diretamente no formato esperado
    if metric in METRIC_MAPPING.values():
        return metric
    
    # Dicionﾃ｡rio expandido de mapeamentos
    extended_mapping = {
        **METRIC_MAPPING,
        "r2score": "Rﾂｲ",
        "rsquared": "Rﾂｲ",
        "determinacao": "Rﾂｲ",
        "coeficienteajuste": "Rﾂｲ",
        "mae": "MAE",
        "erro_absoluto_medio": "MAE",
        "mean_absolute_error": "MAE",
        "erro_absoluto": "MAE",
        "mse": "MSE",
        "erro_quadratico_medio": "MSE",
        "mean_squared_error": "MSE",
        "erro_quadratico": "MSE"
    }
    
    # Tentar mapear
    mapped_metric = extended_mapping.get(metric_clean)
    
    # Se nﾃ｣o encontrou, verificar diretamente nas chaves do METRIC_MAPPING
    if mapped_metric is None and metric in METRIC_MAPPING:
        mapped_metric = METRIC_MAPPING[metric]
        
    # Adicionar debug
    st.write(f"Mﾃｩtrica original: {metric}, limpa: {metric_clean}, mapeada: {mapped_metric}")
    
    return mapped_metric
    
def final_page():
    st.title("Resumo Final dos Modelos Treinados")

    # **CONFIGURAﾃﾃ髭S UTILIZADAS**
    st.subheader("Configuraﾃｧﾃｵes Utilizadas")

    # Tipo de Modelo
    model_type = st.session_state.get('model_type', 'Indefinido')
    st.write(f"**Tipo de Modelo:** {model_type}")

    # Modelo Selecionado
    selected_model_name = st.session_state.get('selected_model_name', 'Nﾃ｣o Selecionado')
    st.write(f"**Modelo Selecionado:** {selected_model_name}")

    # Recupera mﾃｩtricas salvas (sem re-treinar)
    original_metrics = st.session_state.get('resultado_sem_selecao', {})
    selected_metrics = st.session_state.get('resultado_com_selecao', {})

    # Exibir estatﾃｭsticas sobre os conjuntos de dados
    if 'X_train' in st.session_state and 'X_train_selected' in st.session_state:
        X_train_original = st.session_state.X_train
        X_train_selected = st.session_state.X_train_selected
        
        # Calcular percentuais
        total_samples = X_train_original.shape[0] + st.session_state.X_test.shape[0]
        train_percent = (X_train_original.shape[0] / total_samples) * 100
        test_percent = (st.session_state.X_test.shape[0] / total_samples) * 100
        
        st.subheader("投 Informaﾃｧﾃｵes dos Conjuntos de Dados")
        st.write(f"窶｢ Amostras de Treino: {X_train_original.shape[0]} ({train_percent:.1f}% do total)")
        st.write(f"窶｢ Amostras de Teste: {st.session_state.X_test.shape[0]} ({test_percent:.1f}% do total)")
        st.write(f"窶｢ Features Originais: {st.session_state.X_train_original.shape[1] if 'X_train_original' in st.session_state else X_train_original.shape[1]}")
        st.write(f"窶｢ Features Apﾃｳs Seleﾃｧﾃ｣o: {X_train_selected.shape[1]}")

    # Recuperar features selecionadas
    if 'selected_features' in st.session_state:
        st.subheader("笨 Features Selecionadas")
        st.write(st.session_state.selected_features)

    # Recupera a mﾃｩtrica escolhida para seleﾃｧﾃ｣o de features
    scoring_metric = st.session_state.get("selected_scoring", None)

    # Validar se a mﾃｩtrica foi definida
    if not scoring_metric:
        st.error("Nenhuma mﾃｩtrica foi selecionada. Volte para a etapa de Seleﾃｧﾃ｣o de Features.")
        return

    # Obter o nome capitalizado da mﾃｩtrica com base no mapeamento
    scoring_metric_capitalized = get_metric_mapping(scoring_metric)
    if not scoring_metric_capitalized:
        st.error(f"A mﾃｩtrica '{scoring_metric}' nﾃ｣o ﾃｩ vﾃ｡lida ou nﾃ｣o estﾃ｡ disponﾃｭvel.")
        return

    # **COMPARAﾃﾃグ DE Mﾃ欝RICAS**
    st.subheader("Comparaﾃｧﾃ｣o de Mﾃｩtricas")

    # Formatar valores com 4 casas decimais
    def format_metric(value):
        try:
            return float(f"{float(value):.4f}")
        except (ValueError, TypeError):
            return None

    # Criar tabela de mﾃｩtricas
    if model_type == "Classificaﾃｧﾃ｣o":
        comparison_df = pd.DataFrame({
            'Modelo': ['Sem Seleﾃｧﾃ｣o de Features', 'Com Seleﾃｧﾃ｣o de Features'],
            'Accuracy': [
                format_metric(original_metrics.get('Accuracy', 'N/A')),
                format_metric(selected_metrics.get('Accuracy', 'N/A'))
            ],
            'Precision': [
                format_metric(original_metrics.get('Precision', 'N/A')),
                format_metric(selected_metrics.get('Precision', 'N/A'))
            ],
            'Recall': [
                format_metric(original_metrics.get('Recall', 'N/A')),
                format_metric(selected_metrics.get('Recall', 'N/A'))
            ],
            'F1-Score': [
                format_metric(original_metrics.get('F1-Score', 'N/A')),
                format_metric(selected_metrics.get('F1-Score', 'N/A'))
            ],
            'Best Parameters': [
                st.session_state.get('best_params', 'N/A'),
                st.session_state.get('best_params_selected', 'N/A')
            ]
        })
    elif model_type == "Regressﾃ｣o":
        comparison_df = pd.DataFrame({
            'Modelo': ['Sem Seleﾃｧﾃ｣o de Features', 'Com Seleﾃｧﾃ｣o de Features'],
            'Rﾂｲ': [
                format_metric(original_metrics.get('Rﾂｲ', 'N/A')),
                format_metric(selected_metrics.get('Rﾂｲ', 'N/A'))
            ],
            'MAE': [
                format_metric(original_metrics.get('MAE', 'N/A')),
                format_metric(selected_metrics.get('MAE', 'N/A'))
            ],
            'MSE': [
                format_metric(original_metrics.get('MSE', 'N/A')),
                format_metric(selected_metrics.get('MSE', 'N/A'))
            ],
            'Best Parameters': [
                st.session_state.get('best_params', 'N/A'),
                st.session_state.get('best_params_selected', 'N/A')
            ]
        })
    else:
        st.error("Tipo de modelo nﾃ｣o reconhecido. Nﾃ｣o ﾃｩ possﾃｭvel gerar a tabela de mﾃｩtricas.")
        return

    # Exibir tabela de mﾃｩtricas com ajustes finos
    st.dataframe(comparison_df.style.format({
        'Accuracy': '{:.4f}' if 'Accuracy' in comparison_df.columns else None,
        'Precision': '{:.4f}' if 'Precision' in comparison_df.columns else None,
        'Recall': '{:.4f}' if 'Recall' in comparison_df.columns else None,
        'F1-Score': '{:.4f}' if 'F1-Score' in comparison_df.columns else None,
        'Rﾂｲ': '{:.4f}' if 'Rﾂｲ' in comparison_df.columns else None,
        'MAE': '{:.4f}' if 'MAE' in comparison_df.columns else None,
        'MSE': '{:.4f}' if 'MSE' in comparison_df.columns else None,
    }).set_table_styles([
        {'selector': 'th', 'props': [('font-size', '14px'), ('background-color', '#f0f0f0'), ('text-align', 'center'), ('font-weight', 'bold')]},  # Cabeﾃｧalho
        {'selector': 'td', 'props': [('font-size', '14px'), ('text-align', 'center')]},  # Tamanho das cﾃｩlulas e alinhamento
        {'selector': 'table', 'props': [('width', '100%'), ('border-collapse', 'collapse')]},  # Largura da tabela e bordas
        {'selector': 'tr:nth-child(even)', 'props': [('background-color', '#f9f9f9')]},  # Cor de fundo alternada para as linhas
        {'selector': 'tr:nth-child(odd)', 'props': [('background-color', '#ffffff')]},  # Cor de fundo para linhas ﾃｭmpares
    ]))

    # Verificar se a mﾃｩtrica escolhida existe no DataFrame
    if scoring_metric_capitalized not in comparison_df.columns:
        st.error(f"A mﾃｩtrica '{scoring_metric}' nﾃ｣o estﾃ｡ disponﾃｭvel no DataFrame.")
        return


    # **GRﾃ：ICOS DAS Mﾃ欝RICAS**
    st.subheader("Grﾃ｡fico Interativo de Comparaﾃｧﾃ｣o de Mﾃｩtricas")

    # Determinar as mﾃｩtricas disponﾃｭveis com base no tipo de modelo
    if model_type == "Classificaﾃｧﾃ｣o":
        metric_columns = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
    elif model_type == "Regressﾃ｣o":
        metric_columns = ['Rﾂｲ', 'MAE', 'MSE']
    else:
        st.error("Tipo de modelo nﾃ｣o reconhecido. Nﾃ｣o ﾃｩ possﾃｭvel gerar grﾃ｡ficos.")
        return

    # Adicionar um filtro interativo para a seleﾃｧﾃ｣o da mﾃｩtrica
    selected_metric = st.selectbox(
        "Selecione a mﾃｩtrica para visualizar:",
        metric_columns,
        index=0  # Mﾃｩtrica padrﾃ｣o exibida no inﾃｭcio
    )

    # Criar o grﾃ｡fico apenas para a mﾃｩtrica selecionada
    if selected_metric in comparison_df.columns:
        fig, ax = plt.subplots(figsize=(10, 6))
        
        # Dados para o grﾃ｡fico
        bars = ax.bar(
            comparison_df['Modelo'],
            comparison_df[selected_metric],
            color=['#9ACD32', '#006400']  # Verde claro e verde escuro
        )
        
        # Adicionar valores nas barras
        for bar in bars:
            height = bar.get_height()
            ax.annotate(f'{height:.4f}',
                        xy=(bar.get_x() + bar.get_width() / 2, height),
                        xytext=(0, 3),
                        textcoords="offset points",
                        ha='center', va='bottom',
                        fontsize=12)
        
        ax.set_title(f"Comparaﾃｧﾃ｣o de {selected_metric}", fontsize=14)
        ax.set_ylabel(selected_metric, fontsize=12)
        ax.set_xlabel("Modelo", fontsize=12)
        
        # Ajustar altura para caber os valores
        plt.ylim(0, max(comparison_df[selected_metric]) * 1.1)
        
        # Exibir grﾃ｡fico no Streamlit
        st.pyplot(fig)
    else:
        st.error(f"A mﾃｩtrica selecionada '{selected_metric}' nﾃ｣o estﾃ｡ disponﾃｭvel.")

    # **DETERMINAR O MELHOR MODELO BASEADO NA Mﾃ欝RICA ESCOLHIDA**
    scoring_values = comparison_df[scoring_metric_capitalized].values  # Recupera os valores da mﾃｩtrica na tabela
    if len(scoring_values) == 2:  # Certifique-se de que existem dois valores (sem e com seleﾃｧﾃ｣o)
        score_without_selection = scoring_values[0]
        score_with_selection = scoring_values[1]

        # Determina o melhor modelo
        if score_with_selection > score_without_selection:
            best_model = "Com Seleﾃｧﾃ｣o de Features"
            best_score = score_with_selection
        else:
            best_model = "Sem Seleﾃｧﾃ｣o de Features"
            best_score = score_without_selection
    else:
        st.warning("Erro na determinaﾃｧﾃ｣o das mﾃｩtricas na tabela.")
        return

    # Exibir mensagem com o melhor modelo
    st.success(f"脂 **O melhor modelo ﾃｩ:** {best_model} com base na mﾃｩtrica: {scoring_metric_capitalized} ({best_score:.4f})")

    # **INTERPRETAﾃﾃグ DAS Mﾃ欝RICAS**
    st.subheader("Interpretaﾃｧﾃ｣o das Mﾃｩtricas")
    try:
        # Gerar interpretaﾃｧﾃ｣o para cada modelo
        if model_type == "Classificaﾃｧﾃ｣o":
            interpretation_without = generate_metrics_interpretation(original_metrics)
            interpretation_with = generate_metrics_interpretation(selected_metrics)
        elif model_type == "Regressﾃ｣o":
            interpretation_without = generate_regression_interpretation(original_metrics)
            interpretation_with = generate_regression_interpretation(selected_metrics)
        else:
            raise ValueError("Tipo de modelo desconhecido para interpretaﾃｧﾃ｣o.")

        # Exibir interpretaﾃｧﾃｵes
        st.write("### Sem Seleﾃｧﾃ｣o de Features")
        st.write(interpretation_without)

        st.write("### Com Seleﾃｧﾃ｣o de Features")
        st.write(interpretation_with)
    except Exception as e:
        st.error(f"Erro ao gerar a interpretaﾃｧﾃ｣o das mﾃｩtricas: {e}")
        
    # **DOWNLOAD DO MODELO TREINADO**
    st.subheader("Download do Melhor Modelo Treinado")
    model = st.session_state.models.get(st.session_state.selected_model_name)
    model_filename = save_best_model(model, with_feature_selection=(best_model == "Com Seleﾃｧﾃ｣o de Features"))

    if model_filename:
        with open(model_filename, "rb") as file:
            st.download_button(
                label="沈 Download Melhor Modelo",
                data=file,
                file_name=model_filename,
                mime="application/octet-stream",
            )

    # **DOWNLOAD DO RELATﾃ迭IO EM PDF**
    try:
        pdf_buffer = gerar_relatorio_pdf(comparison_df, best_model, st.session_state)
        pdf_file_name = f"relatorio_final_{st.session_state.get('selected_model_name', 'modelo')}.pdf"
        st.download_button(
            label="沈 Download Relatﾃｳrio PDF",
            data=pdf_buffer,
            file_name=pdf_file_name,
            mime="application/pdf",
        )
    except Exception as e:
        st.error(f"Erro ao gerar relatﾃｳrio em PDF: {e}")

    # **CONCLUIR**
    if st.button("Concluir"):
        st.session_state.clear()  # Limpa o cache do Streamlit
        st.rerun()

############ Relatﾃｳrio Final para Clustering ###################
# Classe personalizada para PDF
class CustomPDF(FPDF):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # Baixar o logo no inﾃｭcio para reutilizﾃ｡-lo
        self.logo_path = None
        logo_url = 'https://www.ipleiria.pt/normasgraficas/wp-content/uploads/sites/80/2017/09/estg_v-01.jpg'
        try:
            response = requests.get(logo_url)
            if response.status_code == 200:
                with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as tmpfile:
                    tmpfile.write(response.content)
                    self.logo_path = tmpfile.name
        except Exception as e:
            print(f"Erro ao baixar o logo: {e}")

    def header(self):
        # Posicionar o cabeﾃｧalho no topo da pﾃ｡gina
        self.set_y(10)
        
        # Adicionar a imagem no cabeﾃｧalho se o logo foi baixado com sucesso
        if self.logo_path:
            self.image(self.logo_path, 10, 10, 25)
        
        # Configurar fonte para o tﾃｭtulo
        self.set_font('Arial', 'B', 12)
        
        # Adicionar o tﾃｭtulo centralizado
        # Deixar espaﾃｧo para o logo
        self.cell(25)  # Espaﾃｧo para o logo
        self.cell(0, 10, 'MLCase - Plataforma de Machine Learning', 0, 0, 'C')
        
        # Adicionar uma linha horizontal apﾃｳs o cabeﾃｧalho
        self.ln(15)
        self.ln(5)  # Espaﾃｧo apﾃｳs o cabeﾃｧalho

    def footer(self):
        # Ir para 1.5 cm da parte inferior
        self.set_y(-20)
        
        # Adicionar uma linha horizontal antes do rodapﾃｩ
        self.line(10, self.get_y(), 200, self.get_y())
        self.ln(3)
        
        # Definir fonte para o rodapﾃｩ
        self.set_font('Arial', 'I', 8)
        
        # Data atual
        current_date = datetime.now().strftime('%d/%m/%Y')
        
        # Adicionar rodapﾃｩ com a data e nﾃｺmero da pﾃ｡gina
        self.cell(0, 10, f'{current_date} - Pﾃ｡gina {self.page_no()}  |  Autora da Plataforma: Bruna Sousa', 0, 0, 'C')
# Funﾃｧﾃ｣o para gerar o relatﾃｳrio PDF
import os
import matplotlib.pyplot as plt
from fpdf import FPDF
from io import BytesIO

def gerar_relatorio_clustering_pdf(initial_metrics, retrain_metrics, best_model_type, st_session):
    pdf = CustomPDF(format='A4')
    pdf.set_auto_page_break(auto=True, margin=15)
    pdf.add_page()

    # Tﾃｭtulo
    pdf.set_font("Arial", style="B", size=16)
    pdf.cell(0, 10, txt="Relatﾃｳrio Final do Modelo Treinados", ln=True, align="C")
    pdf.ln(10)

    # Modelo Selecionado
    pdf.set_font("Arial", style="B", size=12)
    pdf.cell(50, 10, txt="Modelo Selecionado:", ln=False)
    pdf.set_font("Arial", size=12)
    model_info = st_session['selected_model_name']
    
    # Adicionar informaﾃｧﾃ｣o de componentes para KMeans e Clustering Hierﾃ｡rquico
    if st_session['selected_model_name'] in ["KMeans", "Clustering Hierﾃ｡rquico"]:
        model_info += f" (PCA: {st_session.get('pca_n_components', 'N/A')} componentes)"
    
    pdf.cell(0, 10, txt=model_info, ln=True)
    pdf.ln(5)

    # Melhor Modelo
    pdf.set_font("Arial", style="B", size=12)
    pdf.cell(50, 10, txt="Melhor Modelo Treinado:", ln=False)
    pdf.set_font("Arial", size=12)
    pdf.cell(0, 10, txt=best_model_type, ln=True)
    pdf.ln(10)

    # Adicionar mﾃｩtricas do treino inicial e re-treino
    pdf.set_font("Arial", style="B", size=12)
    pdf.cell(0, 10, txt="Mﾃｩtricas Obtidas", ln=True)
    pdf.set_font("Arial", size=9)
    pdf.set_fill_color(200, 220, 255)

    # Cabeﾃｧalho da tabela
    pdf.set_fill_color(144, 238, 144)  # Verde claro (light green)
    col_widths = [40, 40, 40, 40]
    pdf.cell(col_widths[0], 10, "Treino", 1, 0, 'C', True)
    pdf.cell(col_widths[1], 10, "Silhouette Score", 1, 0, 'C', True)
    pdf.cell(col_widths[2], 10, "Davies-Bouldin Index", 1, 0, 'C', True)
    pdf.cell(col_widths[3], 10, "Calinski-Harabasz Score", 1, 1, 'C', True)

    # Dados da tabela
    pdf.set_font("Arial", size=8)
    pdf.cell(col_widths[0], 10, "Treino Inicial", 1, 0, 'C')
    pdf.cell(col_widths[1], 10, f"{initial_metrics['Silhouette Score']:.2f}", 1, 0, 'C')
    pdf.cell(col_widths[2], 10, f"{initial_metrics['Davies-Bouldin Index']:.2f}", 1, 0, 'C')
    pdf.cell(col_widths[3], 10, f"{initial_metrics['Calinski-Harabasz Score']:.2f}", 1, 1, 'C')

    if retrain_metrics:
        pdf.cell(col_widths[0], 10, "Re-Treino", 1, 0, 'C')
        pdf.cell(col_widths[1], 10, f"{retrain_metrics['Silhouette Score']:.2f}", 1, 0, 'C')
        pdf.cell(col_widths[2], 10, f"{retrain_metrics['Davies-Bouldin Index']:.2f}", 1, 0, 'C')
        pdf.cell(col_widths[3], 10, f"{retrain_metrics['Calinski-Harabasz Score']:.2f}", 1, 1, 'C')

    pdf.ln(10)

    # Adicionar interpretaﾃｧﾃｵes
    def add_interpretation(metrics, treino_name):
        pdf.set_font("Arial", size=10)
        pdf.cell(0, 10, txt=f"{treino_name}:", ln=True)
        pdf.multi_cell(0, 10, txt=f"  Silhouette Score: {metrics['Silhouette Score']:.2f} - "
                                  f"{'Excelente' if metrics['Silhouette Score'] > 0.75 else 'Bom' if metrics['Silhouette Score'] > 0.5 else 'Moderado' if metrics['Silhouette Score'] > 0.25 else 'Fraco'} separaﾃｧﾃ｣o entre clusters.")
        pdf.multi_cell(0, 10, txt=f"  Davies-Bouldin Index: {metrics['Davies-Bouldin Index']:.2f} - "
                                  f"{'Muito bom' if metrics['Davies-Bouldin Index'] < 0.5 else 'Bom' if metrics['Davies-Bouldin Index'] < 1.0 else 'Moderado' if metrics['Davies-Bouldin Index'] < 2.0 else 'Fraco'} compactaﾃｧﾃ｣o e separaﾃｧﾃ｣o.")
        pdf.multi_cell(0, 10, txt=f"  Calinski-Harabasz Score: {metrics['Calinski-Harabasz Score']:.2f} - "
                                  f"{'Excelente' if metrics['Calinski-Harabasz Score'] > 2500 else 'Bom' if metrics['Calinski-Harabasz Score'] > 1500 else 'Moderado' if metrics['Calinski-Harabasz Score'] > 500 else 'Fraco'} densidade e separaﾃｧﾃ｣o.")
        pdf.ln(5)

    add_interpretation(initial_metrics, "Treino Inicial")
    if retrain_metrics:
        add_interpretation(retrain_metrics, "Re-Treino")

    # Adicionar grﾃ｡ficos
    metrics_to_plot = ["Silhouette Score", "Davies-Bouldin Index", "Calinski-Harabasz Score"]
    graphs = []
    for metric in metrics_to_plot:
        plt.figure(figsize=(6, 4))
        labels = ["Treino Inicial"]
        values = [initial_metrics[metric]]
        if retrain_metrics:
            labels.append("Re-Treino")
            values.append(retrain_metrics[metric])
        plt.bar(labels, values, color=['#90EE90', '#006400'], edgecolor='black')
        plt.title(f"{metric} por Treino")
        plt.ylabel(metric)
        plt.xlabel("Treino")
        graph_path = f"temp_{metric.replace(' ', '_')}.png"
        plt.savefig(graph_path)
        plt.close()
        graphs.append(graph_path)

    # Inserir grﾃ｡ficos no PDF
    pdf.add_page()
    pdf.set_font("Arial", style="B", size=12)
    pdf.cell(0, 10, txt="Grﾃ｡ficos das Mﾃｩtricas", ln=True, align='C')
    pdf.ln(10)

    x_offset = 10
    y_offset = pdf.get_y()
    for i, graph in enumerate(graphs):
        pdf.image(graph, x=x_offset, y=y_offset, w=90, h=70)
        x_offset += 100
        if (i + 1) % 2 == 0:  # Nova linha a cada dois grﾃ｡ficos
            x_offset = 10
            y_offset += 75
        os.remove(graph)  # Remover o arquivo temporﾃ｡rio apﾃｳs usﾃ｡-lo

    # Salvar o PDF no buffer
    pdf_buffer = BytesIO()
    pdf_output = pdf.output(dest='S').encode('latin1')
    pdf_buffer.write(pdf_output)
    pdf_buffer.seek(0)

    return pdf_buffer

# Pﾃ｡gina final para clustering
def clustering_final_page():
    st.title("Relatﾃｳrio Final do Clustering")

    # Verificar se os dados estﾃ｣o disponﾃｭveis
    if "selected_model_name" not in st.session_state or "initial_metrics" not in st.session_state:
        st.error("Nenhuma informaﾃｧﾃ｣o de clustering disponﾃｭvel. Por favor, execute o treino primeiro.")
        return

    # Mostrar o modelo selecionado
    st.subheader("Modelo Selecionado")
    st.write(f"**Modelo:** {st.session_state.selected_model_name}")

    # Adicionar informaﾃｧﾃ｣o sobre o nﾃｺmero de componentes
    if st.session_state.selected_model_name in ["KMeans", "Clustering Hierﾃ｡rquico"]:
        st.write(f"**Nﾃｺmero de Componentes PCA:** {st.session_state.get('pca_n_components', 'N/A')}")
    
# Exibir mﾃｩtricas do treino inicial
    st.subheader("Mﾃｩtricas do Treino Inicial")
    st.table(fix_dataframe_types(pd.DataFrame([st.session_state.initial_metrics])))

    # Interpretaﾃｧﾃ｣o personalizada para o treino inicial
    initial_metrics = st.session_state.initial_metrics
    st.write("**Interpretaﾃｧﾃ｣o do Treino Inicial:**")
    st.markdown(f"""
    - **Silhouette Score:** {initial_metrics["Silhouette Score"]:.2f} - {"Excelente" if initial_metrics["Silhouette Score"] > 0.75 else "Bom" if initial_metrics["Silhouette Score"] > 0.5 else "Moderado" if initial_metrics["Silhouette Score"] > 0.25 else "Fraco"} separaﾃｧﾃ｣o entre clusters.
    - **Davies-Bouldin Index:** {initial_metrics["Davies-Bouldin Index"]:.2f} - {"Muito bom" if initial_metrics["Davies-Bouldin Index"] < 0.5 else "Bom" if initial_metrics["Davies-Bouldin Index"] < 1.0 else "Moderado" if initial_metrics["Davies-Bouldin Index"] < 2.0 else "Fraco"} compactaﾃｧﾃ｣o e separaﾃｧﾃ｣o.
    - **Calinski-Harabasz Score:** {initial_metrics["Calinski-Harabasz Score"]:.2f} - {"Excelente" if initial_metrics["Calinski-Harabasz Score"] > 2500 else "Bom" if initial_metrics["Calinski-Harabasz Score"] > 1500 else "Moderado" if initial_metrics["Calinski-Harabasz Score"] > 500 else "Fraco"} densidade e separaﾃｧﾃ｣o.
    """)

    # Exibir mﾃｩtricas do re-treino (se disponﾃｭveis)
    retrain_silhouette_score = None  # Inicializa como None para evitar erros
    if "retrain_metrics" in st.session_state:
        st.subheader("Mﾃｩtricas do Re-Treino")
        st.table(fix_dataframe_types(pd.DataFrame([st.session_state.retrain_metrics])))

        # Interpretaﾃｧﾃ｣o personalizada para o re-treino
        retrain_metrics = st.session_state.retrain_metrics
        retrain_silhouette_score = retrain_metrics["Silhouette Score"]
        st.write("**Interpretaﾃｧﾃ｣o do Re-Treino:**")
        st.markdown(f"""
        - **Silhouette Score:** {retrain_metrics["Silhouette Score"]:.2f} - {"Excelente" if retrain_metrics["Silhouette Score"] > 0.75 else "Bom" if retrain_metrics["Silhouette Score"] > 0.5 else "Moderado" if retrain_metrics["Silhouette Score"] > 0.25 else "Fraco"} separaﾃｧﾃ｣o entre clusters.
        - **Davies-Bouldin Index:** {retrain_metrics["Davies-Bouldin Index"]:.2f} - {"Muito bom" if retrain_metrics["Davies-Bouldin Index"] < 0.5 else "Bom" if retrain_metrics["Davies-Bouldin Index"] < 1.0 else "Moderado" if retrain_metrics["Davies-Bouldin Index"] < 2.0 else "Fraco"} compactaﾃｧﾃ｣o e separaﾃｧﾃ｣o.
        - **Calinski-Harabasz Score:** {retrain_metrics["Calinski-Harabasz Score"]:.2f} - {"Excelente" if retrain_metrics["Calinski-Harabasz Score"] > 2500 else "Bom" if retrain_metrics["Calinski-Harabasz Score"] > 1500 else "Moderado" if retrain_metrics["Calinski-Harabasz Score"] > 500 else "Fraco"} densidade e separaﾃｧﾃ｣o.
        """)

    # Determinar o melhor modelo considerando apenas o Silhouette Score
    if retrain_silhouette_score is not None and retrain_silhouette_score > initial_metrics["Silhouette Score"]:
        melhor_modelo = "Re-Treino"
        best_metrics = st.session_state.retrain_metrics
        best_model = st.session_state.models[st.session_state.selected_model_name]
    else:
        melhor_modelo = "Treino Inicial"
        best_metrics = st.session_state.initial_metrics
        best_model = st.session_state.models[st.session_state.selected_model_name]

    st.subheader("Melhor Modelo")
    st.success(f"脂 **{melhor_modelo}** com Silhouette Score: {max(initial_metrics['Silhouette Score'], retrain_silhouette_score or 0):.4f}")


    # **Grﾃ｡ficos Interativos das Mﾃｩtricas**
    st.subheader("Grﾃ｡fico Interativo de Mﾃｩtricas")
    metrics_to_plot = ["Silhouette Score", "Davies-Bouldin Index", "Calinski-Harabasz Score"]
    selected_metric = st.selectbox("Selecione a mﾃｩtrica para visualizar:", metrics_to_plot)
    
    # Criar o grﾃ｡fico
    if selected_metric:
        # Verificar se os dados do re-treino estﾃ｣o presentes
        if "retrain_metrics" in st.session_state:
            # Se o re-treino foi realizado, exibe "Treino Inicial" e "Re-Treino"
            data_to_plot = pd.DataFrame({
                "Treino": ["Treino Inicial", "Re-Treino"],
                selected_metric: [
                    initial_metrics[selected_metric],
                    retrain_metrics[selected_metric]
                ]
            })
        else:
            # Se o re-treino nﾃ｣o foi realizado, exibe todas as mﾃｩtricas para "Treino Inicial"
            data_to_plot = pd.DataFrame({
                "Treino": ["Treino Inicial"] * len(metrics_to_plot),
                selected_metric: [initial_metrics[metric] for metric in metrics_to_plot]
            })
    
        # Criar grﾃ｡fico com base nos dados disponﾃｭveis
        fig, ax = plt.subplots(figsize=(6, 4))
        ax.bar(data_to_plot["Treino"], data_to_plot[selected_metric], color=['#a8ddb5', '#005a32'], edgecolor='black')
        ax.set_title(f"Comparaﾃｧﾃ｣o de {selected_metric}", fontsize=14, fontweight='bold')
        ax.set_ylabel(selected_metric, fontsize=12)
        ax.set_xlabel("Treino", fontsize=12)
        ax.tick_params(axis='both', which='major', labelsize=10)
        st.pyplot(fig) 


    # Gerar o relatﾃｳrio PDF
    pdf_buffer = gerar_relatorio_clustering_pdf(
        initial_metrics,
        st.session_state.get("retrain_metrics"),
        melhor_modelo,
        st.session_state
    )

    # Botﾃ｣o para download do relatﾃｳrio em PDF
    pdf_filename = f"Relatorio__{st.session_state.selected_model_name}_{st.session_state.model_type}_{melhor_modelo.replace(' ', '_').lower()}.pdf"
    st.download_button(
        label="Baixar Relatﾃｳrio em PDF",
        data=pdf_buffer,
        file_name=pdf_filename,
        mime="application/pdf"
    )

    # Botﾃ｣o para download do melhor modelo treinado
    model_buffer = BytesIO()
    pickle.dump(best_model, model_buffer)
    model_buffer.seek(0)

    st.download_button(
        label="Baixar Melhor Modelo Treinado",
        data=model_buffer,
        file_name=f"melhor_modelo_{melhor_modelo.replace(' ', '_').lower()}.pkl",
        mime="application/octet-stream"
    )

    # Botﾃ｣o para concluir o processo
    if st.button("Concluir"):
        st.info("Clustering finalizado. Redirecionando para o inﾃｭcio...")
        st.session_state.clear()
        st.session_state.step = 'file_upload'
        st.rerun()

def initialize_session_state():
    # Inicializando as variﾃ｡veis principais de estado
    default_values = {
        'step': 'file_upload',
        'page': 'file_upload',
        'data': None,
        'target_column': None,
        'target_column_confirmed': False,
        'validation_method': None,
        'validation_confirmed': False,
        'model_type': None,
        'model_type_confirmed': False,
        'X_train': None,
        'X_test': None,
        'y_train': None,
        'y_test': None,
        'knn_neighbors': 5,
        'rf_estimators': 100,
        'svc_kernel': 'rbf',
        'kmeans_clusters': 3,
        'feature_selection_done': False,
        'X_train_selected': None,
        'X_test_selected': None,
        'model_name': None,
        'selected_model': None,
        'selected_model_name': None,
        'models': {
            "Support Vector Classification (SVC)": SVC(),
            "K-Nearest Neighbors (KNN)": KNeighborsClassifier(),
            "Random Forest": RandomForestClassifier(),
            "KMeans": KMeans(),
            "Clustering Hierﾃ｡rquico": AgglomerativeClustering(linkage='ward'),
            "Regressﾃ｣o Linear Simples (RLS)": LinearRegression(),
            "Regressﾃ｣o por Vetores de Suporte (SVR)": SVR(),
        },
        'model_trained': False,
        'clustering_final_page': False,  # Pﾃ｡gina do relatﾃｳrio final de clustering
        'grid_search_confirmed': False,  # Adicionando grid_search_confirmed ao estado
        'manual_params': {}, # Inicializando manual_params como um dicionﾃ｡rio vazio
        'best_params_str': {},
        'treinos_realizados': [],  # Inicializando a lista de treinos realizados
        'scoring_confirmed': False,  # Inicializando scoring_confirmed
        'target_column_type': None,  # Adiciona tipo da coluna alvo
        'selected_scoring': 'F1-Score'  # Inicializando 'selected_scoring' com o valor 'f1'
    }

    # Usando valores padrﾃ｣o para inicializar session_state
    for key, value in default_values.items():
        if key not in st.session_state:
            st.session_state[key] = value

    # Adicionalmente, vocﾃｪ pode garantir que os parﾃ｢metros do modelo sejam vﾃ｡lidos:
    if st.session_state.knn_neighbors < 1:
        st.session_state.knn_neighbors = 5  # Default para KNN
    if st.session_state.kmeans_clusters < 1:
        st.session_state.kmeans_clusters = 3  # Default para KMeans

# Funﾃｧﾃ｣o principal
def main():
    # Inicializaﾃｧﾃ｣o das variﾃ｡veis de estado da sessﾃ｣o
    initialize_session_state()

    # Exibir estado atual para depuraﾃｧﾃ｣o
    #st.write(f"東 Estado atual: {st.session_state.step}")

    # Roteamento baseado no estado atual
    if st.session_state.step == 'file_upload':
        upload_file()
    elif st.session_state.step == 'data_preview':
        data_preview()
    elif st.session_state.step == 'missing_values':
        handle_missing_values()
    elif st.session_state.step == 'outlier_detection':
        outlier_detection()
    elif st.session_state.step == 'data_summary':
        data_summary()
    elif st.session_state.step == 'model_selection':
        model_selection()
    elif st.session_state.step == 'feature_selection':
        feature_selection()
    elif st.session_state.step == 'train_with_selected_features':  
        train_with_selected_features_page()
    elif st.session_state.step == 'evaluate_and_compare_models':
        evaluate_and_compare_models()
    elif st.session_state.step == 'clustering_final_page':  # 笨 Adicionado!
        clustering_final_page()  # 笨 Chama a funﾃｧﾃ｣o do relatﾃｳrio final de clustering
    elif st.session_state.step == 'final_page':
        final_page()
    else:
        st.error(f"笞 Etapa desconhecida: {st.session_state.step}. Reiniciando a aplicaﾃｧﾃ｣o.")
        st.session_state.step = 'file_upload'
        st.rerun()
        
    # Exibir o estado apﾃｳs a execuﾃｧﾃ｣o para depuraﾃｧﾃ｣o
    #st.write(f"Estado final: {st.session_state.step}")


if __name__ == "__main__":
    main()
